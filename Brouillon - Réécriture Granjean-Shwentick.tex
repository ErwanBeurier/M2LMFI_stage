\documentclass{report}

\include{packages}
\include{macros}

% Pour les itemize : 			\setlength{\itemsep}{-1mm}

% Variables pour le document

\author{BEURIER Erwan}
\title{M2 LFMI \\ BROUILLON }
\date{Année 2015-2016}

% Commandes de ce document

\newcommand{\sRAMif}[2]{\text{IF} (A=B) \{I( #1 )\} \text{ ELSE } \{I( #2 )\}}
\newcommand{\sRAMifc}[2]{\text{IF} (A_1=A_2) \{I( #1 )\} \text{ ELSE } \{I( #2 )\}}
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\TRec}[1]{\text{TRec}\left(\mathbb{#1}\right)}
\newcommand{\TRecd}[1]{\text{TRec}_{2}\left(\mathbb{#1}\right)}
\newcommand{\dtimeram}{\text{DTIME}_{\text{RAM}}\left( n^H \right)}
\newcommand{\dtimeramarg}[1]{\text{DTIME}_{\text{RAM}}\left( n^{#1} \right)}
\newcommand{\eqpred}[3]{#1\left[ #2^{\leftarrow}(#3) \right]_{#3}}
\newcommand{\eqpredf}[4]{#1\left[ #2^{\leftarrow}(#3) \right]_{#4}} % four arguments
\newcommand{\eqpredfi}[5]{#1\left[ #2^{\leftarrow}(#3) #4 \right]_{#5}} % five arguments
\newcommand{\arite}[1]{\text{arité}\left( #1 \right)}
\newcommand{\leqa}{\left( \leqslant a \right)}


\begin{document}
	
	\maketitle
	
	\tableofcontents
	
	\pagebreak 
	
	\chapter{LSRS et temps polynomial}
	\label{chap:LSRS}
	
	\section*{README}
		\label{sec:README}
	
	Ce fichier n'est qu'un brouillon de réécriture de l'article de Grandjean et Schwentick \emph{Machine-independent characterizations and complete problems for deterministic linear time} pour voir si ces notions peuvent s'étendre au temps $\grozo{n^K}$.
	
	C'est plus ou moins une traduction littérale en français et en remplaçant $\grozo{n}$ par $\grozo{n^K}$, pour voir si ça coince à un endroit.
	
	Je n'aime pas que les liens hypertextes soient trop visibles, mais je n'ai pas encore cherché comment on pouvait les rendre discrets, donc pour l'instant, ils sont simplement invisibles. Ne pas hésiter à passer la souris sur des mots comme \emph{ici}, \emph{plus bas}, etc ; ils seront très probablement munis d'un lien hypertexte. Les références à des sections, définitions, théorèmes, etc. sont aussi cliquables.
	
	
	\section*{Notations}
		\label{sec:Notations}
	
	
	Par abus de notation découlant de la théorie des ensembles, j'écrirai $n$ pour $\intint{0}{n-1}$ voire pour $\intint{1}{n}$ quand il n'y aura pas d'ambiguïté. De manière générale, si un terme ressemble à un entier naturel mais qu'il est mis à la place d'un ensemble (typiquement, dans le domaine de départ ou d'arrivée d'une fonction), il faut le lire comme l'ensemble $\intint{0}{n-1}$.
	
	\pagebreak
	
	\section{Preliminaries} (p.198)
		\label{sec:Preliminaries}
	
		\subsection{RAM data structures} (p.198)
			\label{sec:RAM_data_structures}

		\begin{definition}[RAM data structures]
			\label{def:RAM_data_structures}
			Soit $t$ un type, c'est-à-dire une signature fonctionnelle ne contenant que des symboles de constantes ou de fonctions unaires.
			
			Une RAM-structure $s$ de type $t$ est un uplet constitué de :
			\begin{itemize}[itemsep=-1mm]
				\item 	$n \in \naturels$ qui est la taille de la structure ;
				\item 	$C \in \naturels$ pour chaque symbole $C \in t$ ;
				\item 	$f : n \to \naturels$ pour chaque symbole $f \in t$.
			\end{itemize}
			
			On notera $s.n, s.C, s.f$ les composantes $n, C, f$ de $s$ (cette notation est à rapprocher de l'accès à un attribut ou à une fonction membre en programmation objet).
			
			On dira que $s$ est $c$-bornée pour $c\in\naturels$ lorsque $s.C, s.f(i) < c s.n$ pour tous $C, f \in t$ et $i \in n$.
		\end{definition}
	
	
		\begin{definition}[Fonction de RAM]
			\label{def:fonction_de_RAM}
			Soient $t_1, t_2$ des types. 
			
			Une $(t_1, t_2)$-fonction de RAM $\Gamma$ est une fonction telle qu'il existe $c_1, c_2 \in \naturels$, tels que $\Gamma$ envoie les structures $c_1$-bornées de type $t_1$ sur des structures $c_2$-bornées de type $t_2$\footnotemark.
			
				\footnotetext{On rappelle que "$c$-borné" ne concerne que la structure par rapport à sa propre taille ; ici on ne compare pas la taille de l'entrée et de la sortie}
			
			On dit que $\Gamma$ est polynomiale lorsque $\Gamma(s).n = \grozo{(s.n)^K}$.
			
			% Là, si.
		\end{definition}
	
	
	
		\subsection{Machine RAM} (p.200)
			\label{subsec:machine_RAM}
		
		La machine RAM reste la même (heureusement). On va utiliser la $\{+\}$-RAM ou des versions un brin plus puissantes comme la $\{+ , -, \times, \div k \}$-RAM pour un $k\in \naturels$ fixé\footnotemark. 
		
			\footnotetext{L'ajout de ces opérations ne rend pas la $\{+\}$-RAM plus puissante \cite{Grandjean1994}, \cite{GrandjeanSchwentick2002}.}
		
		\begin{definition}[Temps polynomial]
			\label{def:temps_poly_RAM}
			On définit $\dtimeram$ comme étant l'ensemble des fonctions calculables sur $\{+\}$-RAM en temps $\grozo{n^K}$, telles que le nombre de registres utilisés, les valeurs entières manipulées (y compris les adresses de registres) soient bornés par $\grozo{n^K}$.
		\end{definition}
		
		
		
		\subsection{Réductions affines} (p.202)
			\label{subsec:reductions_affines}

		On laisse inchangées les notions de \emph{transformations affines} (définition 2.3), de \emph{réductions affines} (définition 2.5), de \emph{projections affines} (définition 2.7). Les théorèmes et lemmes suivants ou intermédiaires sont aussi inchangés. Ils permettront de définir des réductions qui \emph{restent} dans $\dtimeram$ pour un $k$ fixé. 
		
		\begin{definition}[Fonction affine non-décroissante]
			On appellera \emph{fonction affine non-décroissante}, ou plus simplement, \emph{fonction affine}, une fonction $A$ de la forme $A(x_1, \dots, x_k) = a_0 + a_1 x_1 + \dots + a_k x_k$, telle que soit $a_1, \dots, a_k \geqslant 0$ et $a_0 \in \bb{Z}$, soit $a_1, \dots, a_k = 0$ et $a_0 \geqslant 0$.
		\end{definition}
		
		\begin{definition}[Transformation affine]
			\label{def:transfo_affine}
			Soit $T$ une fonction de RAM qui envoie des RAM-structures de type $t_1$ sur des RAM structures de type $t_2$. 
			
			On dit que $T$ est une transformation affine lorsque : 
			
			\nepasoublier{REMPLIR ICI !!!!!!!!!!!!}
			
			\begin{itemize}[itemsep=-1mm]
				\item 	
			\end{itemize}
		\end{definition}
		
		
		
	\section{Le framework algébrique} (p.208)
		\label{sec:framework_algebrique}

		\subsection{LSRS} (p.208)
			\label{subsec:LSRS}
		
		Le LSRS en tant que tel n'a pas l'air collé à la définition du temps linéaire. On garde la définition pour le moment.

		\begin{definition}[Application bornée et \emph{equal-predecesor}]
			\label{def:app_bornee_eq_pred}
			Pour $f : n \to \naturels$, on définit deux opérations :
			
			\begin{itemize}
				\item 	L'application bornée :
				\[
				f[x]_y = \casedist{
					f(x) & \text{si $x<y$} \\
					x 	& \text{sinon}
				}
				\]
				
				\item 	L'opération \emph{equal-predecesor} :
				\[
				f^{\leftarrow} = \casedist{
					\max\left(\left\lbrace y < x | f(x) = f(y)\right\rbrace\right) & \text{si un tel $y$ existe} \\
					x	& \text{sinon}
				}
				\]
			\end{itemize}
			
			Pour $g, g' : n \to \naturels$, on combine ces deux opérations pour en créer une troisième, opération de \emph{récursion} :
			
			\[
			f(x) = \eqpred{g'}{g}{x}
			\]
			
			$f(x) = g'(y)$ où $y$ est de plus grand $z$ tel que $g(x) = g(z)$, ou $f(x) = x$ si un tel $y$ n'existe pas.
		\end{definition}
		
		
		\begin{definition}[LSRS]
			\label{def:LSRS}
			Soit $F$ un ensemble de symboles de fonctions (dites \emph{fonctions de base}), soient $f_1, \dots, f_k$ des symboles de fonctions qui n'apparaissent pas dans $F$. Pour $i\leqslant k$, notons $F_i = F\cup \{f_1, \dots, f_i\}$. 
		
		Un LSRS (\emph{Linear Simultaneous Recursion Scheme}) $S$ sur $f_1, \dots, f_k$ et $F$ est une suite de $k$ équations $\left(E_i\right)_{i_\in k}$ dont chacune est de l'une des deux formes suivantes :
		
		\begin{itemize}
			\item 	(opération) 		$f_i(x) = g(x) * g'(x)$ où $g,g' \in F_{i-1}$ et $* \in \{+, -, \times \}$\footnotemark
			
			\footnotetext{Les opérations peuvent varier, il est dit dans \cite{GrandjeanSchwentick2002} que l'on peut choisir n'importe quelle opération binaire calculable en temps linéaire sur machine de Turing, voire la multiplication. Nous verrons d'ailleurs par la suite que, pour des raisons de facilité, nous aurons besoin de la multiplication.}
			
			\item 	(récursion)			$f_i(x) = \eqpred{g'}{g}{x}$ où $g' \in F_k$ et $g \in F_{i-1}$
		\end{itemize}
		
		\end{definition}
		
		
		On doit refaire la définition 3.3 de l'article (la définition de \emph{linéairement représentable}).
		
		Pour $t$ un type, on note $F_t$ l'ensemble des symboles de fonctions suivants : $\{1(-), n(-), id(-)\} \cup  \{ f_C | C \in t\} \cup \{ f | f \in t\}$.
		
		\begin{remark}
			\label{rk:entree_LSRS}
			Soient $t$ un type et $S$ un LSRS pour $f_1, \dots, f_k$ sur $F_{t}$. 
			L'entrée d'un LSRS peut être vue comme étant une RAM-structure $s$ de type $t$, qu'il \emph{lit} en interprétant les symboles de $F_t$ de la façon suivante : 
			
			\begin{itemize}[itemsep=-1mm]
				\item 	$\forall f \in t_1$ :   $f(i) = 
				\left\lbrace \begin{array}{ll}
				s.f(i) & \text{si } i< s.n \\
				0 & \text{sinon}
				\end{array}\right.$
				
				\item 	$\forall C \in t_1$ :   $f_C(i) = s.C$
				\item 	$1(i) = 1$, $n(i) = s.n$, $id(i) = i$
			\end{itemize}
			
			La sortie du LSRS peut aussi être vue comme une nouvelle structure $s' = S(s)$ de type $\{f_1, \dots, f_k\}$.
		\end{remark}
		
		
		Ici, on a plusieurs possibilités pour adapter le concept de \emph{linéairement représenté} (définition 3.3) à $n^K$.
		
		\begin{definition}[RAM $n^K$-représentée par LSRS - Proposition 1 \footnotemark]
			\label{def:representee_par_LSRS}
			Soient $t_1, t_2$ des types. Soit $\Gamma$ une $(t_1, t_2)$-fonction de RAM.
			
			\footnotetext{Il pourra à l'avenir y avoir d'autres possibilités mais cette définition semble bien élargir la définition pour le temps linéaire.} 
			
			Soit $S$ un LSRS pour $f_1, \dots, f_k$ sur $F_{t_1}$. 
			
			On dit que $\Gamma$ est $n^K$-représentée par $S$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée, $S$ définit des fonctions $f_1, \dots, f_k : c (s.n)^K \to c (s.n)^K$ telles que $\Gamma(s) = P((s.n)^K, S(s))$ ($S(s)$ est la structure définie par le LSRS).
			
			\emph{La modification réside dans le domaine de définition des fonctions et la taille de la sortie dans la projection.}
		\end{definition}
			
		
		\begin{prob}
			Est-ce qu'on capture tout $\dtimeram$ ? 
		\end{prob}
		
		\begin{prob}
			Est-ce que $P((s.n)^K, S(s))$ capture toutes les structures de taille $\grozo{n^K}$ ?
		\end{prob}
			
	
		Si $\Gamma$ est $n^K$-représentée par un LSRS alors on dit que $\Gamma$ est définissable par LSRS. 
		
		
		% Eventuellement d'autres possibilités
		
		La définition 3.4 (définition par cas) et les lemmes 3.5 (la définition par cas ne change pas la puissance des LSRS) et 3.6 (composition de fonctions définissables par LSRS reste définissable par LSRS) restent les mêmes.
		
		
		\subsection{LRS} (p.211)
			\label{subsec:LRS}
		
		
		La définition d'un terme récursif (non numérotée) et d'un LRS (définition 3.7) restent les mêmes. Le lemme 3.8 d'existence d'une solution unique au LRS aussi. 
		
		On doit adapter la définition de $n^K$-représentée par LRS : 
		
		\begin{definition}[RAM $n^K$-représentée par LRS - Proposition 1]
			\label{def:representee_par_LRS}
			Soit $\Gamma$ une fonction de RAM. 
			
			Soit $E$ un LSRS $g(x) = \sigma(x)$. 
			
			On dit que $\Gamma$ est $n^K$-représentée par $E$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée, $E$ définit une fonction $g : c (s.n)^K \to c (s.n)^K$ telle que $\Gamma(s) = P((s.n)^K, E(s)$ ($E(s)$ est la structure définie par le LRS, elle est de type $\{g\}$).
			
			\emph{La modification réside dans le domaine de définition de $g$.}
		\end{definition}
		
		Et là, c'est la foire.
		
		On doit vérifier si le théorème principal de l'article tient encore au temps polynomial.
		
		
		
		\begin{conj}[Adaptation du lemme 3.10 (p.212)]
			\label{conj:rep_LSRS_rep_LRS}
			Toute fonction de RAM $n^K$-représentée par LSRS est aussi $n^K$-représentable par LRS.
		\end{conj}
		
		\begin{proof}
			Ici commence la relecture de la preuve.
			
			Soit $\Gamma$ une fonction de RAM $n^K$-représentée par un LSRS $S$ sur $F_{t_1}$ pour $f_0, \dots, f_{k-1}$, comme dans la définition, et soit $P$ la projection affine associée. Soit $s$ une RAM-structure $c$-bornée de type $t_1$. On note $s' = S(s)$ la structure définie par $S$ avec entrée $s$, et on note $s'' = \Gamma(s) = P\left( (s.n)^K, s'\right)$. Pour simplifier les notations, on va simplement écrire $n$ au lieu de $s.n$.
			
			L'idée est de coder les fonctions $f_0, \dots, f_{k-1} : cn^K \to cn^K$ par une unique fonction $g$. Pour s'assurer que la fonction \emph{Equal-Predecessor} fonctionne correctement, le codage des $f_i$ doit avoir des domaines disjoints et des images disjointes. Cela peut se faire, pour chaque $i<k$, en n'utilisant que des valeurs congrues à $i$ modulo $k$ pour la fonction $f_i$. Puisqu'on va en avoir besoin pour coder/décoder les opérations, on va aussi coder la fonction $\div k$ dans $g$.\\
			
			On va étendre le domaine de $g$ pour encoder $\Gamma(s).f$ pour chaque symbole de fonction $f$ du type de la structure de sortie, \emph{by function values of a contiguous interval} (???).
			
			Précisément, $g$ va être définie sur $2kcn^K$ de sorte que :
			
			\begin{itemize}[itemsep=-1mm]
				\item	pour tout $b \in kcn^K$, on a $g(b) = b \div k$ ;
				\item 	pour tous $a \in cn^K$ et $i \in k$, on a $g\left( kcn^K + ka + i \right) = kcn^K + k f_i(a) + i$ $\textcolor{red}{(\star)}$
			\end{itemize}
			% Commentaire bidon
			
			\espace
			
			\fbox{
				\begin{minipage}{0.9\textwidth}
					
					{\footnotesize \begin{minipage}{0.95\textwidth}
						On a donc besoin d'un moyen de calculer $n^K$ dans la fonction de sortie.
						
						Pour l'instant, pour des raisons de simplicité, puisqu'on sait que la fonction est $n^K$-représentable, on va supposer qu'on a accès à une fonction $i \mapsto n^K$. Pour ce faire, on peut soit la définir par multiplication en rajoutant $H$ équations \emph{(ce qui ne me semble pas naturel, parce qu'on modifie le LSRS à chaque fois qu'on veut changer la taille d'arrivée ?)} soit on peut rajouter une fonction constante $i \mapsto n^K$ \emph{(on ajoute ou remplace une fonction dans les fonctions de base du LSRS ? Ça veut dire qu'on rajoute un symbole de fonction au type de départ, qui sait déjà ce qu'on va en faire ?)}.
						
						Problème : apparemment le LSRS sait qu'il représente des fonctions de taille $n^K$, mais est-ce suffisant pour justifier l'insertion de cette fonction en tant que fonction du domaine ? Apparemment, un même LSRS peut être utilisé pour toutes les tailles de domaines, donc peut-être qu'on peut, au cas par cas, rajouter une fonction en plus selon le domaine d'arrivée ?
						
						(La question ici est de savoir si on peut rajouter naturellement cette fonction constante d'accès direct au max, sans avoir l'air de tricher pour se faciliter la vie.)
						
						Ou bien on peut voir l'ajout de cette fonction comme un ajout de contrainte. C'est l'ajout de cette fonction qui impose que le domaine d'arrivée sera de taille $\grozo{n^K}$.
					\end{minipage} }
				
					
					\redtext{Solution}
					%En fait, dans la démonstration originale, $kcn$ n'est pas calculée à la volée dans le système. Le LSRS est défini à l'aide d'une fonction constante $n(-)$ qui, malgré la notation, n'a rien à voir avec le $n$ de $kcn = kcs.n$. En fait, $n(-)$ permet d'accéder à la borne du domaine de définition des fonctions du LSRS. Ici, $n(y) = kcs.n$, ce qui permet au LSRS d'avoir accès à cette valeur. Dans le cas polynomial, on a $n(y) = kc(s.n)^K$, ce qui est bien ce qu'on voulait. La démonstration se tient. 
					Dans la démonstration originale, $kcn$ est calculé à la volée dans le LRS. Il s'agit d'une abréviation : $cn = \underset{c \text{ fois}}{\underbrace{n(y) + \dots + n(y)}}$, et $kcn = \underset{k \text{ fois}}{\underbrace{cn + \dots + cn}}$. 
					
					La constante $c$ est toujours une valeur explicite, donnée par définition de la fonction. On peut donc construire ce terme. Idem pour $k$, qui est le nombre d'équations. 
					
					Dans notre cas, on ne peut pas faire un terme similaire pour $n^K$, car on ne peut pas autoriser un terme dont la longueur dépend de l'entrée (un LRS est défini par une équation fixée dès le départ). Il faut donc qu'on ajoute la multiplication comme opération de base de la définition par LRS, et on pourra écrire un terme similaire : $n^K = \underset{K \text{ fois}}{\underbrace{n(y) \times \dots \times n(y)}}$, puis $cn^K = \underset{c \text{ fois}}{\underbrace{n^K + \dots + n^K}}$, et $kcn^K = \underset{k \text{ fois}}{\underbrace{cn^K + \dots + cn^K}}$. 
					
					L'ajout de la multiplication ne rend pas le LSRS, le LRS et la RAM plus puissants, d'après la partie 3.1 et la proposition 2.1 de \cite{GrandjeanSchwentick2002}. 
					
				\end{minipage}
			}
				
				\espace
			
			
			En d'autres termes, pour $b \geqslant kcn^K$ et $b \mod{k} = j$, on a $g(b) = kcn^K + k f_j\left( (b-kcn^K) \div k \right) + j$.
			On va définir la valeur de $g(b)$ pour $b \in 2kcn^K$ par distinction de cas sur $b \mod{k}$, comme décrit dans (1) à (3) ci-dessous.
			
			\begin{enumerate}[itemsep=-1mm,leftmargin=2cm]
				%\setlength{\itemsep}{-1mm}
				\item  
					Renumérotons les symboles de $F_{t_1} = \{f | f \in t_1\} \cup \{f_C | C \in t_1\} \cup \{1(-), n(-), id(-)\}$ ainsi $\{ f^0, \dots, f^{l-1} \}$. On va limiter les occurrences de ces symboles. Premièrement, les symboles $f_0, \dots, f_{k-1}$ sont remplacés par $f^{l}, \dots, f^{l+k-1}$ respectivement. Ensuite, On va introduire $l$ nouvelles fonctions $f_0, \dots, f_{l-1}$ et $l$ équations pour les définir :
					
						\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
							\item	Si $f^i$ vient de $t_1$, alors l'équation associée est $f_i(x) = f^i(x)$ ;
							\item 	Si $f^i = id$, alors l'équation associée est $f_i(x) = x$ ;
							\item 	Si $f^i = f_C$, ou $1$ ou $n$, alors l'équation associée est (respectivement) $f_i(x) = C, 1, n$. 
						\end{itemize}
				
					% Subitem ne marche pas ?
					
					On appellera ces équations des \emph{équations d'entrée} ; elles servent justement à remplacer les entrées du LSRS.
					
					Enfin, on remplace dans le LSRS $S$ tous les anciens symboles de fonctions (ceux de $F_{t_1}$) par les nouveaux (les $(f_i)_{i \in l+k}$). Après ces remplacements, $S$ ne contient plus aucune référence à $F_{t_1}$, sauf pour les équations d'entrée. 
					
				\item 	
					Les équations de $S$ sont combinées en une seule équation $g(y) = \sigma(y)$ comme suit. Le terme $\sigma(y)$ est principalement une distinction de cas dépendant de $y$ et $y \mod{k}$. 
				
					\[
						g(y) = 
							\left\lbrace \begin{array}{ll}
								0 				& \text{si $y \leqslant k-1$} \\
								g(y-k) + 1 		& \text{si $k \leqslant y < kcn^K$} \\
								\sigma_i(y)		& \text{si $y \geqslant kcn^K$ et $y \mod{k} = i$}
							\end{array}\right.
					\]
					
					où $\sigma_i(y)$ est un terme récursif qu'on explicitera tout de suite après. 
					
					Notons que les deux premiers cas donnent $g(y) = y \div k$ pour chaque $y < kcn^K$, comme voulu. Cela nous permet d'exprimer $y \mod{k}$ pour $kcn^K \leqslant y < 2kcn^K$, puisque $(y-kcn^K) - kg(y-kcn^K) = (y-kcn^K) - \sum_{j=1}^{k} g(y-kcn^K)$. \redtext{(Et alors ???)}
					
					Ensuite, on a vu que la distinction de cas ne rendait pas le LSRS plus puissant. 
					
					Enfin, on décrit la construction des termes de récurrence $\sigma_i(y)$ (on distingue la variable $y$ du terme de récurrence, de la variable $x$ des équations) :
					
					\begin{itemize}[itemsep=-1mm, leftmargin=1cm]
						\item
							Si $E_i$ est une équation d'entrée, alors $\sigma_i(y)$ est construit comme suit :
					
							\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
								\item %\subsubitem -		
								Si le terme de droite de l'équation est une constante $C$ (éventuellement $1$ ou $n$), alors $\sigma_i(y) = kcn^K + kC + i$ ;
								
								\item %\subsubitem	-	
								Si le terme de droite de l'équation est $x$, alors $\sigma_i(y) = kcn^K + kg\left( y - kcn^K \right) + i$ ;
								
								\item %\subsubitem	-	
								Si le terme de droite de l'équation est $f^i(x)$, alors $\sigma_i(y) = kcn^K + k f^i\left( g\left( y - kcn^K \right) \right) + i$.
								
								Justifions pourquoi cette définition de $\sigma_i(y)$ est correcte. On le fait pour le troisième cas ; les deux autres sont plus simples. Soit $b = kcn^K + ka + i$ avec $a < cn^K$. Alors $g(b-kcn) = g(ka + i) = (ka + i) \div k = a$, et $\sigma_i(b) = kcn^K + kf^i(a) + i$, comme voulu. 
							\end{itemize}
							
			
						\item
							Si $E_i$ est de la forme $f_i(x) = f_j(x) - f_{j'}(x)$, alors $\sigma_i(y)$ est défini par :
							
							\[
								\sigma_i(y) = \left( g(y - \delta) - k cn^K - j \right) - \left( g(y - \delta') - k cn^K - j' \right) + kcn^K + i
							\]
							
							où $\delta = i -j$ et $\delta' = i - j'$ et, par définition d'un LSRS, $i > j, j'$. Pour vérifier que cette expression est correcte, soient $a \in cn^K$, $b = kcn + ka + i$, où $i<k$. Si $g(b - \delta) = g(kcn^K + ka + j) = kcn^K + kf_j(a) + j$ et $g(b - \delta') = g(kcn^K + ka + j') = kcn^K + kf_{j'}(a) + j'$ alors :
							
							\[
								\left( g\left( b - \delta \right) - kcn^K - j \right) - \left( g\left( b - \delta' \right) - kcn^K - j' \right)
								=  k \left( f_{j}(a) - f_{j'}(a) \right) + kcn^K + i
							\]

							
							Ce qui est ce qu'on voulait.
							
						\item
							Si $E_i$ est de la forme $f_i(x) = f_j(x) + f_{j'}(x)$, alors son traitement est similaire au cas précédent, à ceci près qu'il faut que l'addition $x + y$ renvoie $0$ si $x + y > cn^K$. On redéfinit alors $\sigma_i(y)$ : 
							
							\[
								\sigma_i(y) = \casedist{
									\tau(y) + kcn^K + i & \text{si $\tau(y) < kcn^K$} \\
									kcn^K + i & \text{sinon}
									}
							\]
							
							où $\tau(y) = \left( g\left( b - \delta \right) - kcn^K - j \right) - \left( g\left( b - \delta' \right) - kcn^K - j' \right)$ avec $\delta = i - j$ et $\delta' = i - j'$. 
							
							La vérification se passe de la même manière que précédemment.
							
						\item
							Si $E_i$ est de la forme $f_i(x) = \eqpred{f_{j'}}{f_j}{x}$, où $j < i$ et on suppose sans perte de généralité que $i \leqslant j'$ 
								\footnote{Si $i > j'$ alors on doit rajouter une nouvelle fonction $f_l$ à $S$, telle que $l > i$, et définie par la nouvelle équation $f_l(x) = f_{j'}$ et remplacer $E_i$ par $f_i(x) = \eqpred{f_{l}}{f_j}{x}$}
							, alors $\sigma_i(y)$ est définie par :
							
							\[
								\sigma_i(y) = \eqpredfi{g}{g}{y-\delta}{ + \delta'}{y} - j' + i
							\]
							
							où $\delta = i - j$ et $\delta' = j' - j$.
							
							Pour justifier ce remplacement, on doit s'assurer que le codage de plusieurs fonctions en une seule ne cause par d'effets de bord quand on utilise \emph{Equal-Predecesor}. Ce qui est crucial ici, c'est que, pour chaque $i<k$, les valeurs de $g$ qui codent $f_i$ soient congruentes à $i$ modulo $k$. Pour être plus précis, soit $b = kcn^K + ka + i$, avec $a< cn^K$. Alors $b-\delta = kcn^K + ka + i - (i - j) \underset{i>j}{=} kcn^K + ka + j$. On a deux sous-cas à étudier :
							
							\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
								\item
									$f_j^{\leftarrow}(a) = a$.   Dans ce cas, pour aucun $a' < a$, on n'a $f_j(a') = f_j(a)$, donc il n'y a pas de $a' < a$ pour lequel $g\left( kcn^K + ka' + j \right) = g\left( kcn^K + ka + j \right)$. La définition de $g$ assure que, pour chaque $e \geqslant kcn^K$, on a $\left(g(e) \mod{k} = j \Leftrightarrow e \mod{k} = j\right)$ et $\forall e, e' \in 2kcn^K$, si $g(e) = g(e')$, alors soit $e, e' < kcn^K$, soit $e, e' \geqslant kcn^K$. Donc $g^{\leftarrow} \left( kcn^K + ka + j \right) = kcn^K + ka + j$ et $g^{\leftarrow}\left( b - \delta \right) + \delta' = kcn^K + ka + j' \geqslant kcn^K + ka + i = b$. Ainsi :
									
									\setcounter{equation}{0}
									\begin{eqnarray}
										\sigma_i(b) 	& = &	\eqpredfi{g}{g}{b-\delta}{+\delta'}{b} - j' + i \\
														& = & 	\left( kcn^K + ka + j' \right) - j' + i \\
														& = & 	kcn^K + ka + i \\
														& = & 	kcn^K + k \eqpred{f_{j'}}{f_j}{a} + i \\
														& = & 	kcn^K + k f_i(a) + i \\
														& = & 	g(b),
									\end{eqnarray}
									
									comme voulu.
									
								\item
									$f_j^{\leftarrow}(a) = a'$ pour un certain $a' < a$. Dans ce cas, $g\left( kcn^K + ka + j \right) = kcn^K + ka' + j$. En conséquence : 
									
									\[
										g^{\leftarrow}\left( b - \delta \right) + \delta' = kcn^K + ka' + j' < kcn^K + ka + i = b
									\]
									
									Donc :
									
									\setcounter{equation}{0}
									\begin{eqnarray}
										\sigma_i(b) 	& = &	\eqpredfi{g}{g}{b-\delta}{+\delta'}{b} - j' + i \\
														& = & 	g\left( kcn^K + ka' + j' \right) - j' + i \\
														& = & 	\left( kcn^K + k f_{j'}(a') + j' \right) - j' + i \\
														& = & 	kcn^K + k f_{j'}(a') + j' + i \\
														& = & 	kcn^K + k \eqpred{f_{j'}}{f_j}{a} + i \\
														& = & 	kcn^K + k f_i(a) + i \\
														& = & 	g(b),
									\end{eqnarray}
									
									comme souhaité.
							\end{itemize}
					\end{itemize}
					
					
				\item	
					Maintenant, on complète le LRS pour $g$. Pour une question de simplicité, on va supposer que $t_2$ ne contient que le symbole de constante $n$ et un seul symbole de fonction $h$. Soient $j <k$ et $\alpha$ une fonction affine tels que, pour toute structure $s$, on ait $\Gamma(s).n = P\left((s.n)^K, s'\right).n = s'.f_j\left(\alpha\left( (s.n)^K \right)\right)$, et soient $i <k$ et $A$ une fonction affine tels que, pour toute structure $s$ et tout $a < \Gamma(s).n$, on ait $\Gamma(s).h(a) = P\left((s.n)^K, s'\right).h(a) = s'.f_i\left(A\left( (s.n)^K, a \right)\right)$\footnotemark.
					
						\footnotetext{Ça veut juste dire : on donne des noms aux éléments qui permettent de définir la structure $\Gamma(s)$ ; ces fonctions affines $\alpha, A$ et ces entiers $i,j$ existent par définition d'une fonction $n^K$-représentable par un LSRS. Reste à voir si cette définition a bien du sens, mais si ça a du sens, alors il n'y a pas de problème ici.}
					
					On a construit $g$ de telle manière que toutes les valeurs de fonctions $f_i(a)$ sont, en quelque sorte, disponibles dans $g$, mais on a encore deux problèmes à résoudre. Premièrement, les $f_i(a)$ apparaissent uniquement sous une forme codée ; deuxièmement, elles ne forment pas un intervalle contigu mais sont éparpillées modulo $k$. Il faut donc, avant d'extraire les valeurs à l'aide d'une projection affine bien choisie, décoder les valeurs des fonctions et les ramener dans un même intervalle. Pour ça, on élargit le domaine de $g$ à $(2k+2)cn^K$ et on complète la définition de $g$ :
					
					\[
						g(y) = \casedist{
							\text{comme avant} & \text{si $y < 2kcn^K$} \\
							g\left[ g\left[ k\left( y - 2kcn^K \right) + kcn^K  + i\right]_y - kcn^K  \right]_y & \text{si $2kcn^K \leqslant y < (2k+1)cn^K$} \\
							g\left[ g\left[ k\left( y - (2k + 1)cn^K \right) + kcn^K  + j \right]_y - kcn^K  \right]_y & \text{si $(2k+1)cn^K \leqslant y < (2k+2)cn^K$} \\
							}
					\]
					
					Il découle de l'équation \redtext{$(\star)$} (la définition de $g$ sur $kcn^K$) et de cette définition que, pour tout $a < cn^K$, on a\footnotemark :
					\footnotetext{Analysons :
						\begin{itemize}
							\setlength{\itemsep}{-1mm}
							\item	$(1) \rightarrow (2)$ : parce que soustraction propre.
							\item 	$(2) \rightarrow (3)$ : par définition de l'opérateur \emph{application bornée}.
							\item 	$(3) \rightarrow (4)$ : parce que $k a + kcn^K  + i < 2kcn^K$ donc on reprend la définition de la fonction $g$ sur l'intervalle $2kcn^K$.
							\item 	$(4) \rightarrow (5)$ : \emph{because} soustraction propre.
							\item 	$(5) \rightarrow (6)$ : parce que $f_i(a) < cn^K$ et on a le bon décalage avec $i$.
						\end{itemize}	
					}
					\setcounter{equation}{0}
					\begin{eqnarray}
						g\left( 2kcn^K + a\right)	& = & 		g\left[ g\left[ k\left( \left(2kcn^K + a\right) - 2kcn^K \right) + kcn^K  + i\right]_{2kcn^K + a} - kcn^K  \right]_{2kcn^K + a} \\
													& = & 		g\left[ g\left[ k a + kcn^K  + i\right]_{2kcn^K + a} - kcn^K  \right]_{2kcn^K + a} \\
													& = & 		g\left[ g\left( k a + kcn^K  + i\right) - kcn^K  \right]_{2kcn^K + a} \\
													& = & 		g\left[ \left(kcn^K  +  k f_i(a) + i\right) - kcn^K  \right]_{2kcn^K + a} \\
													& = & 		g\left[ k f_i(a) + i \right]_{2kcn^K + a} \\
													& = & 		f_i(a)
					\end{eqnarray}
					
					De la même manière, on obtient, pour $a < cn^K$, $g\left( (2k+1)cn^K + a\right) = f_j(a)$. 
					
					Maintenant, il est aisé de définir une projection affine $P'$ qui extrait $\Gamma(s)$ à partir de $s'''$ de type $\{g\}$, définie par le LRS :
					\[
						\Gamma(s).n = s'''.g\left( \alpha\left( cn^K\right) + (2k+1)cn^K\right) \footnotemark
					\]
					
						\footnotetext{C'est bien ce qu'il nous faut, parce que $\alpha': a \mapsto \alpha\left(ca\right) + (2k+1)ca$ est bien une fonction affine telle que $P'\left(n^K, s'\right).n = s'''.g\left(\alpha'\left(n^K\right)\right)$.}
					
					et
						\footnote{On rappelle que $s'$ est la structure résultante du LSRS originel, de type $\{f_0, \dots, f_{k-1}\}$, qu'on a compactée dans une seule structure de type $\{g\}$.}
						\footnote{Analysons :
							\begin{itemize}
								\setlength{\itemsep}{-1mm}
								\item 	$(1) \rightarrow (2)$ : par définition de $A$ et $i$.
								\item 	$(2) \rightarrow (3)$ : parce qu'on a $f_i(a) = g\left( 2kcn^K + a \right)$ par construction de $g$.
							\end{itemize}
							}
					 :
					\setcounter{equation}{0}
					\begin{eqnarray}
						\Gamma(s).h(a) 	& = &	P\left( n^K, s' \right).h(a) \\
										& = &	s'.f_i\left( A\left( n^K, a \right) \right) \\
										& = & 	s'''.g\left( 2kcn^K + A\left( n^K, a \right) \right) \footnotemark
					\end{eqnarray}
					
					\footnotetext{C'est bien ce qu'il nous faut parce que $A' : a,b \mapsto 2kcb + A\left( b, a \right)$ est bien une fonction affine telle que $P'\left(n^K, s'\right).h(a) = s'''.g(A'(n^K, a))$.}
					
			\end{enumerate}
			
		
	\end{proof}
	
		
		\subsection{Bilan}
		La démonstration a l'air de marcher !!
		

	\pagebreak
		
		
	\section{LRS et temps linéaire} (p.216)
		\label{LRS_et_temps_lineaire}
	
	\begin{conj}
		\label{conj:big_theorem}
		Soit $\Gamma$ une fonction de RAM. Les propositions suivantes sont équivalentes :
		
		\begin{enumerate}[itemsep=-1mm]
			\item 	$\Gamma \in \dtimeramarg{K}$.
			\item 	$\Gamma$ est $n^K$-représentée par un LSRS.
			\item 	$\Gamma$ est $n^K$-représentée par un LRS.
		\end{enumerate}
	\end{conj}
	
	On a montré $(2) \Rightarrow (3)$ \hyperref[conj:rep_LSRS_rep_LRS]{juste avant}. 
	La preuve de $(3) \Rightarrow (1)$ se trouve \hyperref[conj:rep_LRS_calc_n_K]{ci-dessous}.
	La preuve de $(1) \Rightarrow (2)$ se trouve \hyperref[conj:poly_implique_LSRS]{plus loin}.
	
	
	\subsection{$(3) \Rightarrow (1)$}
		\label{subsec:LRS_implique_poly}
		
		\begin{conj}
			\label{conj:rep_LRS_calc_n_K}
			Si une fonction de RAM $\Gamma$ est $n^K$-représentée par un LRS $E$ alors $\Gamma \in \dtimeramarg{K}$.
		\end{conj}
		
		\begin{proof}
			{
% Je sais, c'est très sale de les mettre ici. Je trouve juste l'écriture plus agréable avec ces macros, et c'est un usage purement local.
\newcommand{\Fin}{F_{\text{in}}}
\newcommand{\Ginv}{G_{\text{inverse}}}

			Soit $\Gamma$ une fonction de RAM $n^K$-représentée par une équation $E$, et soit $P$ la projection affine correspondante. Pour des raisons de simplicité, on va considérer que le type d'entrée de $\Gamma$ ne contient qu'un seul symbole de fonction $f_{\text{in}}$. Rappelons-nous que $E$ est une équation de la forme $g(x) = \sigma(x)$, où $\sigma(x)$ est une terme de récursion, et qu'il existe une constante $c$ (et $H$ ?) telles que la sortie $s' = \Gamma(s)$ peut être extraite de $(s.n)^K$ \footnotemark et $g : cn^K \to cn^K$.
				
				\footnotetext{Donc on a clairement besoin d'une fonction $i \mapsto n^K$. Sauf qu'il faut en limiter l'accès pour éviter de s'en servir de manière à obtenir des nombres trop gros ? Ou ce n'est pas notre problème parce que de toute façon, si le calcul dépasse les bornes imposées, on n'est plus dans la même classe de complexité ?}
			
			Au lieu de définir formellement la RAM pour $\Gamma$, on va donner un algorithme qui pourra facilement être converti en une RAM à plusieurs mémoires, puis en une RAM à une mémoire\footnotemark. 
			
				\footnotetext{\redtext{Est-ce toujours vrai pour une RAM polynomiale ?}}

			
			
			Sa structure de données associée sera constituée de variables $p$, $x$ et, en plus de la structure d'entrée $s$ et de la structure de sortie $s'$, de quatre tableaux à une dimension $\Fin, G, \Ginv$ et $EP$. 
			
			Dans $p$, on va stocker $c(s.n)^K$\footnotemark qui borne le domaine de $g$, définie par $E$ et $s$.
				\footnotetext{Du coup on n'a plus besoin de la fonction $i \mapsto n^K$ ? Dans l'article, on a déjà accès à $cs.n$ pour simuler la machine. D'où vient cette connaissance ?}
			Ensuite, on a décrire le sens de $F_{\text{in}}, G, G_{\text{inverse}}$ et $EP$. Pour des indices plus grands que $s.n$, $F_{\text{in}}$ vaut $0$; pour les indices plus petits que $s.n$, $\Fin$ contient la valeur de $s.f$. Le calcul principal se fait en $p$ étapes, numérotés de $0$ à $p-1 = cn^K-1$.
			
			Après le tour n°$i$, on devrait avoir les invariants suivants : 
			
			\begin{enumerate}[itemsep=-1mm,leftmargin=1cm,label=(\alph*)]
				\item	pour tout $j < p$, $G[j]= \casedist{
													g(j) 	& 	\text{si $j \leqslant i$} \\ 
													j		&	\text{si $i < j < p$}
											}$ ;
				\item 	$EP[j] = g^{\leftarrow}(j)$ pour chaque $j \leqslant i$ ;
				\item 	pour tout $j < p$, $\Ginv[j] = \casedist{
													\maxset{l \leqslant i | g(l) = j} 	& 	\text{si un tel $l$ existe} \\
													p									& 	\text{sinon}
											}$ .
			\end{enumerate}
			
			Le calcul des valeurs de $G[i]$ est assez immédiat. On associe à chaque terme de récurrence $\sigma(x)$ un terme de programmation $\sigma[x]$ de la façon suivante : 
			
			\begin{itemize}[itemsep=-1mm]
				\item 	Si $\sigma(x)$ est $1, n, x$ alors $\sigma[x]$ est $1, n, x$, respectivement ;
				
				\item 	Si $\sigma(x) = g^{\leftarrow}(x - \delta)$ pour une certain $\delta$, alors $\sigma[x] = EP[x - \delta]$ ;
				
				\item 	Si $\sigma(x) = g[\tau(x)]_x$ pour un certain terme de récurrence $\tau(x)$, alors $\sigma[x] = G[\tau[x]]$ ;
				
				\item 	Si $\sigma(x) = f_{\text{in}}(\tau(x))$ pour un certain terme de récurrence $\tau(x)$, alors $\sigma[x] = \Fin[\tau[x]]$ ;
				
				\item 	Si $\sigma(x) = \tau_1(x) * \tau_2(x) $ pour des termes de récurrence $\tau_1(x)$ et $\tau_2(x)$, alors $\sigma[x] = \tau_1[x] * \tau_2[x]$.
			\end{itemize}
			
			Au tour $i$, on calcule $G[i]$ en évaluant $\sigma[i]$. La seule difficulté est de bien évaluer les termes $g^{\leftarrow}(x-\delta)$, qui devrait prendre plus qu'un nombre constant d'étapes pour être évalué, suivant la méthode directe. Au lieu de faire ça, on va utiliser le tableau $EP$ qui contient, après le tour $i$, la valeur de $g^{\leftarrow}(j)$ pour chaque $j \leqslant i$. $\Ginv$ contient toujours une inverse partielle de $g$ sur $\intint{0}{j}$ et est utilisé pour calculer $EP$. 
			
			Voici l'algorithme pour $\Gamma$ :
			
			\begin{algorithm}[H]
				\KwIn{s}
				
				\tcp{Initializations}
				
				$p := c(s.n)^K$ \;
				$EP[0] := 0$ \;
				\For{$j=0 \text{ to } s.n-1$}{
					$\Fin[j] := s.f(j)$\;
				}
				\For{$j=s.n \text{ to } p-1$}{
					$\Fin[j] := 0$\;
					}
				\For{$j = 0 \text{ to } p-1$}{
					$G[j] := j$\;
					$\Ginv[j] := p$ \;
					}
				
				\tcp{Main loop}
				
				\For{$i=0 \text{ to } p-1$}{
					$G[i] = \sigma[i]$ \;
					$EP[i] := \minset{i, \Ginv[G[i]]}$ \;
					$\Ginv[G[i]] := i$ \;
					}
					
				\KwOut{Compute $s''$ by applying the affine projection $P_{(s.n)^K}$ to the 	structure $s' = (p, G)$}
			\end{algorithm}
			
			On montre par induction que les invariants (a), (b), (c) sont maintenus pendant chaque étape de calcul. Bien évidemment, les invariants sont respectés après l'initialisation, c'est-à-dire, avant le tour $0$. 
			Pour l'induction, supposons que les invariants (a), (b), (c) sont respectés \emph{avant} le tour $i \in p$. On montre qu'ils sont valides \emph{après} le tour $i$, et donc avant le tour $i+1$. 
			
			\begin{itemize}
				\item 	On a $G[i] = g(i)$ parce que, par induction, $G[j] = g[j]_i$ pour tout $j \in p$, et $EP[j] = g^{\leftarrow}(j)$ pour tout $j \in i$ ; ainsi, l'opérateur d'application bornée et la fonction \emph{Equal-Predecesor} sont évaluées correctement.
				
				\item 	L'assignation $EP[i] := \minset{i, \Ginv[G[i]]}$ implique que $EP[i] = g^{\leftarrow}(i)$ par induction.
				
				\item 	Il est immédiat de voir que (c) est maintenu par l'assignation $\Ginv[G[i]] := i$. 
			\end{itemize}
			
			Donc (a), (b) et (c), et en particulier (a), sont maintenus, donc le programme est correct.
			
			Le nombre d'étapes pour évaluer un terme de récurrence est linéaire en la longueur du terme. Comme le terme de récurrence de $E$ est fixé, il s'agit d'un nombre constant d'étapes. Ainsi, $G[i]$ n'a besoin que d'un nombre constant d'étapes, de sorte que le programme tourne en temps $\grozo{p} = \grozo{n^K}$.
			
}
			
		\end{proof}
		
		\pagebreak
		
	\subsection{Calculable en temps polynomial $\Rightarrow$ LSRS}
		\label{subsec:poly_implique_LSRS}
		
		\begin{conj}
			\label{conj:poly_implique_LSRS}
			Si une fonction de RAM $\Gamma$ est calculable en temps $\grozo{n^K }$ sur une $\{+, -\} \footnotemark$-RAM $M$, alors $\Gamma$ est $n^K$-représentable par un LSRS.
		
				\footnotetext{On aura probablement besoin de l'opération $\times$ si on a besoin d'avoir la fonction $i \mapsto n^K$.}
		\end{conj}
		
		\begin{proof}
			Soit $\Gamma$ une fonction de RAM. Pour des raisons de simplicité, on va supposer que les types d'entrée et de sortie de la fonction n'ont qu'un seul symbole de fonction $f$. Soit $M$ une RAM calculant $\Gamma$ comme stipulé dans les hypothèses. Soit $c$ tel que le temps de calcul est borné par $c n^K$ sur des entrées $c(\Gamma)$-bornées\footnotemark de taille $n$.
			
				\footnotetext{Je ne comprends pas si ce $c(\Gamma)$ est le même que celui qu'on vient d'instancier.}
				
			On va construire un LSRS qui utilise des fonctions $I, A, B, R_A, N$, qui décrivent l'état courant de la RAM \emph{avant} chaque étape $x$, de la façon suivante :
			
			\begin{itemize}[itemsep=-1mm]
				\item	$I(x)$ contient le numéro courant d'instruction du programme de la RAM ;
				\item 	$A(x), B(x), N(x)$ contiennent les valeurs des registres $A, B, N$ de $M$ ;
				\item 	$R_A(x)$ contient la valeur du registre dont l'adresse est actuellement dans le registre $A$.
			\end{itemize}
			
			Par commodité, on va aussi utiliser des fonctions $I', A', B', R_A', N'$, qui décrivent l'état de la RAM \emph{après} l'étape $x$. 
			
			En définissant $I(x)$ par distinction de cas, une bonne partie de la simulation de la RAM est immédiate. Par exemple, si $M$ exécute $A:= A + B$ alors les équations doivent forcer $A'(x) = A(x) + B(x)$. Le principal problème réside dans l'instruction $A:= R_A$, qui récupère le contenu d'un registre. Notons que les fonctions définies dans $S$ n'encodent pas explicitement les valeurs de tous les registres de $M$ à chaque étape $t$ mais seulement le contenu du registre dont l'indice est contenu dans $A$. Pour obtenir la valeur de $R_A(t)$, on doit trouver le dernier instant avant $t$ auquel le registre $A$ avait la valeur $A(t)$. Si un tel instant n'existe pas, on doit se référer à l'entrée. Rappelons que $s.f(i)$ est stocké dans $R_i$ au début du calcul. C'est pour cela que l'opération de récursion entre en jeu\footnotemark.
			
				\footnotetext{C'est l'opération $g, g' \mapsto \eqpred{g'}{g}{x}$}
			
			Pour faciliter les instructions $A:=R_A$, on va diviser le domaine en trois sous-domaines : $\intint{0}{cn^K-1}$, $\intint{cn^K}{2cn^K-1}$ et $\intint{2cn^K}{3cn^K-1}$.
			On va utiliser le premier intervalle pour stocker $s.f$, le deuxième pour simuler le calcul de $M$, et le troisième pour extraire la fonction de sortie ; pour ce faire, on va utiliser $R_A$ de manière à encoder la sortie (c'est un usage différent de celui introduit).
			
			%			\footnotemark  <= après "on va diviser le domaine en trois"
				%\footnotetext{\label{astuce_cnK}En fait, le $n^K$ doit être explicité, et on peut le faire, même si cette preuve le passe sous silence pour des raisons de clarté. On peut calculer $cn^K$ en rajoutant quelques équations : $c$ et $K$ étant fixés, si on a la multiplication, il suffit de rajouter $K$ équations $f_1(x) = n(x)$ et $f_{i+1}(x) = f_i(x) \times n(x)$, puis $c-1$ équations $f_{K+j+1}(x) = f_{K+j}(x) + f_{K}(x)(x)$ ; on obtient alors $cn^K$ dès le premier tour de calcul. En revanche, si on n'a que l'addition, on ne peut pas reproduire ce schéma. On peut envisager d'étudier $g$ non pas sur $\intint{0}{3cn^K-1}$ mais sur $\intint{0}{4cn^K-1}$, et de couper ce domaine en quatre ; le premier domaine servirait alors à calculer $cn^K$ (mais comment délimiter ce premier domaine explicitement, avec un LSRS ?) et le reste serait le même que présenté ici. }
			
			On va s'autoriser la définition par cas (on a vu dans un lemme précédent que ça ne rendait pas le LSRS plus puissant) et quelques opérations simples qui ne sont pas directement disponibles dans la définition d'un LSRS. Le système qu'on va proposer n'est pas un LSRS à proprement parler, mais sa traduction en véritable LSRS est immédiate, quoiqu'elle nécessite quelques fonctions en plus.
			
			Pour simplifier la présentation, on va présenter les définitions des fonctions sur les trois intervalles ; elles peuvent être combinées via un LSRS. 
			
			Dans un LSRS, l'ordre est important. Ici, les fonctions doivent être présentées dans cet ordre : $I$, $A$, $B$, $N$, $R_A$, $I'$, $A'$, $B'$, $N'$, $R_A'$.
			
			Dans l'article d'origine \cite{GrandjeanSchwentick2002} de ce brouillon, les auteurs prennent la convention que l'addition est bornée ; si $a+b > cn$ où $cn$ est la taille du domaine, alors $a+b$ est en fait évalué à $0$ dans le LSRS. On peut utiliser cette convention pour simuler une définition par cas sur la taille entière du domaine. Si le LSRS est défini sur $3cn^K$ alors le test $x < cn^K$ peut se simuler avec le test : $x=0$ ou $x + x + x >0$.
			
			Ainsi, avant de définir les autres fonctions, intéressons-nous à un moyen d'obtenir $cn^K$ dans le LSRS, ce qui nous sera fort utile pour la suite.
			
			\begin{equation}
				f_0(x) = \casedist{
					1 & \text{ si $x = 0$} \\
					x + 1 & \text{ si $3x >0$} \\
					f_0(x-1) & \text{ sinon}
				}
			\end{equation}
			
			
			La définition de $f_0$ est telle que $f_0(cn^K-1) = cn^K$ et pour $x>cn^K$, $f_0(x) = cn^K$. 
			
			On peut utiliser cette astuce pour définir, en même temps que la phase d'initialisation, une fonction qui permet d'obtenir $cn^K$. Si $x<cn^K$, c'est-à-dire, $x=0$ ou $x + x + x >0$, on définit $A$ et $R_A'$ par :
			
			
			%Pour $x<cn^K$, $A$ et $R_A$ sont définies par\footnotemark :
				
			%	\footnotetext{On pourrait profiter de cette initialisation pour définir une fonction qui va calculer }
				
			\setcounter{equation}{0}
			\begin{eqnarray}
				A(x) & = & x \\
				R_A'(x) & = & f(x)
			\end{eqnarray}
			
			Les autres fonctions valent $0$ sur cet intervalle, sauf $f_0(x) = x+1$. Pour l'instant, on n'utilise pas $f_0(x)$, qui de toute façon n'est pas prête à l'emploi.
			
			 Cette initialisation va permettre de récupérer les valeurs de $R_A$ dans la deuxième partie du domaine. 
			
			\espace
			
			Sur le domaine $\intint{cn^K}{2cn^K-1}$ : 
			
			\espace 
			
			\fbox{
				\begin{minipage}{0.9\textwidth}
					
				\setcounter{equation}{0}
				\begin{eqnarray}
					I(x) & = & \casedist{
							1			& 	\text{si $x = c n^K$} \\
							I'(x-1)		& 	\text{sinon}
							} \\
					I'(x) & = & \casedist{
							i			& 	\text{si $I(x)$ est de la forme $\sRAMif{i}{j}$ et $A(x) = B(x)$} \\
							j			& 	\text{si $I(x)$ est de la forme $\sRAMif{i}{j}$ et $A(x) \neq B(x)$} \\
							I(x)		&  	\text{si $I(x)$ est de la forme $\text{HALT}$} \\
							I(x) + 1 	& 	\text{sinon} 
							} \\
					A(x) & = & \casedist{
							0			& 	\text{si $x = cn^K$} \\
							A'(x-1)		& 	\text{sinon}
							} \\
					A'(x) & = & \casedist{
							c			& 	\text{si $I(x)$ est de la forme $A := c$} \\
							A(x) * B(x)	& 	\text{si $I(x)$ est de la forme $A := A + B$} \\
							R_A(x) 		& 	\text{si $I(x)$ est de la forme $A := R_A$} \\
							N(x)		& 	\text{si $I(x)$ est de la forme $A := N$} \\
							A(x)		& 	\text{sinon}
							} \\
					B(x) & = & \casedist{
							0			& 	\text{si $x = cn^K$} \\
							B'(x-1)		& 	\text{sinon}
							} \\
					B'(x) & = & \casedist{
							A(x)		& 	\text{si $I(x)$ est de la forme $B := A$} \\
							B(x)		& 	\text{sinon}
							} \\
					N(x) & = & \casedist{
							n			& 	\text{si $x = cn^K$} \\
							N'(x-1)		& 	\text{sinon}
							} \\
					N'(x) & = & \casedist{
							A(x)		& 	\text{si $I(x)$ est de la forme $N := A$} \\
							N(x)		& 	\text{sinon}
							} \\
					R_A(x) & = & \eqpred{R_A'}{A}{x} \\
					R_A'(x) & = & \casedist{
							B(x)		& 	\text{si $I(x)$ est de la forme $R_A := B$} \\
							R_A(x)		& 	\text{sinon}
							}
				\end{eqnarray}
			
				\end{minipage}
				}
			
			\espace 
			
			Notons que $cn^K$ code le premier instant du calcul. 
			
			Les fonctions $I(x)$ et $I'(x)$ ne prennent qu'un nombre fini de valeurs. Ainsi, en écrivant \emph{si $I(x)$ est de la forme $R_A := B$}, on écrit en fait \emph{si $I(x) = i_1$ ou $I(x) = i_2$ \dots ou $I(x) = i_m$} où $i_1, \dots, i_m$ sont les numéros des instructions $R_A := B$ dans le programme de la RAM.
			
			De plus, les opérations $I'(x-1)$ sont en réalité écrites $\eqpred{I}{1}{x}$ dans un vrai LSRS. Comme on l'a précédemment dit, on utilise $R_A$ pour extraire les valeurs de sortie du calcul de $M$, donné par la valeur $n'$ de $N$ et le contenu $R(0), \dots, R(n'-1)$, à la fin du calcul. Pour $2cn^K \leqslant x < 3cn^K$, le LSRS consiste en les équations suivantes :
			
			\setcounter{equation}{0}
			\begin{eqnarray}
				A(x) & = & x - 2cn^K \\
				R_A(x) & = & \eqpred{R_A'}{A}{x}
			\end{eqnarray}
			
			Les autres fonctions du LSRS sont définies par $h(x) = h(x-1)$ (les fonctions stationnent).
			
			Il ne reste plus qu'à voir que les équations de $S$ définissent les fonctions attendues pour décrire le calcul de $M$ et que $S$ produit la bonne sortie.
			
			Soit $s$ la RAM-structure, $1(-), n(-), id(-)$ \footnotemark comme précédemment, et soient $I, A, B, N, R_A, I', A', B', N', R_A'$ des fonctions qui vérifient les équations de $S$.
				\footnotetext{On obtient la fonction $i \mapsto cn^K$ avec une astuce, cf \hyperref[astuce_cnK]{cette note}.}
				
			Premièrement, il est clair que pour $a < c(s.n)^K$, on a :
			
			\setcounter{equation}{0}
			\begin{eqnarray}
				A(a) & = & a \\
				R_A'(a) & = & s.f(a) \\
				I(a) & = & B(a) = N(a) = R_A(a) = I'(a) = A'(a) = B'(a) = N'(a) = 0
			\end{eqnarray}
			
			Les valeurs des fonctions au point $c(s.n)^K$ décrivent la configuration de la RAM au début du calcul. De plus, la définition de $A$ et $R_A'$ sur la première partie du domaine permet de rendre compte du faire que $M$ contient $s.f(i)$ dans le registre $R_i$.
			
			On montre facilement par induction sur $t$ que la valeur des fonctions au temps $c(s.n)^K+t$ sont correctes. La partie la plus difficile réside dans le calcul de $R_A$. Remarquons premièrement que, puisque $A(a) < c(s.n)^K$ pour chaque $a$, l'initialisation sur la première partie du domaine assure que $A^{\leftarrow}(a) < a$ pour tout $a \in \intint{cn^K}{2cn^K-1}$. Deux cas possibles :
			
			\begin{itemize}[itemsep=-1mm]
				\item 	
					$c(s.n)^K \leqslant A^{\leftarrow}(a) < a$. Dans ce cas, il existe $b \in \intint{c(s.n)^K}{a-1}$ tel que $A(a) = A(b)$ ; donc le registre courant $R$ a déjà été visité pendant le calcul, et $A^{\leftarrow}(a)$ est la dernière étape où cela s'est produit ; ainsi $R_A'\left(A^{\leftarrow}(a)\right)$ donne la bonne valeur de $R_A(a)$.
					
				\item 
					$A^{\leftarrow}(a) < c(s.n)^K$. Dans ce cas, il n'y a pas de $b \in \intint{c(s.n)^K}{a-1}$ tel que $A(a) = A(b)$ ; donc le registre courant $R$ n'a pas été visité pour le moment, et devrait toujours contenir la valeur de $f(A^{\leftarrow}(a))$\footnotemark. Par initialisation, il découle que $A^{\leftarrow}(a) = A(a)$ et $R_A'(A(a)) = f(A(a))$, comme voulu.
					
						\footnotetext{Petite erreur de frappe dans l'article original ? Il y est écrit : $f(A(a))$.}
			\end{itemize}
			
			Enfin, on doit encore montrer que $S$ définit correctement $\Gamma(s)$. Par définition de $R_A$ et $N$ sur la troisième partie du domaine du LSRS, pour chaque $a$, $R_A\left( 2 c(s.n)^K + a \right)$ contient les valeurs de du registre $R_a$ à la fin du calcul. De plus, $N(3c(s.n)^K-1)$ contient la valeur de $N$ à la fin du calcul. Ainsi, avec une projection bien choisie, $\Gamma(s)$ peut être extraite des fonctions définies par $S$. 
			
			Ainsi, on a montré que $S$ calcule correctement $\Gamma(s)$. Donc $\Gamma$ est $n^K$-représentée par $S$.
			
		\end{proof}

		
		
		
		
	\pagebreak
	 
	\pagebreak
		
	\chapter{LSRS à arité multiple}
	\label{chap:rLSRS}
	
	\section{Introduction}
	\label{sec:rintroduction}
	
	Avant toute chose, il peut être bon de rappeler à quoi ressemblent les LSRS. \cite{Schwentick1997} \cite{GrandjeanSchwentick2002}. Cependant, pour les besoins du présent chapitre, nous allons ajouter une troisième forme d'opération.
	
	
	\begin{definition}[LSRS]
		\label{def:LSRS_2}
		Soit $F$ un ensemble de symboles de fonctions (dites \emph{fonctions de base}), soient $f_1, \dots, f_k$ des symboles de fonctions qui n'apparaissent pas dans $F$. Pour $i\leqslant k$, notons $F_i = F\cup \{f_1, \dots, f_i\}$. 
	
		Un LSRS (\emph{Linear Simultaneous Recursion Scheme}) $S$ sur $f_1, \dots, f_k$ et $F$ est une suite de $k$ équations $\left(E_i\right)_{i_\in k}$ dont chacune est de l'une des trois formes suivantes :
		
		\begin{itemize}[itemsep=-1mm]
			\item 	(opération) 		$f_i(x) = g(x) * g'(x)$ où $g,g' \in F_{i-1}$ et $* \in \{+, -, \times \}$\footnotemark
			
			\footnotetext{Les opérations peuvent varier, il est dit dans \cite{GrandjeanSchwentick2002} que l'on peut choisir n'importe quelle opération binaire calculable en temps linéaire sur machine de Turing, voire la multiplication. Nous verrons d'ailleurs par la suite que, pour des raisons de facilité, nous aurons besoin de la multiplication.}
			
			\item 	(récursion)			$f_i(x) = \eqpred{g'}{g}{x}$ où $g' \in F_k$ et $g \in F_{i-1}$			
			
			\item 	(composition)			$f_i(x) = g'\left[g(x)\right]_x$ où $g' \in F_k$ et $g \in F_{i-1}$
		\end{itemize}
	\end{definition}
	
	\begin{remark}
		L'ajout de cette opération facilitera les calculs qui suivront. Elle ne rend pas le LSRS plus puissant \cite{GrandjeanSchwentick2002}.
	\end{remark}
	
	Le but de ce chapitre est d'étendre les LSRS définis dans \cite{GrandjeanSchwentick2002} et utilisés \hyperref[chap:LSRS]{plus haut} aux fonctions d'arité $>1$. Remarquons dans un premier temps que si l'on définit un LSRS avec des fonctions d'arité $a$, où $a$ est le même pour chaque fonction, alors l'ordre lexicographique permet de facilement définir un LSRS équivalent d'arité $1$, ce qui n'apporte pas grand-chose par rapport à ce dont nous avons déjà parlé.
	
	Ici, on ne va pas considérer de LSRS dont toutes les fonctions sont de même arité. Soit $a>1$, on suppose que le LSRS est composé de fonctions $f_i$ d'arité $r_i \leqslant a$, où $i \in k$, et telles que $r_i = a$ pour au moins un $i\in k$. On qualifie de $a$-LSRS un tel LSRS. On notera LSRS les systèmes utilisés dans le chapitre précédent, qui sont en fait des $1$-LSRS. 
	
	Dans la suite, on notera $\leqa$-uplet pour parler d'un $n$-uplet où $n\leqslant a$. 
	
	
	\section{Les ennuis commencent}
	\label{sec:ennuis_commencent}
	
		\subsection{Bon ordre sur les $\leqa$-uplets}
		\label{subsec:bon_ordre_uplets}
		
		Dans un LSRS, l'ordre est très important ; que ce soit au niveau de l'ordre des fonctions ou des variables. De manière générale, dans un $1$-LSRS, $f(x)$ ne peut être défini avec $g(y)$ que si $g(y)$ a été calculé avant, que ce soit parce que $y<x$ ou, si $x=y$, parce que $g$ est définie avant elle dans le LSRS. 
		Pour reproduire l'importance de l'ordre sur des fonctions d'arité multiple, et ne pas trahir la définition originelle du LSRS, on peut utiliser un ordre lexicographique sur les $\leqa$-uplets tels que $\bar{x} <_{\text{naïf}} \bar{y} \Leftrightarrow \left|\bar{x}\right| < \left|\bar{y}\right| \text{ ou } \bar{x} <_{\text{lex}} \bar{y}$. 
			
		Cela revient à calculer, dans l'ordre, d'abord les fonctions d'arité $1$, puis ensuite toutes les fonctions d'arité $2$, etc. jusqu'à calculer les fonctions d'arité $a$. Le problème de cette définition est que les fonctions d'arité $a'$ ne peuvent faire appel qu'aux fonctions d'arité $<a'$, ce qui ressemble à une contrainte inutile\footnotemark.
		
			\footnotetext{Il y avait de plus une autre raison, mais je ne m'en souviens plus au moment où j'écris ces lignes. C'est apparemment un bon ordre (\emph{tout ensemble non vide a un plus petit élément}), mais il y avait un contre-argument qui nous avait fait chercher un autre ordre. Parce qu'on atteint très vite les bornes du domaine ?}
		
		Nous avons choisi un ordre moins naturel, mais qui est un bon ordre, permet de faire des projections (récupérer des éléments d'un $a'$-uplet, sauf au moins un, et les utiliser comme argument d'une autre fonction), et permet de calculer tour à tour des fonctions d'arité $1$, puis $2$, etc., puis revenir aux arités $1$, $2$...
		
		\begin{definition}
			\label{def:bon_ordre_sur_uplets}
			On définit $<$ sur les $\leqa$-uplets par :
			
			\[
				\bar{x} < \bar{y} \Leftrightarrow \left\lbrace
														\begin{array}{ccc}
															\max\left(\bar{x}\right) < \max\left(\bar{y}\right) & & \\
															\text{ou } \max\left(\bar{x}\right) = \max\left(\bar{y}\right) & 
																\text{ et } \left|\bar{x}\right| < \left|\bar{y}\right| & \\
															\text{ou } \max\left(\bar{x}\right) = \max\left(\bar{y}\right) & 
																\text{ et } \left|\bar{x}\right| = \left|\bar{y}\right| & 
																\text{ et } \bar{x} <_{\text{lex}} \bar{y}\\
														\end{array}
													\right. 
			\]
			
			Si toutes les arités $\leqslant a$ ne sont pas permises (par exemple, si notre LSRS ne contient que des fonctions d'arité $1$ et $3$, mais pas $2$), alors on ampute cet ordre des $n$-uplets correspondant.
		\end{definition}
	
	
		\begin{example}
			Pour $a = 3$, avec les arités $1,2,3$ : $(0) < (0, 0) < (0,0,0) < (1) < (0,1) < (1,0) < (1,1) < (0,0,1) < (0,1,0) < (0,1,1) < (1,0,0) < (1,0,1) < (1,1,0) < (1,1,1) < (2) < (0,2) < \dots$.
			
			Pour $a = 3$, avec les arités $1,3$ : $(0) < (0,0,0) < (1) < (0,0,1) < (0,1,0) < (0,1,1) < (1,0,0) < (1,0,1) < (1,1,0) < (1,1,1) < (2) < (0,0,2) <\dots$.
		\end{example}
		
		\begin{remark}
			Quelques remarques informelles : 
			\begin{itemize}
				\item 	Le premier $\leqa$-uplet d'un max $m$ donné est le $1$-uplet $\left( m \right)$.
				\item 	Le premier $\leqa$-uplet d'une arité $r$ donnée, à un max $m$ donné, est le $r$-uplet $\left( 0, \dots, 0, m\right)$. 
			\end{itemize}
		\end{remark}
		
		
		
		\subsection{Propriétés combinatoires}
		\label{subsec:bon_ordre_prop_combinatoires}
		
		\begin{prop}
			\label{prop:bon_ordre_combinatoire}
			Soit $\bar{x}$ un $\leqa$-uplet. On suppose que toutes les arités sont possibles. Notons $m = \maxs{\bar{x}}$ et $r = \abs{\bar{x}}$.
			
			Le bon ordre $<$ sur les $\leqa$-uplets vérifie les égalités suivantes :
			
			\begin{enumerate}
				\item 	\label{itm:bon_ordre_combinatoire1} $\card{\bar{y} < \bar{x} | \maxs{\bar{y}} < \maxs{\bar{x}}} = \sum_{i=1}^{a} m^i$\footnote{J'ai mis $-1$ ici au début, et j'avais l'air convaincu, mais je ne sais plus pourquoi...} ;
				\item 	\label{itm:bon_ordre_combinatoire2} $\card{\bar{y} < \bar{x} | \maxs{\bar{y}} = \maxs{\bar{x}} \wedge \abs{\bar{y}} < \abs{\bar{x}} } = \sum_{i=1}^{r} \left( \left(m+1\right)^i -1 \right)$ ; 
				\item 	\label{itm:bon_ordre_combinatoire3} $\card{\bar{y} < \bar{x} | \maxs{\bar{y}} = \maxs{\bar{x}} \wedge \abs{\bar{y}} = \abs{\bar{x}} \wedge \bar{y} <_{\text{lex}} \bar{x} } = \sum_{i=1}^{r-1} x_i m^{r-i}$. \redtext{PROBABLEMENT FAUX parce qu'il faut être sûr de garder un $m$ quelque part dans le $r$-uplet.}
			\end{enumerate}
		\end{prop}
		
		\begin{proof}
			\ref{itm:bon_ordre_combinatoire1}. L'entier $a$ étant fixé, pour un max $m$ donné, on peut construire un $\leqa$-uplet en choisissant une arité $i \leqslant a$ et $i$ composantes, toutes des entiers $< m$. Pour $i$ fixé, on peut faire $m^{i}$ $i$-uplets, d'où au total $\sum_{i=1}^{a} m^i$.
			
			\ref{itm:bon_ordre_combinatoire2}. Pour construire un tel $\bar{y}$, il faut choisir son arité $i<r$, puis le nombre de composantes que l'on fixe à $m$ ; le reste des composantes est libre et $<m$, d'où le résultat.
			
			\ref{itm:bon_ordre_combinatoire3}. \redtext{Je dois y réfléchir.}
		\end{proof}
		
		\begin{coro}
			\label{coro:rang_bon_ordre}
			Le rang de $\bar{x}$ est $\left( \sum_{i=1}^{a} m^i \right) + \left( \sum_{i=1}^{r} \left( \left(m+1\right)^i -1 \right) \right) + \text{\redtext{quelque chose}}$.
		\end{coro}
		
	
	\section{Déroulement d'un calcul de $a$-LSRS}
	\label{par:deroulement_aLSRS}
	Au lieu d'incrémenter $x$ de façon naturelle en $x+1$, ici, on compte passer de $\bar{x}$ à son successeur dans cet ordre. Notons-le $s\left(\bar{x}\right)$. Ainsi, le calcul d'un $a$-LSRS se déroule de la façon suivante :
	
	\begin{itemize}[itemsep=-1mm]
		\item 	$\bar{x} = (0)$ : calcul de toutes les fonctions d'arité $1$ en $(0)$, comme dans un LSRS normal. Dans cette première étape, les fonctions ne peuvent bien sûr pas faire référence aux fonctions d'arité supérieure, car elles ne sont même pas encore définies ; leur évaluation doit respecter les règles du LSRS énoncées plus haut. 
		\item 	$\bar{x} = (0,0) = s\left( \left( 0 \right)\right)$ : calcul de toutes les fonctions d'arité $2$ en $(0,0)$. Ces fonctions peuvent faire appel aux valeurs des fonctions d'arité $1$ en $0$. Une fonction $f_i$ d'arité $2$ peut aussi faire référence à des fonctions d'arité $2$ en $(0,0)$, à condition qu'elles aient déjà été évaluées ; c'est-à-dire, à condition qu'elles soient définies dans des équations $E_j$ telles que $j<i$ ;
		\item  	$s\left(\bar{x}\right)$ :
			\begin{itemize}[itemsep=-1mm]
				\item 	Si $r = \abs{\bar{x}} = \abs{s\left(\bar{x}\right)}$ alors on calcule à nouveau les fonctions d'arité $r$ pour $s\left(\bar{x}\right)$ ; ces fonctions peuvent faire appel aux résultats des fonctions d'arité plus petite. 
				\item 	Si $r = \abs{\bar{x}} < \abs{s\left(\bar{x}\right)} = r'$ alors on évalue les fonctions d'arité $r'$ en $s\left(\bar{x}\right)$ ; idem, ces fonctions peuvent faire appel aux résultats des fonctions d'arité plus petite.
				\item 	Si $\abs{s\left(\bar{x}\right)} = 1 < a = \abs{\bar{x}}$ alors on évalue les fonctions d'arité $1$ en $s\left(\bar{x}\right)$ ; ces fonctions d'arité $1$ peuvent faire référence aux résultats des fonctions d'arité plus grande, à condition qu'ils aient été calculés au tour précédent. 
			\end{itemize}
	\end{itemize}
	
Une fois l'ordre compris, il faut voir comment interpréter les opérations typiques du LSRS : addition, soustraction, multiplication, récursion et composition.


		\paragraph{Définissons le $a$-LSRS}
		
		Ce qui suit est la piste principale que nous suivons en ce moment. Le LSRS pourrait être élargi de bien des manières, mais c'est celle-ci qui nous semble la plus intéressante et la plus naturelle, une fois que l'on a choisi comment ordonné les $\leqa$-uplets.
		
		Soit $a \in \naturels$. Le symbole "$<$" renvoie selon les cas à l'ordre sur les entiers ou à l'ordre \hyperref[def:bon_ordre_sur_uplets]{juste au-dessus}. Pour des raisons de lisibilité, on notera $\bar{x'} \ll \bar{x}$ pour dire :  $\bar{x'} < \bar{x}$, $\abs{\bar{x'}} < \abs{\bar{x}}$ et $\forall j \: \exists j' \:\: x'_j = x_{j'}$\footnotemark.
		
			\footnotetext{Autrement dit : $\bar{x'}$ est obtenu à partir de $\bar{x}$ en récupérant ses composantes, en les mélangeant, en les dupliquant, mais en veillant à ce que $\abs{\bar{x'}} < \abs{\bar{x}}$. Par exemple, pour $\bar{x} = (x_1, x_2, x_3)$, les $\bar{x'}$ pourraient être $(x_1, x_2)$, $(x_3, x_1)$ ou $(x_3, x_3)$.}
		
		\begin{definition}[$a$-LSRS]
			\label{def:aLSRS}
			Soit $F$ un ensemble de symboles de fonctions de base. Soient $f_1, \dots, f_k$ de nouveaux symboles de fonctions n'apparaissant pas dans $F$, d'arités respectives $1 \leqslant r_1 \leqslant r_2 \leqslant \dots \leqslant r_k = a$\footnotemark. 			
				\footnotetext{L'ordre entre des fonctions de même arité a de l'importance, mais pas entre des fonctions d'arités différentes. On peut donc considérer que les équations sont ordonnées par l'arité de la fonction qu'elles définissent.}
			On note $F_i = F \cup \{f_j | r_j = r_i \text{ et } j < i\}$, $F'_i = F \cup \{f_j | r_j = r_i\}$, et $G_i = F \cup \{ f_j | r_j < r_i\}$\footnotemark.
			
				\footnotetext{$F_i$ est l'ensemble des symboles des fonctions qui ont la même arité que $f_i$ mais qui sont définies avant $f_i$. 
					
					$F'_i$ est l'ensemble des symboles des fonctions qui ont la même arité que $f_i$, qu'elles soient définies avant ou après $f_i$.
					
					$G_i$ est l'ensemble des symboles des fonctions d'arité strictement inférieure à celle de $f_i$.}
				
			Un $a$-LSRS $S$ sur $F$ et $f_1, \dots, f_k$ est une suite d'équations $E_1, \dots, E_k$ où chaque $E_i$ est de l'une des formes suivantes :
			
			\begin{itemize}[itemsep=-1mm]
				\item	(opération) 	$f_i\left(\bar{x}\right) = A * B$ où $* \in \{+,-,\times\}$ et $A, B$ sont de la forme suivante : 
						\begin{itemize}[itemsep=-1mm]
							\item 	$g\left(\bar{x}\right)$, avec $g \in F_i$ ;
							\item 	$g\left(\bar{x'}\right)$, avec $g \in G_i$, c et $\bar{x'} \ll \bar{x'}$.
						\end{itemize}
									
				\item 	(récursion)		$f_i\left(\bar{x}\right) = \eqpred{g'}{g}{\bar{x'}}$, où $\text{arité}(g) = \text{arité}(g')$ et l'un des deux cas suivants se réalise :
						\begin{itemize}[itemsep=-1mm]
							\item 	Soit $\bar{x'} = \bar{x}$, et dans ce cas $g \in F_i$ et $g' \in F'_i$ ;
							\item 	Soit $\bar{x'} \ll \bar{x}$ et dans ce cas $g, g' \in G_i$. 
						\end{itemize}
						
				\item 	(composition)\footnote{Cette opération n'est pas forcément nécessaire ; je l'avais rajoutée vraisemblablement en me trompant quand à son usage. Je reviendrai probablement dessus plus tard.}
					$f_i\left(\bar{x}\right) = g'\left[ g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right) \right]_{\bar{x}}$, où $\text{arité}(g') = r$, et pour chaque $j \in r$, l'un des cas suivants se réalise : 
						\begin{itemize}
							\item 	Soit $\bar{x_j} = \bar{x}$, et dans ce cas $g_j \in F_i$ ;
							\item 	Soit $\bar{x_j} \ll \bar{x}$ et dans ce cas $g_j \in G_i$. 
						\end{itemize}
						De plus :
						\begin{itemize}
							\item 	Soit $r = \abs{\bar{x}}$, et dans ce cas $g' \in F_i$\footnotemark ;
							\item 	Soit $r < \abs{\bar{x}}$ et dans ce cas $g' \in G_i$. 
						\end{itemize}
						
							\footnotetext{Si $\bar{x} < \left(g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right)\right)$ il faut définir une valeur par défaut à $g'\left[ g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right) \right]_{\bar{x}}$. Dans le \hyperref[def:app_bornee_eq_pred]{LSRS normal}, on se contente de renvoyer $x$. On peut ici renvoyer une composante par défaut, par exemple $x_1$.}
			\end{itemize}
			
		\end{definition}
		
		\begin{remark}
			\label{rk:fonctions_de_base_LSRS}
			Par défaut, on suppose, une nouvelle fois, que l'ensemble des fonctions de base contient par défaut les symboles $n(-)$ et $1(-)$ pour toutes les arités possibles, ainsi que les $\pi^i_j(-)$, projections récupérant la $i$-ième composante d'un $j$-uplet. On peut de plus rajouter $a(-)$, permettant de récupérer $a$, quoique cette fonction est facilement calculable par un LSRS.
		\end{remark}
		
		On doit aussi adapter les notions de structures de RAM et de fonctions de RAM. On garde le même $a$ que pour le $a$-LSRS.
		
		\begin{definition}[RAM-structure]
			\label{def:RAM_data_structures_a}
			Soit $t$ un $a$-type, c'est-à-dire une signature fonctionnelle contenant des fonctions d'arité $\leqslant a$ et dont au moins un symbole est d'arité $a$. 
			
			Une RAM-structure $s$ de type $t$ est un uplet constitué de :
			\begin{itemize}[itemsep=-1mm]
				\item 	$n \in \naturels$ qui est la taille de la structure ;
				\item 	$C \in \naturels$ pour chaque symbole $C \in t$ ;
				\item 	$f : n\times \dots \times n \to \naturels$ pour chaque symbole $f \in t$.
			\end{itemize}
			
			On notera $s.n, s.C, s.f$ les composantes $n, C, f$ de $s$.
			
			On dira que $s$ est $c$-bornée pour $c\in\naturels$ lorsque $s.C, s.f\left(\bar{x}\right) < c s.n$ pour tous $C, f \in t$ et $\bar{x} \in n\times \dots \times n$.
		\end{definition}
		
		
		\begin{definition}[Fonction de RAM]
			\label{def:fonction_de_RAM_a}
			Soient $t_1, t_2$ des $a_1,a_2$-types. 
					
			Une $(t_1, t_2)$-fonction de RAM $\Gamma$ est une fonction telle qu'il existe $c_1, c_2 \in \naturels$, tels que $\Gamma$ envoie les structures $c_1$-bornées de type $t_1$ sur des structures $c_2$-bornées de type $t_2$\footnotemark.
			
				\footnotetext{On rappelle que "$c$-borné" ne concerne que la structure par rapport à sa propre taille ; ici on ne compare pas la taille de l'entrée et de la sortie}
			
			On dit que $\Gamma$ est polynomiale lorsque $\Gamma(s).n = \grozo{(s.n)^K}$, et linéaire lorsque $\Gamma(s).n = \grozo{s.n}$.
			
			% Là, si.
		\end{definition}
		
		
		\begin{definition}[RAM représentée par $a$-LSRS]
			\label{def:representee_par_aLSRS}
			Soient $t_1, t_2$ respectivement un $a_1$-type et un $a_2$-type. Soit $\Gamma$ une $(t_1, t_2)$-fonction de RAM.
			
			Soit $S$ un $a$-LSRS pour $f_1, \dots, f_k$ sur $F_{t_1}$. 
			
			On dit que $\Gamma$ est représentée par $S$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée, $S$ définit des fonctions $f_1, \dots, f_k : cs.n \times \dots \times cs.n \to (cs.n)^a$ telles que $\Gamma(s) = P((s.n)^a, S(s))$ ($S(s)$ est la structure définie par le $a$-LSRS).
		\end{definition}
		
		A priori, les $a$-LSRS prennent en entrée des structures dont l'arité maximale est au plus $a$, car les définitions que l'on a données ne permettent pas d'utiliser des fonctions dont l'arité est plus grande que $a$. 
		
		
		
		
		
	
		
	\section{Lien entre les deux notions de LSRS}
	\label{sec:lien_LSRS_aLSRS}
	
	L'idée principale de ce chapitre est de chercher un lien entre les LSRS et les $a$-LSRS.
	
	\begin{conj}
		Soit $\Gamma$ une $(t_1,t_2)$-fonction de RAM, où $t_1$ est un $1$-type et $t_2$ est un $a$-type.
		
		$\Gamma$ est représentable par un $a$-LSRS ssi $\Gamma$ est $n^a$-représentable par un LSRS.
	\end{conj}
		
	Pour prouver cette conjecture, nous allons avoir besoin de coder l'entrée.
	
	\begin{lemma}
		\label{lem:decomp_rang_par_LSRS}
		
		Il existe un $1$-LSRS $S$ qui, à une variable $x$, associe le $\leqa$-uplet $\bar{x}$ de rang $x$. 
	\end{lemma}
		
	\begin{proof}
		Pour simplifier, on va de nouveau se placer dans le cas où toutes les arités $\leqslant a$ sont représentées\footnotemark. On va se servir du codage du rang donné en proposition \ref{prop:bon_ordre_combinatoire}, montrer qu'on peut le décoder par LSRS.
		
			\footnotetext{Si certaines arités ne sont pas représentées, alors il suffit de retirer les quelques équations qui y sont associées et de modifier, le cas échéant, les initialisations de certaines fonctions.}
			
		\paragraph{Calcul du max $m$.}\label{par:calcul_max_bon_ordre}
			Considérons le LSRS suivant :
			
				\begin{eqnarray}
					f_1(x) & = & \casedist{	
									0 & \text{ si $x < a$} \\
									1 & \text{ si $x = a$} \\
									f_1(x-1) & \text{ si $x < f_{4a-1}(x-1)$} \\
									f_1(x-1)+1 & \text{ si $x = f_{4a-1}(x-1)$}
									} 
									\label{eqn:f_0_max_courant}\\
					f_{i_1 + 1}(x) & = & f_{i_1}(x) \times f_1(x) \label{eqn:m_pow_i}\\
					f_{a + 1}(x) & = & f_{1}(x) + f_{2}(x) \label{eqn:sum_m_pow_init}\\
					f_{a + i_2 + 1}(x) & = & f_{a + i_2}(x) + f_{i_2 + 2}(x) \label{eqn:sum_m_pow}\\
					f_{2a}(x) & = & f_1(x) + 1 \label{eqn:m_plus_1}\\
					f_{2a + i_3 + 1}(x) & = & f_{2a + i_3}(x) \times f_{2a}(x) \label{eqn:m1_pow_i}\\
					f_{3a}(x) & = & f_{2a}(x) + f_{2a + 1}(x) \label{eqn:sum_m1_pow_init}\\
					f_{3a + i_4 + 1}(x) & = & f_{3a + i_4}(x) + f_{2a + i_4+2}(x) \label{eqn:sum_m1_pow}
				\end{eqnarray}
		
			On rappelle que $f_1(x-1) = \eqpred{f_1}{1}{x}$ et que la distinction de cas ne rend pas le LSRS plus puissant \cite{GrandjeanSchwentick2002}. 
			
			Expliquons les équations.
			
			(\ref{eqn:f_0_max_courant}) est censée calculer $m$, donc le max courant. Sachant cela, les équations suivantes coulent de source : 
			
			\begin{itemize}
				\item 	(\ref{eqn:m_pow_i}) calcule $m^{i_1}$ pour $i_1 \in \intint{1}{a}$. On a aussi $f_{a}(x) = m^a$. 
				
				\item 	(\ref{eqn:sum_m_pow_init}) calcule $\sum_{i = 1}^{2} m^i$ et (\ref{eqn:sum_m_pow}) calcule $\sum_{i = 1}^{i_2+2} m^i$ pour $i_2+2 \leqslant a$. On a ainsi $f_{2a-1}(x) = \sum_{i = 1}^{a} m^i$, soit le rang du $1$-uplet $(m)$.
				
				\item 	Les équations suivantes sont sur le même schéma, si ce n'est qu'elles travaillent sur $m+1$ au lieu de $m$. On a $f_{3a-1}(x) = (m+1)^a$ et $f_{4a-1}(x) = \sum_{i = 1}^{a} (m+1)^i$, donc $f_{4a-1}(x)$ contient le rang du $1$-uplet du max suivant, à savoir $(m+1)$. 
			\end{itemize}
			
			Ceci étant explicité, étudions la définition de $f_1(x)$. 
			
			$f_1(x)$ s'initialise naturellement en $0$. On parcourt alors tous les $\leqa$-uplets $(0, \dots, 0)$ jusqu'à l'arité $a$, donc on énumère les $a$ premiers $\leqa$-uplets, et donc l'élément de rang $a$ a bien un max égal à $1$ : $f_1(x)$ passe à $1$. Quand le calcul avance, $x$ finit par atteindre le rang du max suivant, que l'on calcule par avance dans $f_{4a-1}(x)$. Une fois que ce rang est atteint, on incrémente $f_1(x)$ ; le reste des équations se met aussi à jour, ce qui assure que $f_{4a-1}(x)$ est toujours le rang du max suivant. La distinction de cas de $f_1(x)$ est donc bien complète.
			
		\paragraph{Calcul de l'arité courante}
			Le calcul de l'arité du $\leqa$-uplet de rang $x$ s'effectue avec la même idée. Pour des raisons de lisibilité, le système présenté ne sera pas un véritable LSRS mais on laisse le lecteur se convaincre qu'il est facile de réécrire ce système sous une véritable forme de LSRS en s'inspirant de ce qui a été fait \hyperref[par:calcul_max_bon_ordre]{au-dessus}, quitte à remanier la numérotation des fonctions. 
			
			\begin{eqnarray}
				f_{4a}(x) & = & \casedist{
									1 & \text{si $x = 0$} \\
									f_{4a}(x-1) & \text{si $x < f_{8a+2}(x-1)$}\\
									1 & \text{si $x = f_{4a-1}(x-1)$} \\
									f_{4a}(x-1) + 1 & \text{si $x = f_{8a+2}(x-1)$} 
									} \label{eqn:f_4a_arite_courante} \\
				f_{i_1+4a}(x) & = & \casedist{
										(m+1)^{i_1} - 1 & \text{si $1 \leqslant i_1 \leqslant f_{4a}(x) \leqslant a$} \\
										0 & \text{sinon}
										} \label{eqn:max_1_pow_1}\\
				f_{i_2+5a}(x) & = & \sum_{i=1}^{i_2} f_{i+4a}(x)\label{eqn:cumul_sum_max_1_pow_1} \\
					& = & \sum_{i=1}^{\min\left(f_{4a}(x), i_2\right)} \left((m+1)^{i} - 1\right) \nonumber\\
				f_{6a +1}(x) & = & \casedist{
										f_{4a}(x)+1 & \text{si $f_{4a}(x) < a$} \\
										1 	& \text{sinon}
										} \label{eqn:f_4a_arite_suivante}\\
				f_{i_3+6a+1}(x) & = & \casedist{
										(m+1)^{i_3} - 1 & \text{si $1 \leqslant i_3 \leqslant f_{6a +1}(x) \leqslant a$} \\
										0 & \text{si $f_{6a +1}(x)=1 = i_3$} \\
										0 & \text{sinon} 
										} \label{eqn:max_2_pow_1}\\
				f_{i_4+7a+1}(x) & = & \sum_{i=1}^{i_4} f_{i+6a+1}(x)\label{eqn:cumul_sum_max_2_pow_1} \\
					& = & \casedist{
								\sum\limits_{i=1}^{\min\left(f_{6a+1}(x), i_4\right)} \left((m+1)^{i} - 1\right) & \text{si $1 < f_{6a +1}(x)$} \\
								0 & \text{sinon}
								} \nonumber \\
				f_{8a+2}(x) & = & f_{8a+1}(x) + f_{4a}(x) \label{eqn:rang_arite_suivante}
			\end{eqnarray}
			
			
			On va un peu plus vite : les équations (\ref{eqn:max_1_pow_1}), (\ref{eqn:cumul_sum_max_1_pow_1}), (\ref{eqn:max_2_pow_1}) et (\ref{eqn:cumul_sum_max_2_pow_1}) sont des calculs intermédiaires aux résultats suivants : si $f_{4a}(x)$ calcule l'arité courante, alors $f_{6a +1}(x)$ calcule l'arité suivante, et $f_{8a+2}(x)$ calcule le rang du premier $f_{6a +1}(x)$-uplet. Enfin, $f_{4a}(x)$ vérifie le résultat de $f_{8a+2}(x-1)$ pour s'adapter : si $x = f_{8a+2}(x)$ alors on a atteint le seuil qui détermine qu'on est passé à l'arité suivante. Comme $f_{4a}(x)$ se met à jour, $f_{8a+2}(x)$ se met aussi à jour pour donner le rang du premier terme d'arité suivante, donc $x$ ne dépasse jamais $f_{8a+2}(x)$. Les autres cas de la distinction de cas sont triviaux : on retourne à $1$ quand la fonction $f_{4a-1}(x-1)$ (cf. plus haut) indique qu'on a changé de max courant.
			
		\paragraph{Calcul des cordonnées.}
			
			\redtext{Ici, c'est rigolo. Cette partie de la démonstration est entièrement dépendante de la sous-section \ref{subsec:bon_ordre_prop_combinatoires}, pour laquelle il me manque la troisième partie.}
			
			Je peux néanmoins glisser quelques idées qu'il faudra prendre en compte, après avoir rappelé que le max courant est noté $m$ :
			
			\begin{itemize}[itemsep=-1mm]
				\item	Puisque l'arité est changeante, certaines fonctions devront renvoyer une valeur par défaut, par exemple si leur indice est plus grand que l'arité courante. 
				\item 	Le premier terme d'une arité donnée est $(0, \dots, 0, m)$ et le suivant est soit $(0, \dots, 1,m)$ si $m\neq 1$ soit $(0, \dots, 1, 0)$ si $m = 1$.
				\item 	Quand on parcourt les termes d'une arité fixée, à part le fait de garder le max quelque part, le reste des calculs revient à une énumération de mots de longueur donnée, dans un alphabet de taille $m$.
			\end{itemize}
			
		
		
		
	\end{proof}
	
	Une fois qu'on a les coordonnées, le deuxième problème qui se pose est l'utilisation des coordonnées partielles, ce que transcrivait la notation $\bar{x'} \ll \bar{x}$ : $\bar{x'}$ est obtenu en prenant des coordonnées à $\bar{x}$, en les mélangeant, éventuellement en dupliquant certaines, tant que $\abs{\bar{x'}} < \abs{\bar{x}}$. Une fois qu'on a le $\leqa$-uplet $\bar{x}$, il est facile de récupérer $\bar{x'}$. Mais il faut ensuite convertir $\bar{x'}$ en son rang $x'$ afin que le LSRS d'arité $1$ puisse s'en servir. 
	
	\redtext{Le calcul du rang $x'$ de $\bar{x'}$ est entièrement dépendant de la troisième partie de la sous-section \ref{subsec:bon_ordre_prop_combinatoires}. \\
		Admettons qu'on ait cette fonction qui à un $\leqa$-uplet associe son rang.} Alors on a le lemme suivant :
	
	
	\begin{lemma}
		\label{lem:operation_simulable}
		Supposons que $g_1, g_2$ sont des symboles de fonctions utilisés ou définies dans un $a$-LSRS, et sont simulées par $\hat{g_1},\hat{g_2}$ dans un LSRS.
		
		Alors l'opération $f\left(\bar{x}\right) = g_1\left(\bar{x_1}\right) * g_2\left(\bar{x_2}\right)$, pour $* \in \{+, -, \times \}$ d'un $a$-LSRS est simulable par LSRS.
	\end{lemma}
	
	\begin{proof}
		Tout d'abord, présentons quelques notations. Notons $\text{sub}_j(x)$ l'application qui à un code $x$ associe $x_j$, code du $\leqa$-uplet $\bar{x_j} \ll \bar{x}$. 
		
		\redtext{On a dû admettre l'existence de $\text{sub}_j(x)$ plus haut, mais j'ai bon espoir qu'elles soient effectivement définissables par un LSRS.} 
		
		Dans le LSRS d'arité $1$, on va devoir ajouter la composition bornée $g,g' \mapsto \left(x \mapsto g'\left[ g(x)\right]_x\right)$. Cet ajout ne rend pas le LSRS plus puissant \cite{GrandjeanSchwentick2002}. 
		
		Le LSRS suivant\footnotemark  simule $f\left(\bar{x}\right) = g_1\left(\bar{x_1}\right) * g_2\left(\bar{x_2}\right)$ :
			
			\footnotetext{La numérotation est disjointe de celles précédemment données.}
		
		\begin{eqnarray}
			f_1(x) & = & \hat{g_1}\left[ \text{sub}_1(x) \right]_x \\
			f_2(x) & = & \hat{g_2}\left[ \text{sub}_2(x) \right]_x \\
			f^1(x) & = & f_1(x) * f_2(x) 
		\end{eqnarray}
	\end{proof}
	
	\begin{remark}
		Avant de traiter l'opération suivante, il faut apporter une précision. Soit le $2$-LSRS suivant :
		
		\begin{eqnarray}
			f_1(x) & = & n(x) + 1(x) \\
			f_2(x,y) & = & f_1(x) + 1(y) \\
			f_3(x,y) & = & f_1(x) + id(y)
		\end{eqnarray}
	
		Supposons-le simulé par un LSRS $S$ sur $\hat{f_1}, \hat{f_2}, \hat{f_3}$ (avec éventuellement d'autres fonctions, mais elles ne sont pas importantes pour la remarque).
		
		L'ordre est : $(0) < (0,0) < (1) < (0,1) < (1,0) < (1,1) < (2) \dots$. Soit $x$ un $\left(\leqslant 2\right)$-uplet, notons $r(x)$ son rang. La définition donnée ici du déroulement d'un $a$-LSRS suppose, en quelque sorte, qu'une fois que $f_1(0)$ est calculé, on avance d'un pas dans l'ordre et $f_1(0)$ reste \emph{figé} en attendant qu'on ait calculé $f_2(0,0)$ et $f_3(0,0)$. Dans la simulation par un LSRS, chaque fonction renvoie constamment un résultat ; ici, que renvoie $\hat{f_1}(r(0,0))$ ? Une idée pourrait être simplement de renvoyer son dernier résultat. L'idée est que $\hat{f_1}$ effectue son calcul sur $r\left(\bar{x}\right)$ tant que $\abs{\bar{x}} = \text{arité}\left(f_1\right)$, puis renvoie son dernier résultat tant que l'arité n'est pas la bonne. Il faut donc que chaque fonction ait accès à l'arité de la fonction qu'elle simule, ce qui peut se faire en rajoutant des fonctions auxiliaires, incluses dans le LSRS. 
		
		En conclusion, si $f\left(\bar{x}\right) = \sigma\left(\bar{x}\right)$ est une équation de $a$-LSRS, avec $f$ d'arité $r$, alors sa simulation est de la forme :
	
		\begin{eqnarray}
			\text{arité}_1(x) & = & 1(x) \\
			 & \dots & \nonumber \\
			\text{arité}_a(x) & = & \text{arité}_{a-1}(x) + 1(x) \\
			 & \dots & \nonumber \\
			\text{a\_c}(x) & = & \text{arité courante} \\
			 & \dots & \nonumber \\
			\hat{f}(x) & = & \casedist{
								\hat{\sigma}\left(\bar{x}\right) & \text{si $\text{a\_c}(x) = \text{arité}_r(x)$}\\
								\hat{f}(x-1) & \text{sinon}
								}
		\end{eqnarray}
	
		Dans la suite, pour des raisons de lisibilité, on évitera ce genre d'écriture, et on notera simplement la simulation sous sa forme $\hat{f}(x) = \hat{\sigma}\left(\bar{x}\right)$.
	\end{remark}
	
	\begin{lemma}
		Supposons que $g_1, g_2$ sont des symboles de fonctions utilisés ou définies dans un $a$-LSRS, et sont simulées par $\hat{g_1},\hat{g_2}$ dans un LSRS.
		
		Alors l'opération $f(x) = \eqpredf{g_1}{g_2}{\bar{x'}}{\bar{x}}$ d'un $a$-LSRS est simulable par un LSRS.
	\end{lemma}
	
	\begin{proof}
		De nouveau, quelques notations : notons
	\end{proof}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\bibliographystyle{plain}
	\bibliography{biblio}
	
\end{document}

















