%\documentclass{report}
%
%\include{packages}
%\include{macros}
%
%% Pour les itemize : 			\setlength{\itemsep}{-1mm}
%
%% Variables pour le document
%
%\author{BEURIER Erwan}
%\title{M2 LFMI \\ BROUILLON }
%\date{Année 2015-2016}


% Commandes de ce document

%\newcommand{\sRAMif}[2]{\text{IF} (A=B) \{I( #1 )\} \text{ ELSE } \{I( #2 )\}}
%\newcommand{\sRAMifc}[2]{\text{IF} (A_1=A_2) \{I( #1 )\} \text{ ELSE } \{I( #2 )\}}
%\newcommand{\bbA}{\mathbb{A}}
%\newcommand{\TRec}[1]{\text{TRec}\left(\mathbb{#1}\right)}
%\newcommand{\TRecd}[1]{\text{TRec}_{2}\left(\mathbb{#1}\right)}
\newcommand{\dtimeram}{\text{DTIME}_{\text{RAM}}\left( n^K \right)}
\newcommand{\dtimeramarg}[1]{\text{DTIME}_{\text{RAM}}\left( n^{#1} \right)}
\newcommand{\eqpred}[3]{#1\left[ #2^{\leftarrow}(#3) \right]_{#3}}
\newcommand{\eqpredf}[4]{#1\left[ #2^{\leftarrow}(#3) \right]_{#4}} % four arguments
\newcommand{\eqpredfi}[5]{#1\left[ #2^{\leftarrow}(#3) #4 \right]_{#5}} % five arguments
\newcommand{\arite}[1]{\text{arité}\left( #1 \right)}
\newcommand{\leqa}{\left( \leqslant a \right)}
\newcommand{\rang}[1]{\text{rang}\left( #1 \right)}

%\begin{document}
%	
%	\maketitle
%	
%	\tableofcontents
%	
	\pagebreak 
	

	 
	\pagebreak
		
	\chapter{Deux caractérisations du temps polynomial}
	\label{chap:rLSRS}
	
	\section{Présentation des LSRS}
	
	Après avoir cherché une passerelle entre deux modèles de calculs, l'objectif du stage était de travailler sur une caractérisation des classes de complexité à polynôme fixé dans une $\sigma$-RAM (dans ce chapitre et le suivant, la $\sigma$-RAM n'est plus constructive ; on reprend la signature $\sigma = \{+,-,\times \}$). 
	
	Une première piste a été de s'inspirer du travail de Grandjean et Schwentick \cite{GrandjeanSchwentick2002} sur le temps linéaire. Cette section de présentation reprend une partie de cet article et l'adapte au temps polynomial. Les preuves n'étant que des adaptations au temps polynomial de celles de cet article, elles ne sont pas incluses dans le mémoire mais se trouvent en annexe de celui-ci.
		
	Définissons d'abord ce qu'on entend par temps $\grozo{n^K}$. On doit définir les structures de données sur lesquelles on travaille. 
		
		\subsection{Structures de RAM} %(p.198)
		\label{sec:RAM_data_structures}
		
		\begin{definition}[RAM-structure]
			\label{def:RAM_data_structures}
			Soit $t$ un type, c'est-à-dire une signature fonctionnelle ne contenant que des symboles de constantes ou de fonctions unaires.
			
			Une RAM-structure $s$ de type $t$ est un uplet constitué de :
			\begin{itemize}[itemsep=-1mm]
				\item 	$n \in \naturels$ qui est la taille de la structure ;
				\item 	$C \in \naturels$ pour chaque symbole $C \in t$ ;
				\item 	$f : n \to \naturels$ pour chaque symbole $f \in t$.
			\end{itemize}
			
			On notera $s.n, s.C, s.f$ les composantes $n, C, f$ de $s$ (cette notation est à rapprocher de l'accès à un attribut ou à une fonction membre en programmation objet).
			
			On dira que $s$ est $c$-bornée pour $c\in\naturels$ lorsque $s.C, s.f(i) < c s.n$ pour tous $C, f \in t$ et $i \in n$.
		\end{definition}
		
		On peut voir ces structures comme étant des instances d'une \emph{struct} dans le langage C, dont les attributs sont les constantes $C$ et $n$. Les fonctions $f$ seraient interprétées comme des tableaux de taille $n$. 
		
		
		\begin{definition}[Fonction de RAM]
			\label{def:fonction_de_RAM}
			Soient $t_1, t_2$ des types. 
			
			Une $(t_1, t_2)$-fonction de RAM $\Gamma$ est une fonction telle qu'il existe $c_1, c_2 \in \naturels$, tels que $\Gamma$ envoie les structures $c_1$-bornées de type $t_1$ sur des structures $c_2$-bornées \footnotemark de type $t_2$.
			
				\footnotetext{On rappelle que "$c$-borné" ne concerne que la structure par rapport à sa propre taille ; ici on ne compare pas la taille de l'entrée et de la sortie}
			
			On dit que $\Gamma$ est polynomiale lorsque $\Gamma(s).n = \grozo{(s.n)^K}$.
			
			% Là, si.
		\end{definition}
		
		Une fonction de RAM n'est qu'une fonction qui calcule une instance de \emph{struct} à partir d'une autre. 
		
		
		
		\subsection{Machine RAM} % (p.200)
		\label{subsec:machine_RAM}
		
		\begin{definition}[Temps polynomial]
			\label{def:temps_poly_RAM}
			On définit $\dtimeram$ comme étant l'ensemble des fonctions calculables sur $\{+\}$-RAM en temps $\grozo{n^K}$, telles que le nombre de registres utilisés, les valeurs entières manipulées (y compris les adresses de registres) soient bornés par $\grozo{n^K}$.
		\end{definition}
		
		Pour les besoins de ce mémoire, la machine RAM a pour signature $\{+ , -, \times \}$. Elle est intuitivement plus puissante que la $\{+\}$-RAM utilisée dans l'article \cite{Grandjean1994} qui n'utilise que $+$, mais l'ajout de ces deux opérations ne change pas la puissance de calcul. Autrement dit, une $\{+ , -, \times \}$-RAM travaillant en temps $t(n)$ avec des entiers de taille au plus $t(n)$ peut être simulée par une $\{+\}$-RAM calculant en temps $t(n)$ et n'utilisant que des entiers de taille au plus $t(n)$. 
		L'algorithme présenté dans \cite{GrandjeanSchwentick2002} revient en fait à écrire une table de multiplication pour tous les entiers, ce qui se fait avant le calcul de la $\{+\}$-RAM. Les multiplications sont alors remplacées par des accès mémoire, plus rapides.
		
		
		\subsection{Réductions affines} %(p.202)
		\label{subsec:reductions_affines}
		
		On laisse inchangées les notions de \emph{transformations affines}, de \emph{réductions affines}, de \emph{projections affines}. Les théorèmes et lemmes suivants s'y rapportant sont aussi inchangés. Ils permettront de définir des réductions qui \emph{restent} dans $\dtimeram$ pour un $K$ fixé. 
		
		\begin{definition}[Fonction affine non-décroissante]
			On appellera \emph{fonction affine non-décroissante}, ou plus simplement, \emph{fonction affine}, une fonction $A$ de la forme $A(x_1, \dots, x_k) = a_0 + a_1 x_1 + \dots + a_k x_k$, telle que, soit $a_1, \dots, a_k \geqslant 0$ et $a_0 \in \bb{Z}$, soit $a_1, \dots, a_k = 0$ et $a_0 \geqslant 0$.
		\end{definition}
		
		\begin{definition}[Transformation affine]
			\label{def:transfo_affine}
			Soit $T$ une fonction de RAM qui envoie des RAM-structures de type $t$ sur des RAM structures de type $t'$. 
			
			On dit que $T$ est une transformation affine lorsque : 
			
	
			\begin{itemize}[itemsep=-1mm]
				\item 	Il existe des constantes $l, d_1, \dots, d_l$ et des fonctions affines $\alpha_1, \dots, \alpha_l$ telles que, pour chaque structure de RAM $s$ de type $t$, on ait $T(s).n = \sum_{l}^{i=1} d_i \alpha_i(s.n)$ ;

				\item 	Pour chaque symbole de constante $C' \neq n$ de $t'$, il existe une fonction affine $\beta_{C'}$ telle que, pour chaque structure de RAM $s$ de type $t$, il existe $T(s).C' = \beta_{C'}\left( s.\bar{C} \right)$, où $\bar{C}$ est le vecteur des constantes de $t$, $n$ inclus. 
				
				\item 	Pour chaque symbole de fonction $g$ de $t'$, chaque $i \leqslant l$ et chaque $r < d_i$, il existe un symbole de fonction $f_{g,i,r}$ de $t$, éventuellement le symbole de fonction identité, et une fonction affine $A_{g,i,r}$ tels que pour chaque $i \leqslant l$, chaque $r < d_i$, chaque structure $s$, chaque $x < \alpha_i(s.n)$, on ait : 
				
					\[
						T(s).g\left( r + d_i x + \sum_{j<i} d_j \alpha_j(s.n) \right) = A_{g,i,r}\left( s.\bar{C} s.f_{g,i,r}(x) \right)
					\]
					
					où $s.f_{g,i,r}(x) = 0$ si $x \geqslant s.n$.
			\end{itemize}
		\end{definition}
		
		En résumé, une transformation affine est une fonction qui transforme une structure d'un type en une autre structure d'un autre type, en n'appliquant que des opérations affines sur les éléments de la structure de départ. 
		
		Ces transformations sont calculables en temps linéaire et en espace logarithmique par une machine de Turing déterministe. 

		\espace

		Il existe plusieurs raffinements de ces transformations.
		
		\begin{definition}[Réduction affine]
			\label{def:reduc_affine}
			
			Soient $L_1, L_2$ des problèmes de décision (formellement, des ensembles de structures de RAM). 
			
			Une \emph{réduction affine} de $L_1$ dans $L_2$ est une transformation affine telle que pour chaque structure de RAM $s$, on a $s \in L_1$ ssi $T(s) \in L_2$. Si une telle réduction existe, on note $L_1 \leqslant_a L_2$.
		\end{definition}
		
		Ces réductions ont deux avantages : elles sont indépendantes du modèle de calcul utilisé et elles permettent d'effectuer des réductions à l'intérieur d'une classe de complexité restreinte comme peut l'être $\dtimeram$, à $K$ fixé.
		
		\espace
		
		Dans ce mémoire, nous ne parlerons que de fonctions de RAM et pas de problèmes de décision. On doit raffiner les transformations affines d'une autre manière.
		
		\begin{definition}[Projection affine]
			\label{def:proj_affine}
			
			Soient $t$ et $t'$ des types de structures de RAM. Une projection affine est une transformation affine $P : (m,s) \mapsto P_m(s)$ où $m \in \naturels$, $s$ est une structure de RAM de type $t$ et $P_m(s)$ est une structure de RAM de type $t'$, telle que $P_m(s)$ est définie comme suit :
			
			\begin{itemize}
				\item	Pour chaque constante $C$ de $t'$, et en particulier pour $n$, les valeurs de $P_m(s).C$ sont définies de l'une des façons suivantes :
				\begin{enumerate}
					\item 	Il existe un symbole $D_C$ de $t$ tel que, pour chaque structure $s$ de type $t$ et chaque $m \in \naturels$, on ait $P_m(s).C = s.D_C$ ;
					\item 	Ou bien, il existe un symbole de fonction $f_C$ de $t$ et une fonction affine $\alpha_C : \naturels \to \naturels$ tels que, pour chaque structure $s$ de type $t$ et chaque $m \in \naturels$, on ait : $P_m(s).C = s.f_C\left( \alpha_C(m) \right)$.
				\end{enumerate}
				
				\item 	Pour chaque symbole de fonction $g$ de $t'$, il existe un symbole de fonction $f_g$ de $t$ et une fonction affine $A_g : \naturels \times \naturels \to \naturels$ telle que, pour chaque structure $s$ de type $t$ et chaque $m,a \in \naturels$, on ait : $P_m(s).g(a) = s.f_g\left( A_g(m,a) \right)$. 
			\end{itemize}
		\end{definition}
		
		Encore une fois, si $A_g(m,a) \geqslant s.n$, on impose que $s.f_g\left( A_g(m,a) \right) = 0$.
		
		Pour résumer, les projections affines opèrent simplement une sélection des valeurs intéressantes qui doivent apparaitre dans la structure $P_m(s)$ ; cette sélection se fait à partir des fonctions affines, lesquelles \emph{savent où regarder} pour trouver les valeurs. De plus, le paramètre $m$ qui apparait dans la définition est intuitivement la taille de la structure de départ.
		
	
	
	
	\section{Le framework algébrique} %(p.208)
		\label{sec:framework_algebrique}
	
	On introduit l'outil qui nous servira à caractériser le temps polynomial.
	
		\subsection{LSRS} % (p.208)
			\label{subsec:LSRS}
		
		
		
			\begin{definition}[Application bornée et \emph{equal-predecessor}]
				\label{def:app_bornee_eq_pred}
				Pour $f : n \to \naturels$, on définit deux opérations :
				
				\begin{itemize}
					\item 	L'application bornée :
					\[
						f[x]_y = \casedist{
										f(x) & \text{si $x<y$} \\
										x 	& \text{sinon}
									}
					\]
					
					\item 	L'opération \emph{equal-predecessor} :
					\[
						f^{\leftarrow}(x) = \casedist{
													\max\left(\left\lbrace y < x | f(x) = f(y)\right\rbrace\right) & \text{si un tel $y$ existe} \\
													x	& \text{sinon}
												}
					\]
				\end{itemize}
				
				Pour $g, g' : n \to \naturels$, on combine ces deux opérations pour en créer une troisième, l'opération de \emph{récursion} :
				
				\[
					f(x) = \eqpred{g'}{g}{x}
				\]
				
				C'est-à-dire : $f(x) = g'(y)$ où $y$ est le plus grand $z$ tel que $g(x) = g(z)$, ou $f(x) = x$ si un tel $y$ n'existe pas.
			\end{definition}
			
			
			\begin{definition}[LSRS]
				\label{def:LSRS}
				Soit $F$ un ensemble de symboles de fonctions unaires (dites \emph{fonctions de base}), soient $f_1, \dots, f_k$ des symboles de fonctions qui n'apparaissent pas dans $F$. Pour $i\leqslant k$, notons $F_i = F\cup \{f_1, \dots, f_i\}$. 
				
				Un LSRS (\emph{Linear Simultaneous Recursion Scheme}) $S$ sur $f_1, \dots, f_k$ et $F$ est une suite de $k$ équations $\left(E_i\right)_{i_\in k}$ dont chacune est de l'une des deux formes suivantes :
				
				\begin{itemize}
					\item 	(opération) 		$f_i(x) = g(x) * g'(x)$ où $g,g' \in F_{i-1}$ et $* \in \{+, -, \times \}$
					
					\item 	(récursion)			$f_i(x) = \eqpred{g'}{g}{x}$ où $g' \in F_k$ et $g \in F_{i-1}$
				\end{itemize}
			
			\end{definition}
			
			\begin{remark}
				Les opérations peuvent varier, il est dit dans \cite{GrandjeanSchwentick2002} que l'on peut choisir n'importe quelle opération binaire calculable en temps linéaire sur machine de Turing. La multiplication est signalée comme n'augmentant pas la puissance de calcul de la RAM, ce qui n'est plus vrai au-delà du temps linéaire.
			\end{remark}
			
			
			%On doit refaire la définition 3.3 de l'article (la définition de \emph{linéairement représentable}).
			
			Pour $t$ un type, on note $F_t$ l'ensemble des symboles de fonctions suivants : $\{1(-), n(-), id(-)\} \cup  \{ f_C | C \in t\} \cup \{ f | f \in t\}$.
			
			\begin{remark}
				\label{rk:entree_LSRS}
				
				Soient $t$ un type et $S$ un LSRS pour $f_1, \dots, f_k$ sur $F_{t}$. 
				L'entrée d'un LSRS peut être vue comme étant une RAM-structure $s$ de type $t$, qu'il \emph{lit} en interprétant les symboles de $F_t$ de la façon suivante : 
				
				\begin{itemize}[itemsep=-1mm]
					\item 	$\forall f \in t_1$ :   $f(i) = 
					\left\lbrace \begin{array}{ll}
					s.f(i) & \text{si } i< s.n \\
					0 & \text{sinon}
					\end{array}\right.$
					
					\item 	$\forall C \in t_1$ :   $f_C(i) = s.C$
					\item 	$1(i) = 1$, $n(i) = s.n$, $id(i) = i$
				\end{itemize}
				
				La sortie du LSRS peut aussi être vue comme une nouvelle structure $s' = S(s)$ de type $\{f_1, \dots, f_k\}$.
			\end{remark}
			
			
			
			\begin{definition}[RAM $n^K$-représentée par LSRS]
				\label{def:representee_par_LSRS}
				Soient $t_1, t_2$ des types. Soit $\Gamma$ une $(t_1, t_2)$-fonction de RAM.
				
				Soit $S$ un LSRS pour $f_1, \dots, f_k$ sur $F_{t_1}$. 
				
				On dit que $\Gamma$ est $n^K$-représentée par $S$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée de type $t_1$, $S$ définit des fonctions $f_1, \dots, f_k : c (s.n)^K \to c (s.n)^K$ telles que $\Gamma(s) = P((s.n)^K, S(s))$ ($S(s)$ est la structure définie par le LSRS).
				
				
				\[
				\Gamma : \left\lbrace \begin{array}{ccc}
				t_1 & \to & t_2 \\
				s & \to & P((s.n)^K, S(s))
				\end{array} \right.
				\]
				
				Si $\Gamma$ est $n^K$-représentée par un LSRS alors on dit que $\Gamma$ est définissable par LSRS. 
			\end{definition}
			
			La projection affine $P$ permet de redimensionner la sortie du LSRS et d'en extraire les parties importantes pour définir la structure de sortie. Le LSRS définit en quelques sortes des fonctions avec leurs calculs intermédiaires, et la projection affine est un moyen de récupérer les informations utiles du LSRS.
			
			Dans \cite{GrandjeanSchwentick2002}, les fonctions du LSRS allaient de $c s.n$ dans $c s.n$. L'adaptation au temps polynomial passe donc principalement par la taille du domaine de départ et d'arrivée des fonctions.
			

			
			%		\begin{prob}
			%			Est-ce qu'on capture tout $\dtimeram$ ? 
			%		\end{prob}
			%		
			%		\begin{prob}
			%			Est-ce que $P((s.n)^K, S(s))$ capture toutes les structures de taille $\grozo{n^K}$ ?
			%		\end{prob}
			
			
			
			
			% Eventuellement d'autres possibilités
			
			
			\begin{definition}[Définition par cas]
				Une fonction $f_i$ est définie par cas à partir de $g_1, g_2, h_1, h_2 \in F_{i-1}$ lorsqu'elle est définie de la manière suivante :
				
				\[
				f_i(x) = \casedist{
				g_1(x) & \text{si $h_1(x) \leqslant h_2(x)$} \\
				g_2(x) & \text{sinon}
				}
				\]
			\end{definition}
			
			
			\begin{proof}
				On peut la construire à partir de la soustraction propre.
			\end{proof}
			
			\begin{remark}
				La comparaison $h_1(x) \leqslant h_2(x)$ peut être remplacée par $h_1(x) < h_2(x)$, $h_1(x) = h_2(x)$, leur négation. On peut aussi effectuer les opérations booléennes OU et ET.
			\end{remark}
			
		
		
		\subsection{LRS} % (p.211)
		\label{subsec:LRS}
		
		Il existe une variante des LSRS qui n'utilise qu'une seule équation mais autorise la composition.
		
		\begin{definition}[Terme récursif]
			Un terme $\sigma(x)$ est un terme récursif vis-à-vis d'une fonction $g$ et sur le type $t$ lorsque $\sigma(x)$ est de l'une des formes suivantes :
			
			\begin{itemize}[itemsep=-1mm]
				\item 	$\sigma(x) = C$ où $C$ est une constante de $t$ ;
				\item 	$\sigma(x) = f(x)$ où $f$ est une fonction de $t\cup \{1(-), \text{id}(-), n(-) \}$ ;
				\item 	$\sigma(x) = g^{\leftarrow}(x-\delta)$ où $\delta$ est un entier naturel ;
				\item 	$\sigma(x) = g\left[ \sigma'(x)\right]$ où $\sigma'(x)$ est un terme récursif ;
				\item 	$\sigma(x) = f(\sigma'(x))$ où $f \in t$ et $\sigma'(x)$ est un terme récursif ;
				\item 	$\sigma(x) = \sigma_1(x) * \sigma_2(x)$ où $\sigma_1(x)$ et $\sigma_2(x)$ sont des termes récursifs, et $* \in \{+, -, \times\}$
			\end{itemize}
		\end{definition}
		
		\begin{definition}[LRS]
			Un LRS (\emph{Linear Recursion Scheme}) pour $g$ et sur $t$ est une équation de la forme $g(x) = \sigma(x)$ où $\sigma(x)$ est un terme récursif vis-à-vis de $g$ et sur le type $t$.
		\end{definition}
		
		\begin{definition}[RAM $n^K$-représentée par LRS]
			\label{def:representee_par_LRS}
			Soit $\Gamma$ une fonction de RAM. 
			
			Soit $E$ un LRS :
			\[
			(E) \ \ \ g(x) = \sigma(x)
			\]
			
			On dit que $\Gamma$ est $n^K$-représentée par $E$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée, $E$ définit une fonction $g : c (s.n)^K \to c (s.n)^K$ telle que $\Gamma(s) = P\left((s.n)^K, E(s)\right)$ ($E(s)$ est la structure définie par le LRS, elle est de type $\{g\}$).
		\end{definition}
		
		A l'instar de la définition du LSRS, la principale adaptation au temps polynomial consiste à agrandir la taille des domaines de départ et d'arrivée.
		
		\subsection{Théorème principal}
		\label{sec:theorem_principal_grandjean_schwentick}
		
		L'un des principaux théorèmes de l'article \cite{GrandjeanSchwentick2002} s'adapte au temps polynomial :
		
		\begin{conj}
			\label{conj:main_big_theorem}
			Soit $\Gamma$ une fonction de RAM. Les propositions suivantes sont équivalentes :
			
			\begin{enumerate}[itemsep=-1mm]
				\item 	\label{num:main_gamma_in_nk}
				$\Gamma \in \dtimeramarg{K}$.
				
				\item 	\label{num:main_gamma_nk_rep_LSRS}
				$\Gamma$ est $n^K$-représentée par un LSRS.
				
				\item	\label{num:main_gamma_nk_rep_LRS}
				$\Gamma$ est $n^K$-représentée par un LRS.
			\end{enumerate}
		\end{conj}
		
		La démonstration de ce théorème se trouve \hyperref[sec:LRS_et_temps_poly]{en annexe}. Elle n'est pas présentée dans ce mémoire car la valeur ajoutée par rapport à la démonstration d'origine (donc, pour le temps linéaire) est faible. 
		
		% RELU JUSQU'ICI-------------------------------------------------------------------------------
		
	
	
	\section{Introduction aux LSRS d'arité multiple}
	\label{sec:rintroduction}
	
		Avant toute chose, il peut être bon d'adapter les LSRS \cite{Schwentick1997} \cite{GrandjeanSchwentick2002}. Pour les besoins du présent chapitre, nous allons ajouter une troisième forme d'opération.
		
		
		\begin{definition}[LSRS]
			\label{def:LSRS_2}
			Soit $F$ un ensemble de symboles de fonctions (dites \emph{fonctions de base}), soient $f_1, \dots, f_k$ des symboles de fonctions qui n'apparaissent pas dans $F$. Pour $i\leqslant k$, notons $F_i = F\cup \{f_1, \dots, f_i\}$. 
		
			Un LSRS (\emph{Linear Simultaneous Recursion Scheme}) $S$ sur $f_1, \dots, f_k$ et $F$ est une suite de $k$ équations $\left(E_i\right)_{i_\in k}$ dont chacune est de l'une des trois formes suivantes :
			
			\begin{itemize}[itemsep=-1mm]
				\item 	(opération) 		$f_i(x) = g(x) * g'(x)$ où $g,g' \in F_{i-1}$ et $* \in \{+, -, \times \}$ %\footnotemark
				
				%\footnotetext{Les opérations peuvent varier, il est dit dans \cite{GrandjeanSchwentick2002} que l'on peut choisir n'importe quelle opération binaire calculable en temps linéaire sur machine de Turing, voire la multiplication. Nous verrons d'ailleurs par la suite que, pour des raisons de facilité, nous aurons besoin de la multiplication.}
				
				\item 	(récursion)			$f_i(x) = \eqpred{g'}{g}{x}$ où $g' \in F_k$ et $g \in F_{i-1}$			
				
				\item 	(composition)			$f_i(x) = g'\left[g(x)\right]_x$ où $g' \in F_k$ et $g \in F_{i-1}$
			\end{itemize}
		\end{definition}
		
		\begin{remark}
			L'ajout de l'opération de composition bornée n'est pas canonique, mais elle facilitera les calculs qui suivront. Elle ne rend pas le LSRS plus puissant \cite{GrandjeanSchwentick2002}.
		\end{remark}
		
		\label{rmq:choix_arites}
		Le but de ce chapitre est d'étendre les LSRS définis dans \cite{GrandjeanSchwentick2002} et utilisés \hyperref[def:LSRS]{plus haut} aux fonctions d'arité $>1$. Remarquons dans un premier temps que si l'on définit un LSRS avec des fonctions d'arité $a$, où $a$ est le même pour chaque fonction, alors en utilisant l'ordre lexicographique, on peut se ramener à un LSRS équivalent d'arité $1$, ce qui n'apporte pas grand-chose par rapport à ce dont nous avons déjà parlé.
		
		Dans la suite, on suppose \emph{a priori} que l'on est dans un cas plus général, où le LSRS généralisé contient des fonctions de plusieurs arités. Soit $a>1$, on suppose que le LSRS est composé de fonctions $f_i$ d'arité $r_i \leqslant a$, où $i \in k$, et telles que $r_i = a$ pour au moins un $i\in k$. On qualifie de $a$-LSRS un tel LSRS. On notera LSRS les systèmes utilisés dans le chapitre précédent, qui sont en fait des $1$-LSRS. 
		
		\begin{definition}
			Soit $a$ un entier naturel. On appelle $\leqa$-uplet un $n$-uplet où $n\leqslant a$. 
		\end{definition}

	
	
	\section{Un bon ordre} %{Les ennuis commencent}
	\label{sec:ennuis_commencent}
	
		
		\subsection{Bon ordre sur les $\leqa$-uplets}
		\label{subsec:bon_ordre_uplets}
		
		Dans un LSRS, l'ordre est très important ; que ce soit au niveau de l'ordre des fonctions ou des variables. De manière générale, dans un $1$-LSRS, $f(x)$ ne peut être défini avec $g(y)$ que si $g(y)$ a été calculé avant, que ce soit parce que $y<x$ ou, si $x=y$, parce que $g$ est définie avant elle dans le LSRS. 
		Pour reproduire l'importance de l'ordre sur des fonctions d'arité multiple, et ne pas trahir la définition originelle du LSRS, on peut utiliser un ordre lexicographique sur les $\leqa$-uplets tels que $\overline{x} <_{\text{naïf}} \bar{y} \Leftrightarrow \left(\left|\overline{x}\right| < \left|\bar{y}\right| \text{ ou } \overline{x} <_{\text{lex}} \bar{y}\right)$.
			
		Cela revient à calculer, dans l'ordre, d'abord les fonctions d'arité $1$, puis ensuite toutes les fonctions d'arité $2$, etc. jusqu'à calculer les fonctions d'arité $a$. Le problème de cette définition est que les fonctions d'arité $a'$ ne peuvent faire appel qu'aux fonctions d'arité $<a'$, ce qui limite l'usage de cet ordre.
		
		Nous avons choisi un ordre moins naturel, mais qui est un bon ordre, permet de faire des projections (récupérer des éléments d'un $a'$-uplet, sauf au moins un, et les utiliser comme argument d'une autre fonction), et permet de calculer tour à tour des fonctions d'arité $1$, puis $2$, etc., puis revenir aux arités $1$, $2$...
		
		\begin{definition}[\cite{Kunen1980}]
			\label{def:bon_ordre_sur_uplets}
			On définit l'ordre $<$ sur les $\leqa$-uplets par :
			
			\[
				\overline{x} < \bar{y} \Leftrightarrow \left\lbrace
														\begin{array}{ccc}
															\max\left(\overline{x}\right) < \max\left(\bar{y}\right) & & \\
															\text{ou } \max\left(\overline{x}\right) = \max\left(\bar{y}\right) & 
																\text{ et } \left|\overline{x}\right| < \left|\bar{y}\right| & \\
															\text{ou } \max\left(\overline{x}\right) = \max\left(\bar{y}\right) & 
																\text{ et } \left|\overline{x}\right| = \left|\bar{y}\right| & 
																\text{ et } \overline{x} <_{\text{lex}} \bar{y}\\
														\end{array}
													\right. 
			\]

		\end{definition}
				
		Si toutes les arités $\leqslant a$ ne sont pas permises (par exemple, si notre LSRS ne contient que des fonctions d'arité $1$ et $3$, mais pas $2$), alors on ampute cet ordre des $n$-uplets correspondant.
	
		\begin{example}
			\label{ex:bon_ordre}
			Pour $a = 3$, avec les arités $1,2,3$ : $(0) < (0, 0) < (0,0,0) < (1) < (0,1) < (1,0) < (1,1) < (0,0,1) < (0,1,0) < (0,1,1) < (1,0,0) < (1,0,1) < (1,1,0) < (1,1,1) < (2) < (0,2) < \dots$.
			
			Pour $a = 3$, avec les arités $1,3$ : $(0) < (0,0,0) < (1) < (0,0,1) < (0,1,0) < (0,1,1) < (1,0,0) < (1,0,1) < (1,1,0) < (1,1,1) < (2) < (0,0,2) <\dots$.
		\end{example}
		
		\begin{remark}
			Quelques remarques informelles : 
			\begin{itemize}
				\item 	Le premier $\leqa$-uplet d'un max $m$ donné est le $1$-uplet $\left( m \right)$.
				\item 	Le premier $\leqa$-uplet d'une arité $r$ donnée, à un max $m$ donné, est le $r$-uplet $\left( 0, \dots, 0, m\right)$.
			\end{itemize}
		\end{remark}
		
		
		
		\subsection{Propriétés combinatoires}
		\label{subsec:bon_ordre_prop_combinatoires}
		
		Etudions l'ordre sur les $\leqa$-uplets. Ceci nous servira pour les démonstrations à suivre. 
		
		\begin{prop}
			\label{prop:bon_ordre_combinatoire}
			Soit $\overline{x}$ un $\leqa$-uplet. On suppose que toutes les arités sont possibles. Notons $m = \maxs{\overline{x}}$ et $r = \abs{\overline{x}}$.
			
			Le bon ordre $<$ sur les $\leqa$-uplets vérifie les égalités suivantes :
			
			\begin{enumerate}
				\item 	\label{itm:bon_ordre_combinatoire1} 
						$\card{\bar{y} < \overline{x} | \maxs{\bar{y}} < \maxs{\overline{x}}} = \sum_{i=1}^{a} m^i$ ;
						
				\item 	\label{itm:bon_ordre_combinatoire2} 
						$\card{\bar{y} < \overline{x} | \maxs{\bar{y}} = \maxs{\overline{x}} \wedge \abs{\bar{y}} < \abs{\overline{x}} } = \sum_{i=1}^{r} \left( \left(m+1\right)^i -1 \right)$ ; 
						
				\item 	\label{itm:bon_ordre_combinatoire3} 
						$\card{\bar{y} < \overline{x} | \maxs{\bar{y}} = \maxs{\overline{x}} \wedge \abs{\bar{y}} = \abs{\overline{x}} \wedge \bar{y} <_{\text{lex}} \overline{x} } = \sum_{i=1}^{r} c_i$ où $c_i = x_i \times \left(m+1\right)^{r-i}$ si $x_{i-1} = m$ ou $x_{i-2} = m$ $\dots$ $x_{1} = m$, et $c_i = \left(m+1\right)^{r-i}-m^{r-i}$ sinon. 
			\end{enumerate}
		\end{prop}
		
		\begin{proof}
			\ref{itm:bon_ordre_combinatoire1}. L'entier $a$ étant fixé, pour un max $m$ donné, on peut construire un $\leqa$-uplet en choisissant une arité $i \leqslant a$ et $i$ composantes, toutes des entiers $< m$. Pour $i$ fixé, on peut faire $m^{i}$ $i$-uplets, d'où au total $\sum_{i=1}^{a} m^i$.
			
			\ref{itm:bon_ordre_combinatoire2}. Pour construire un tel $\bar{y}$, il faut choisir son arité $i<r$, puis le nombre de composantes que l'on fixe à $m$ ; le reste des composantes est libre et $<m$. On obtient un résultat de la forme $\sum_{i=1}^{r} \sum_{j= 1}^{i} m^j$ et le résultat après réécriture.
			
			\ref{itm:bon_ordre_combinatoire3}. D'abord, remarquons que $c_i$ peut être exprimé avec $+,-,\times$ sous la forme $c_i = \left( m+1 \right)^{r-i} - \varepsilon_i m^{r-i}$. Par définition de la soustraction propre, $\varepsilon_i$ défini\footnotemark\ par $1 - \left(\left(x_{i-1} +1 \right)-m\right) \dots  - \left(\left(x_1 +1 \right)-m\right)$ vaut $1$ si $ \forall j \in \intint{1}{i-1} \: x_j < m$ et $0$ si l'un des $x_j$ est égal à $m$.
			
				\footnotetext{Attention : $\varepsilon_i$ ne prend en compte que les coordonnées $x_1$ à $x_{i-1}$.}
			
			Expliquons maintenant le résultat par un dénombrement. Observons $\overline{x}$. Pour construire $\bar{y} < \overline{x}$ de même max et de même longueur, on peut choisir $y_1<x_1$, puis laisser les autres coordonnées libres. On a $x_1 \times \left(m+1\right)^{r-1}$ façons de construire $\bar{y}$ tel que $\bar{y} <_{\text{lex}} \overline{x}$ et $y_1<x_1$. Cependant, on ne doit pas compter les $r$-uplets qui n'affichent pas le max $m$ dans leurs coordonnées. Comme $y_1 < x_1 \leqslant m$, $m$ ne peut pas apparaître dans la première coordonnée de $\bar{y}$. Pour respecter l'ordre que nous avons donné, il faut que le max apparaisse dans au moins une des $r-1$ autres coordonnées. Il faut donc retirer du compte les $r$-uplets tels que $y_1<x_1$ et qui ne contiennent pas le max $m$ dans leurs $r-1$ autres coordonnées, ce qui donne : $c_1 = x_1 \times \left(\left(m+1\right)^{r-1} - m^{r-1}\right)$.

			
			Une fois que $\sum_{j=1}^{i-1} c_j$ a été calculé pour $i \leqslant r$, pour construire un $\bar{y} < \overline{x}$ de même max, de même longueur et ayant fixé les $i-1$ premières coordonnées, on peut fixer $y_i<x_i$ et choisir librement les $r-i$ dernières coordonnées, ce qui se fait dans un premier temps de $x_i \times \left(m+1\right)^{r-i}$, sauf qu'il faut s'assurer que le max $m$ apparaisse bien dans $\bar{y}$. On a deux possibilités :
			
			\begin{itemize}[itemsep=-1mm]
				\item	Soit $x_j = m$ pour l'un des $\left(x_j\right)_{j < i}$ et dans ce cas il n'y a rien à changer ; on a bien $c_i = x_i \times \left(m+1\right)^{r-i}$
				\item 	Soit $x_j < m$ pour tous les $\left(x_j\right)_{j < i}$ alors il faut retirer du compte les $r$-uplets tels que $x_j = y_j$ pour $j < i$, $y_i < x_i$ et qui ne contiennent pas le max $m$ dans leurs $r-i$ autres coordonnées, ce qui donne : $x_i \times \left(\left(m+1\right)^{r-i} - m^{r-i}\right)$.
			\end{itemize}
			
			Finalement, le nombre de $r$-uplets $\bar{y}$ tels que $\bar{y} < \overline{x}$ et ayant les $j<i$ premières coordonnées en commun est $c_i + \sum_{j=1}^{i-1} c_j$.
		\end{proof}
		
		\begin{definition}
			Le rang de $\overline{x}$ est défini par récurrence : 
			
			\begin{itemize}[itemsep=-1mm]
				\item 	$\rang{0} = 0$
				\item 	$\rang{s\left(\overline{x}\right)} = 1 + \rang{\overline{x}}$
			\end{itemize}
		\end{definition}
		
		C'est le numéro de $\overline{x}$ dans l'ordre $<$.
		
		\begin{coro}
			\label{coro:rang_bon_ordre}
			\[
				\rang{\overline{x}} = \left( \left( \sum_{i=1}^{a} m^i \right) + \left( \sum_{i=1}^{r} \left( \left(m+1\right)^i -1 \right) \right) + \left(\sum_{i=1}^{r} c_i \right) \right)
			\]
			où $c_i = x_i \times \left(m+1\right)^{r-i}$ si $x_{i-1} = m$ ou $x_{i-2} = m$ ou $\dots$ ou $x_{1} = m$, et $c_i = \left(m+1\right)^{r-i}-m^{r-i}$ sinon. 
		\end{coro}
		
		
	
	\section{Introduction aux $a$-LSRS}
	\label{par:deroulement_aLSRS}
	
	\paragraph{Déroulement du calcul}
		Au lieu d'incrémenter $x$ de façon naturelle en $x+1$, ici, on compte passer de $\overline{x}$ à son successeur dans cet ordre. Notons-le $s\left(\overline{x}\right)$. Ainsi, le calcul d'un $a$-LSRS se déroulera de la façon suivante :
		
		\begin{itemize}[itemsep=-1mm]
			\item[(1)] 	
					$\overline{x} = (0)$ : calcul de toutes les fonctions d'arité $1$ en $(0)$, comme dans un LSRS normal. Dans cette première étape, les fonctions ne peuvent bien sûr pas faire référence aux fonctions d'arité supérieure, car elles ne sont même pas encore définies ; leur évaluation doit respecter les règles du LSRS énoncées plus haut. 
					
			\item[(2)] 
					$\overline{x} = (0,0) = s\left( \left( 0 \right)\right)$ : calcul de toutes les fonctions d'arité $2$ en $(0,0)$. Ces fonctions peuvent faire appel aux valeurs des fonctions d'arité $1$ en $0$. Une fonction $f_i$ d'arité $2$ peut aussi faire référence à des fonctions d'arité $2$ en $(0,0)$, à condition qu'elles aient déjà été évaluées ; c'est-à-dire, à condition qu'elles soient définies dans des équations $E_j$ telles que $j<i$ ;
					
			\item[(n)]   	
					$s\left(\overline{x}\right)$ :
					\begin{itemize}[itemsep=-1mm]
						\item 	Si $r = \abs{\overline{x}} = \abs{s\left(\overline{x}\right)}$ alors on calcule à nouveau les fonctions d'arité $r$ pour $s\left(\overline{x}\right)$ ; ces fonctions peuvent faire appel aux résultats des fonctions d'arité plus petite. 
						\item 	Si $r = \abs{\overline{x}} < \abs{s\left(\overline{x}\right)} = r'$ alors on évalue les fonctions d'arité $r'$ en $s\left(\overline{x}\right)$ ; idem, ces fonctions peuvent faire appel aux résultats des fonctions d'arité plus petite.
						\item 	Si $\abs{s\left(\overline{x}\right)} = 1 < a = \abs{\overline{x}}$ alors on évalue les fonctions d'arité $1$ en $s\left(\overline{x}\right)$ ; ces fonctions d'arité $1$ peuvent faire référence aux résultats des fonctions d'arité plus grande, à condition qu'ils aient été calculés au tour précédent. 
					\end{itemize}
		\end{itemize}
		
		Une fois l'ordre compris, il faut voir comment interpréter les opérations typiques du LSRS : addition, soustraction, multiplication, récursion et composition.
	
	
			\paragraph{Définissons le $a$-LSRS}
			
			Soit $a \in \naturels$. Le symbole "$<$" renvoie selon les cas à l'ordre sur les entiers ou à l'ordre \hyperref[def:bon_ordre_sur_uplets]{juste au-dessus}. Pour des raisons de lisibilité, on notera $\overline{x}' \ll \overline{x}$ pour dire :  $\overline{x}' < \overline{x}$, $\abs{\overline{x}'} < \abs{\overline{x}}$ et $\forall j \: \exists j' \:\: x'_j = x_{j'}$ ; autrement dit : $\overline{x}'$ est obtenu à partir de $\overline{x}$ en récupérant ses composantes, en les mélangeant, en les dupliquant, mais en veillant à ce que $\abs{\overline{x}'} < \abs{\overline{x}}$. Par exemple, pour $\overline{x} = (x_1, x_2, x_3)$, les $\overline{x}'$ pourraient être $(x_1, x_2)$, $(x_3, x_1)$ ou $(x_3, x_3)$.
			
			\begin{definition}[$a$-LSRS]
				\label{def:aLSRS}
				Soit $F$ un ensemble de symboles de fonctions de base. Soient $f_1, \dots, f_k$ de nouveaux symboles de fonctions n'apparaissant pas dans $F$, d'arités respectives $1 \leqslant r_1 \leqslant r_2 \leqslant \dots \leqslant r_k = a$.
				
				On note $F_i = F \cup \{f_j | r_j = r_i \text{ et } j < i\}$, $F'_i = F \cup \{f_j | r_j = r_i\}$, et $G_i = F \cup \{ f_j | r_j < r_i\}$\footnotemark.
				
					\footnotetext{$F_i$ est l'ensemble des symboles des fonctions qui ont la même arité que $f_i$ mais qui sont définies avant $f_i$. 
						
						$F'_i$ est l'ensemble des symboles des fonctions qui ont la même arité que $f_i$, qu'elles soient définies avant ou après $f_i$.
						
						$G_i$ est l'ensemble des symboles des fonctions d'arité strictement inférieure à celle de $f_i$.}
					
				Un $a$-LSRS $S$ sur $F$ et $f_1, \dots, f_k$ est une suite d'équations\footnotemark $E_1, \dots, E_k$ où chaque $E_i$ est de l'une des formes suivantes :
					
					\footnotetext{L'ordre entre des fonctions de même arité a de l'importance, mais pas entre des fonctions d'arités différentes. On peut donc considérer que les équations sont ordonnées par l'arité de la fonction qu'elles définissent.}.
				
				\begin{itemize}[itemsep=-1mm]
					\item	(opération) 	$f_i\left(\overline{x}\right) = A * B$ où $* \in \{+,-,\times\}$ et $A, B$ sont de la forme suivante : 
							\begin{itemize}[itemsep=-1mm]
								\item 	$g\left(\overline{x}\right)$, avec $g \in F_i$ ;
								\item 	$g\left(\overline{x}'\right)$, avec $g \in G_i$, c et $\overline{x}' \ll \overline{x}'$.
							\end{itemize}
										
					\item 	(récursion)		$f_i\left(\overline{x}\right) = \eqpred{g'}{g}{\overline{x}'}$, où $\text{arité}(g) = \text{arité}(g')$ et l'un des deux cas suivants se réalise :
							\begin{itemize}[itemsep=-1mm]
								\item 	Soit $\overline{x}' = \overline{x}$, et dans ce cas $g \in F_i$ et $g' \in F'_i$ ;
								\item 	Soit $\overline{x}' \ll \overline{x}$ et dans ce cas $g, g' \in G_i$. 
							\end{itemize}
%						
%				\item 	(composition)\footnote{Cette opération n'est pas forcément nécessaire ; je l'avais rajoutée vraisemblablement en me trompant quand à son usage. Je reviendrai probablement dessus plus tard.}
%					$f_i\left(\overline{x}\right) = g'\left[ g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right) \right]_{\overline{x}}$, où $\text{arité}(g') = r$, et pour chaque $j \in r$, l'un des cas suivants se réalise : 
%						\begin{itemize}
%							\item 	Soit $\bar{x_j} = \overline{x}$, et dans ce cas $g_j \in F_i$ ;
%							\item 	Soit $\bar{x_j} \ll \overline{x}$ et dans ce cas $g_j \in G_i$. 
%						\end{itemize}
%						De plus :
%						\begin{itemize}
%							\item 	Soit $r = \abs{\overline{x}}$, et dans ce cas $g' \in F_i$\footnotemark ;
%							\item 	Soit $r < \abs{\overline{x}}$ et dans ce cas $g' \in G_i$. 
%						\end{itemize}
%						
%							\footnotetext{Si $\overline{x} < \left(g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right)\right)$ il faut définir une valeur par défaut à $g'\left[ g_1\left(\bar{x_1}\right), \dots, g_r\left(\bar{x_r}\right) \right]_{\overline{x}}$. Dans le \hyperref[def:app_bornee_eq_pred]{LSRS normal}, on se contente de renvoyer $x$. On peut ici renvoyer une composante par défaut, par exemple $x_1$.}
				\end{itemize}
				
			\end{definition}
			
			\begin{remark}
				L'opération $\eqpred{g'}{g}{\overline{x}'}$ nécessite une brève redéfinition, puisqu'on ne peut plus renvoyer l'unique variable contrairement à \hyperref[def:app_bornee_eq_pred]{précédemment}. 
				
				\[
					\eqpred{g'}{g}{\overline{x}} = \casedist{
													g'\left( \bar{y} \right) & \text{où $\bar{y} = \maxset{\bar{z} < \overline{x} | g\left( \overline{x} \right) = g\left( \bar{z} \right)}$ si un tel $y$ existe} \\
													x'_i & \text{sinon, où $x'_i$ est l'une des coordonnées de $\overline{x}'$}
												}
				\]
				
				On impose que la coordonnée renvoyée soit toujours la même au cours du calcul. On pourrait la préciser en ajoutant un label à l'opération, mais dans la suite de ce mémoire, elle ne sera pas cruciale.
			\end{remark}
			
			\begin{remark}
				\label{rk:fonctions_de_base_LSRS}
				Par défaut, on suppose, une nouvelle fois, que l'ensemble des fonctions de base contient par défaut les symboles $n(-)$ et $1(-)$ pour toutes les arités possibles, ainsi que les $\pi^i_j(-)$, projections récupérant la $i$-ième composante d'un $j$-uplet. On peut de plus rajouter $a(-)$, permettant de récupérer $a$, quoique cette fonction est facilement calculable par un LSRS.
			\end{remark}
			
			On doit aussi adapter les notions de structures de RAM et de fonctions de RAM. On garde le même $a$ que pour le $a$-LSRS.
			
			\begin{definition}[RAM-structure]
				\label{def:RAM_data_structures_a}
				Soit $t$ un $a$-type, c'est-à-dire une signature fonctionnelle contenant des fonctions d'arité $\leqslant a$ et dont au moins un symbole est d'arité $a$. 
				
				Une RAM-structure $s$ de type $t$ est un uplet constitué de :
				\begin{itemize}[itemsep=-1mm]
					\item 	$n \in \naturels$ qui est la taille de la structure ;
					\item 	$C \in \naturels$ pour chaque symbole $C \in t$ ;
					\item 	$f : n\times \dots \times n \to \naturels$ pour chaque symbole $f \in t$.
				\end{itemize}
				
				On notera $s.n, s.C, s.f$ les composantes $n, C, f$ de $s$.
				
				On dira que $s$ est $c$-bornée pour $c\in\naturels$ lorsque $s.C, s.f\left(\overline{x}\right) < c s.n$ pour tous $C, f \in t$ et $\overline{x} \in n\times \dots \times n$.
			\end{definition}
			
			
			\begin{definition}[Fonction de RAM]
				\label{def:fonction_de_RAM_a}
				Soient $t_1, t_2$ des $a_1,a_2$-types. 
						
				Une $(t_1, t_2)$-fonction de RAM $\Gamma$ est une fonction telle qu'il existe $c_1, c_2 \in \naturels$, tels que $\Gamma$ envoie les structures $c_1$-bornées de type $t_1$ sur des structures $c_2$-bornées de type $t_2$\footnotemark.
				
					\footnotetext{On rappelle que "$c$-borné" ne concerne que la structure par rapport à sa propre taille ; ici on ne compare pas la taille de l'entrée et de la sortie.}
				
				On dit que $\Gamma$ est polynomiale lorsque $\Gamma(s).n = \grozo{(s.n)^K}$, et linéaire lorsque $\Gamma(s).n = \grozo{s.n}$.
				
				% Là, si.
			\end{definition}
			
			
			\begin{definition}[RAM représentée par $a$-LSRS]
				\label{def:representee_par_aLSRS}
				Soient $t_1, t_2$ respectivement un $a_1$-type et un $a_2$-type. Soit $\Gamma$ une $(t_1, t_2)$-fonction de RAM.
				
				Soit $S$ un $a$-LSRS pour $f_1, \dots, f_k$ sur $F_{t_1}$. 
				
				On dit que $\Gamma$ est représentée par $S$ lorsqu'il existe un entier $c$ et une projection affine $P$ tels que, pour chaque structure $s$ $c$-bornée, $S$ définit des fonctions $f_1, \dots, f_k : cs.n \times \dots \times cs.n \to (cs.n)^a$ telles que $\Gamma(s) = P((s.n)^a, S(s))$ ($S(s)$ est la structure définie par le $a$-LSRS).
			\end{definition}
			
			A priori, les $a$-LSRS prennent en entrée des structures dont l'arité maximale est au plus $a$, car les définitions que l'on a données ne permettent pas d'utiliser des fonctions dont l'arité est plus grande que $a$. 
			
			
			
			
			
	
		
	\section{Lien entre les deux notions de LSRS}
	\label{sec:lien_LSRS_aLSRS}
	
	L'idée principale de ce chapitre est de chercher un lien entre les LSRS et les $a$-LSRS, par le biais du résultat suivant :
	
	\begin{theorem}
		\label{thm:rep_a_LSRS_rep_na_LSRS}
		Soit $\Gamma$ une $(t_1,t_2)$-fonction de RAM, où $t_1$ est un $1$-type et $t_2$ est un $a$-type.
		
		$\Gamma$ est représentable par un $a$-LSRS ssi $\Gamma$ est $n^a$-représentable par un LSRS.
	\end{theorem}
		
	Pour prouver cette conjecture, nous allons avoir besoin de coder l'entrée.
	
	\begin{lemma}
		\label{lem:decomp_rang_par_LSRS}
		
		Pour tout $a \in \naturels$, il existe un $1$-LSRS $S$ qui, à une variable $x$, associe le $\leqa$-uplet $\overline{x}$ de rang $x$. 
	\end{lemma}
		
	\begin{proof}
		Pour simplifier, on va de nouveau se placer dans le cas où toutes les arités $\leqslant a$ sont représentées\footnotemark. On va se servir du codage du rang donné en proposition \ref{prop:bon_ordre_combinatoire}, montrer qu'on peut le décoder par LSRS.
		
			\footnotetext{Si certaines arités ne sont pas représentées, alors il suffit de retirer les quelques équations qui y sont associées et de modifier, le cas échéant, les initialisations de certaines fonctions.}
			
		\paragraph{Calcul du max $m$.}
			\label{par:calcul_max_bon_ordre}
			Considérons le LSRS suivant :
			
				\begin{eqnarray}
					f_1(x) & = & \casedist{	
									0 & \text{ si $x < a$} \\
									1 & \text{ si $x = a$} \\
									f_1(x-1) & \text{ si $x < f_{4a-1}(x-1)$} \\
									f_1(x-1)+1 & \text{ si $x = f_{4a-1}(x-1)$}
									} 
									\label{eqn:f_0_max_courant}\\
					f_{i_1 + 1}(x) & = & f_{i_1}(x) \times f_1(x) \label{eqn:m_pow_i} \:\:\:\:\: \text{ pour $ 1 < i_1 < a $}\\
					f_{a + 1}(x) & = & f_{1}(x) + f_{2}(x) \label{eqn:sum_m_pow_init}\\
					f_{a + i_2 + 1}(x) & = & f_{a + i_2}(x) + f_{i_2 + 2}(x) \label{eqn:sum_m_pow}  \:\:\:\:\: \text{ pour $ 1 < i_2 < a $}\\
					f_{2a}(x) & = & f_1(x) + 1 \label{eqn:m_plus_1}\\
					f_{2a + i_3 + 1}(x) & = & f_{2a + i_3}(x) \times f_{2a}(x) \label{eqn:m1_pow_i}  \:\:\:\:\: \text{ pour $ 1 < i_3 < a $}\\
					f_{3a}(x) & = & f_{2a}(x) + f_{2a + 1}(x) \label{eqn:sum_m1_pow_init}\\
					f_{3a + i_4 + 1}(x) & = & f_{3a + i_4}(x) + f_{2a + i_4+2}(x) \label{eqn:sum_m1_pow}  \:\:\:\:\: \text{ pour $ 1 < i_4 < a $}
				\end{eqnarray}
		
			Notons\footnotemark que $f_1(x-1) = \eqpred{f_1}{1}{x}$ et que la distinction de cas ne rend pas le LSRS plus puissant \cite{GrandjeanSchwentick2002}. 
			
				\footnotetext{$1^{\leftarrow}(x) = \max(0, x-1)$ ; en effet, si $x = 0$ alors on ne peut pas trouver la valeur précédente qui donne le même résultat, et si $x > 0$, la valeur précédant $x$ qui donne 1 par la fonction $1(-)$ est la valeur précédente, à savoir $x-1$.}
			
			Expliquons les équations.
			
			L'équation (\ref{eqn:f_0_max_courant}), calculant $f_1(x)$, est censée calculer $m$, donc le maximum courant. Sachant cela, les équations suivantes coulent de source : 
			
			\begin{itemize}
				\item 	Les équations (\ref{eqn:m_pow_i}) $\left(\text{pour } f_{i_1 + 1}(x)\right)$ calcule $m^{i_1}$ pour $i_1 \in \intint{1}{a}$. On a aussi $f_{a}(x) = m^a$. 
				
				\item 	L'équation (\ref{eqn:sum_m_pow_init}) $\left(\text{pour } f_{a + 1}(x)\right)$ calcule $\sum_{i = 1}^{2} m^i$ et (\ref{eqn:sum_m_pow}) calcule $\sum_{i = 1}^{i_2+2} m^i$ pour $i_2+2 \leqslant a$. On a ainsi $f_{2a-1}(x) = \sum_{i = 1}^{a} m^i$, soit le rang du $1$-uplet $(m)$.
				
				\item 	Les équations suivantes sont sur le même schéma, si ce n'est qu'elles travaillent sur $m+1$ au lieu de $m$. On a $f_{3a-1}(x) = (m+1)^a$ et $f_{4a-1}(x) = \sum_{i = 1}^{a} (m+1)^i$, donc $f_{4a-1}(x)$ contient le rang du $1$-uplet du max suivant, à savoir $(m+1)$. 
			\end{itemize}
			
			Ceci étant explicité, étudions la définition de $f_1(x)$. 
			
			$f_1(x)$ s'initialise naturellement en $0$. On parcourt alors tous les $\leqa$-uplets $(0, \dots, 0)$ jusqu'à l'arité $a$, donc on énumère les $a$ premiers $\leqa$-uplets, et donc l'élément de rang $a$ a bien un max égal à $1$ : $f_1(x)$ passe à $1$. Quand le calcul avance, $x$ finit par atteindre le rang du max suivant, que l'on calcule par avance dans $f_{4a-1}(x)$. Une fois que ce rang est atteint, on incrémente $f_1(x)$ ; le reste des équations se met aussi à jour, ce qui assure que $f_{4a-1}(x)$ est toujours le rang du max suivant. La distinction de cas de $f_1(x)$ est donc bien complète.
			
		\paragraph{Calcul de l'arité courante}
			\label{par:calcul_arite_courante}
			Le calcul de l'arité du $\leqa$-uplet de rang $x$ s'effectue avec la même idée. Pour des raisons de lisibilité, le système présenté ne sera pas un véritable LSRS mais on laisse le lecteur se convaincre qu'il est facile de réécrire ce système sous une véritable forme de LSRS en s'inspirant de ce qui a été fait \hyperref[par:calcul_max_bon_ordre]{au-dessus}, quitte à remanier la numérotation des fonctions. 
			
			\emph{On parlera d'arité \textquoted{courante} pour parler de l'arité du $\leqa$-uplet de rang $x$ ; l'arité \textquoted{suivante} est, quand à elle, l'arité du premier $\leqa$-uplet situé après celui de rang $x$, et qui n'est pas de même arité que celui de rang $x$. Typiquement, l'arité suivante est soit l'arité du $\leqa$-uplet de rang $x$ + 1, soit 1. }
			
			\begin{eqnarray}
				f_{4a}(x) & = & \casedist{
									1 & \text{si $x = 0$} \\
									f_{4a}(x-1) & \text{si $x < f_{8a+2}(x-1)$}\\
									1 & \text{si $x = f_{4a-1}(x-1)$} \\
									f_{4a}(x-1) + 1 & \text{si $x = f_{8a+2}(x-1)$} 
									} \label{eqn:f_4a_arite_courante} \:\:\:\:\: \text{(calcule l'arité courante)}\\
				f_{4a + i_1}(x) & = & \casedist{
										(m+1)^{i_1} - 1 & \text{si $1 \leqslant i_1 \leqslant f_{4a}(x) \leqslant a$} \\
										0 & \text{sinon}
										} \label{eqn:max_1_pow_1}
										 \:\:\:\:\: \text{ pour $ 1 \leqslant i_1 \leqslant a $}\\
				f_{5a + i_2}(x) & = & \sum_{i=1}^{i_2} f_{i+4a}(x)\label{eqn:cumul_sum_max_1_pow_1} \\
					& = & \sum_{i=1}^{\min\left(f_{4a}(x), i_2\right)} \left((m+1)^{i} - 1\right) \nonumber
					 \:\:\:\:\: \text{ pour $ 1 \leqslant i_2 \leqslant a $}\\
			%\end{eqnarray}
			%
			%\begin{eqnarray}
				f_{6a +1}(x) & = & \casedist{
										f_{4a}(x)+1 & \text{si $f_{4a}(x) < a$} \\
										1 	& \text{sinon}
										} \label{eqn:f_4a_arite_suivante}  \:\:\:\:\: \text{(calcule l'arité suivante)}\\
				f_{6a + i_3 + 1}(x) & = & \casedist{
										(m+1)^{i_3} - 1 & \text{si $1 \leqslant i_3 < f_{6a +1}(x) \leqslant a$} \\
										0 & \text{si $f_{6a +1}(x)=1 = i_3$} \\
										0 & \text{sinon} 
										} \label{eqn:max_2_pow_1} \:\:\:\:\: \text{ pour $ 1 \leqslant i_3 \leqslant a $}\\
				f_{7a + i_4 + 1}(x) & = & \sum_{i=1}^{i_4} f_{i+6a+1}(x)\label{eqn:cumul_sum_max_2_pow_1} \\
					& = & \casedist{
								\sum\limits_{i=1}^{\min\left(f_{6a+1}(x), i_4\right)} \left((m+1)^{i} - 1\right) & \text{si $1 < f_{6a +1}(x)$} \\
								0 & \text{sinon}
								} \nonumber  \:\:\:\:\: \text{ pour $ 1 \leqslant i_4 \leqslant a $} \\
				f_{8a + 2}(x) & = & f_{8a+1}(x) + f_{2a-1}(x) \label{eqn:rang_arite_suivante}   \:\:\:\:\: \text{(calcule le rang où apparaît l'arité suivante)}
			\end{eqnarray}
			
			
			On va un peu plus vite : les équations (\ref{eqn:max_1_pow_1}), (\ref{eqn:cumul_sum_max_1_pow_1}), (\ref{eqn:max_2_pow_1}) et (\ref{eqn:cumul_sum_max_2_pow_1}) sont des calculs intermédiaires aux résultats suivants : si $f_{4a}(x)$ calcule l'arité courante, alors $f_{6a +1}(x)$ calcule l'arité suivante, et $f_{8a+2}(x)$ calcule le rang du premier $f_{6a +1}(x)$-uplet. Enfin, $f_{4a}(x)$ vérifie le résultat de $f_{8a+2}(x-1)$ pour s'adapter : si $x = f_{8a+2}(x)$ alors on a atteint le seuil qui détermine qu'on est passé à l'arité suivante. Comme $f_{4a}(x)$ se met à jour, $f_{8a+2}(x)$ se met aussi à jour pour donner le rang du premier terme d'arité suivante, donc $x$ ne dépasse jamais $f_{8a+2}(x)$. Les autres cas de la distinction de cas sont triviaux : on retourne à $1$ quand la fonction $f_{4a-1}(x-1)$ (cf. plus haut) indique qu'on a changé de max courant.
			
			
			\begin{example}
				Pour $a = 3$, avec les arités $1,2,3$ : $(0) < (0, 0) < (0,0,0) < (1) < (0,1) < (1,0) < (1,1) < (0,0,1) < (0,1,0) < (0,1,1) < (1,0,0) < (1,0,1) < (1,1,0) < (1,1,1) < (2) < (0,2) < \dots$.
				
				On aurait alors : 
				
				\begin{itemize}
					\item 	$f_{4a}(4) = 1$ : le quatrième $\left( \leqslant 3 \right)$-uplet est $(1)$ ;
					\item 	$f_{6a +1}(4) = 2$ : le $\left( \leqslant 3 \right)$-uplet d'arité différente de 1 est $(0,1)$ ; 
					\item 	$f_{8a+2}(4) = 5$ : le $\left( \leqslant 3 \right)$-uplet $(0,1)$ est le successeur de $(1)$.
				\end{itemize}
			\end{example}
			
		\paragraph{Calcul des cordonnées.}
			\label{par:calcul_coordonnees}
			Le LSRS définissant les coordonnées de $\overline{x}$, élément de rang $x$ dans notre ordre, est un brin complexe. 
			
			Faisons quelques observations en premier lieu. Fixons $r = 3$, $m=2$. Le premier triplet de max $m$ dans l'ordre est $\left(0,0,2\right)$, et le dernier est $\left(2,2,2\right)$. Enumérons tous les triplets dans l'ordre lexicographique normal sur $\intint{0}{2}^3$, en faisant ressortir tous ceux qui ne sont pas dans l'ordre lexicographique étendu.
			
			$\left(0,0,2\right) 
				<_{\text{lex}} \redtext{\left(0,1,0\right)} 
				<_{\text{lex}} \redtext{\left(0,1,1\right)} 
				<_{\text{lex}} \left(0,1,2\right) 
				<_{\text{lex}} \left(0,2,0\right) 
				<_{\text{lex}} \left(0,2,1\right) 
				<_{\text{lex}} \left(0,2,2\right) 
				<_{\text{lex}} \redtext{\left(1,0,0\right)} 
				<_{\text{lex}} \redtext{\left(1,0,1\right) }
				<_{\text{lex}} \left(1,0,2\right) 
				<_{\text{lex}} \redtext{\left(1,1,0\right)}
				<_{\text{lex}} \redtext{\left(1,1,1\right)} 
				<_{\text{lex}} \left(1,1,2\right) 
				<_{\text{lex}} \left(1,2,0\right) 
				<_{\text{lex}} \left(1,2,1\right) 
				<_{\text{lex}} \left(1,2,2\right) 
				<_{\text{lex}} \left(2,0,0\right) 
				<_{\text{lex}} \dots  <_{\text{lex}} \left(2,2,2\right)$
				
			On peut voir l'énumération complète (avec l'ordre lexicographique normal) comme une liste des entiers écrits en base $m+1$, et avec l'ordre lexicographique étendu, comme une liste des entiers écrits en base $m+1$ contenant au moins une fois le chiffre $m$. On remarque aussi que si un $r$-uplet est de la forme $\left(x_1, \dots, x_{r-1}, m\right)$ mais que le $r$-uplet suivant dans l'ordre lexicographique ne contient pas le max, alors la dernière coordonnée restera $m$, et on incrémentera $x_{r-1}$ à la place, jusqu'à ce que $x_{r-1} = m$. 
			
			Le calcul crucial porte donc sur la dernière coordonnée et sur la nécessité d'effectuer une simulation du $r$-uplet suivant. %\footnotemark
			
				%\footnotetext{Je sais que pour l'instant ce n'est que du blabla, mais je ne me suis pas encore demandé comment le prouver. De plus, je pense qu'il s'agit plus d'une compréhension du fonctionnement de l'ordre. Mais il faudrait quand même le prouver.}
			
			Considérons que $m$ et $r$ sont fixés. Pour l'instant, on ne considère pas les cas particuliers où l'on change d'arité. De plus, on va numéroter les fonctions non pas par rapport aux LSRS précédents (bien que ceux que nous allons décrire se mettent à la suite des systèmes précédents), mais par rapport aux coordonnées qu'elles représentent. 
			
			Supposons que le $r$-uplet $x_1, \dots, x_r$ est représenté par $f_{1}(x), \dots, f_{r}(x)$. La simulation se fait dans les fonctions $f'_{1}(x), \dots, f'_{r}(x)$. 
			
			La simulation du $r$-uplet suivant dans l'ordre lexicographique est simple : il s'agit d'incrémenter le dernier chiffre de l'écriture en base $m+1$, et de gérer une retenue. Cela se fait facilement avec le système suivant :
			
			\begin{eqnarray}
				f'_{r}(x) & = &\casedist{
									f_r(x-1)+1 & \text{si $f_r(x-1) < m$} \\
									0 & \text{si $f_r(x-1) = m$}
								}\\
				f'_{i-1}(x) & = & \casedist{
									f'_{i-1}(x-1)+1 & \text{si $f'_{i}(x) = 0$ et $f'_{i}(x-1) = m$ et $f'_{i-1}(x-1) < m$} \\
									0 & \text{si $f'_{i}(x) = 0$ et $f'_{i}(x-1) = m$ et $f'_{i-1}(x-1) = m$} \\
									f'_{i-1}(x-1) & \text{sinon}
								} \\
				\text{future\_max}(x) & = & \max\left(f'_{r}(x), \dots, f'_{1}(x)\right)
			\end{eqnarray}
			
			La fonction $\max$ s'obtient à partir de $\max\left(a, b\right) = a + (b-a)$. 
						
			Une fois que le $r$-uplet suivant naïf a été calculé, il est temps de revenir à l'ordre lexicographique étendu :
			
			\begin{eqnarray}
				f_{r}(x) & = & \casedist{
									f_{r}(x-1) + 1 & \text{si $f_{r}(x-1) < m$} \\
									f_{r}(x-1) & \text{si $f_{r}(x-1) = m$ et \textquoted{\emph{le max n'apparaît pas au tour suivant}}}\\
									0 & \text{si $f_{r}(x-1) = m$ et \textquoted{\emph{le max apparaît au tour suivant}}}
									}\\
				f_{i-1}(x) & = & f'_{i-1}(x)
%				f_{r-1}(x) & = & \casedist{
%									f'_{r-1}(x) & \text{si $f_r(x) = f_{r}(x-1) + 1$}\\
%									f_{r-1}(x-1)+1 & \text{si $f_{r}(x) = m$ et $f_{r}(x-1) = m$ et $f_{r-1}(x-1) < m$} \\
%									f_{r-1}(x-1) & \text{si $f_{r}(x) = m$ et $f_{r}(x-1) = m$ et $f_{r-1}(x-1) = m$}
%								}\\
%				f_{i-1}(x) & = & \casedist{
%									f_{i-1}(x) & \text{si $f_i(x) = f_{i}(x-1) + 1$}\\
%									f_{i-1}(x-1)+1 & \text{si $f_{i}(x) = 0$ et $f_{i}(x-1) = m$ et $f_{i-1}(x-1) < m$} \\
%									f_{i-1}(x-1) & \text{si $f_{i}(x) = 0$ et $f_{i}(x-1) = m$ et $f_{i-1}(x-1) = m$}
%								}
			\end{eqnarray}
			
			Expliquons. 
			
			Le premier cas de $f_{r}(x)$ est trivial : si $f_{r}(x-1)<m$, alors le max est déjà quelque part dans le $r$-uplet, donc on peut incrémenter sans souci. Si $f_{r}(x-1) = m$, alors il faut faire attention. L'expression \textquoted{\emph{le max n'apparaît pas au tour suivant}} se traduit par : \textquoted{$\text{future\_max}(x) \neq m$}. Si le max apparaît dans la simulation du tour suivant, alors on peut incrémenter la $r$-ième composante du $r$-uplet sans risquer de perdre le max. Si le $r$-uplet suivant dans l'ordre lexicographique ne contient pas le max, le plus petit $r$-uplet dans l'ordre lexicographique qui possède le max dans ses composantes est celui avec les mêmes coordonnées que celui simulé, sauf la dernière composante, qui contient le max.
			
			Pour $i\leqslant r$, le calcul de $f_{i-1}(x)$ est beaucoup plus simple : il reprend dans tous les cas le résultat de la simulation. Le cas le plus pointilleux est celui pour $i=r$. Si $f_{r}(x) = f_{r}(x-1) + 1$ ou si le max apparaît dans la simulation, alors on suit l'ordre lexicographique naturel. Sinon, la simulation du tour suivant n'a pas fait apparaître le max. Dans ce cas, le $r$-uplet suivant dans l'ordre lexicographique étendu a les mêmes composantes que le $r$-uplet simulé, sauf pour la dernière coordonnée, qui est le max. Ainsi, quel que soit le résultat de la simulation, il est valable pour toutes les coordonnées, sauf la dernière, d'où ce calcul.
			
			
			Maintenant qu'on a les calculs pour arité et max fixés, il reste à gérer les cas où l'un des deux change. Les cas seront décrits informellement, mais il est facile de les traduire en une distinction de cas complétant celles données précédemment.
			
			\begin{itemize}[itemsep=-1mm]
				\item 	Premièrement, on a toujours accès à l'arité courante et au max courant. On peut aussi détecter quand il y a un changement (en comparant avec la valeur précédente). Ceci facilite les calculs suivants.
				
				\item 	Si l'arité courante n'est pas l'arité maximale, alors on impose que les fonctions "coordonnées" (écrites $f_i$ dans les LSRS précédents) renvoient $0$ constamment. Les indices des fonctions "coordonnées" étant fixés, il est facile d'insérer ce cas dans la distinction de cas. 
				
				\item 	 max $m$ fixé, le premier $r$-uplet d'une arité $r$ donnée est toujours de la forme $\left(0, \dots, 0, m\right)$. 
				
				\item 	Le premier $\leqa$-uplet d'un max $m$ donné est toujours $(m)$. 
			\end{itemize}
		
		
		
	\end{proof}
	
	Une fois qu'on a les coordonnées, le deuxième problème qui se pose est l'utilisation des coordonnées partielles, ce que transcrivait la notation $\overline{x}' \ll \overline{x}$ : $\overline{x}'$ est obtenu en prenant des coordonnées à $\overline{x}$, en les mélangeant, éventuellement en dupliquant certaines, tant que $\abs{\overline{x}'} < \abs{\overline{x}}$. Une fois qu'on a le $\leqa$-uplet $\overline{x}$, il est facile de récupérer $\overline{x}'$. Mais il faut ensuite convertir $\overline{x}'$ en son rang $x'$ afin que le LSRS d'arité $1$ puisse s'en servir. 
	
	\begin{lemma}
		\label{lem:sub_uplet_simulable}
		Si la fonction $\overline{x} \mapsto \overline{x}'$, où $\overline{x}' \ll \overline{x}$, est simulable par LSRS, alors la fonction $\overline{x} \mapsto \rang{\overline{x}'}$ est simulable par LSRS.
	\end{lemma}
	
	\begin{proof}
		Il suffit d'exprimer le calcul du rang décrit dans \hyperref[coro:rang_bon_ordre]{ce corollaire}. Le max de $\overline{x}'$ n'est pas forcément le même que celui de $\overline{x}$, donc il faudra rajouter une fonction qui récupérera son max $m'$, ainsi qu'une fonction qui renverra son arité $r'$ (le calcul de $\overline{x}'$ par rapport à $\overline{x}$ est toujours le même, donc il suffit de rajouter manuellement l'arité dans le LSRS). 
		
		Une fois ces résultats obtenus, le rang de $\overline{x}'$ s'exprime ainsi :
		
		\begin{equation}
			\text{rang}\left(\overline{x}'\right) = \left( \sum_{i=1}^{a} m'^i \right) + \left( \sum_{i=1}^{r'} \left( \left(m'+1\right)^i -1 \right) \right) + \left(\sum_{i=1}^{r} c_i \right)
		\end{equation}
		
		où $c_i = x'_i \times \left(m'+1\right)^{r'-i}$ si $x'_{i-1} = m'$ ou $x'_{i-2} = m'$ $\dots$ $x'_{1} = m'$, et $c_i = \left(m'+1\right)^{r'-i}-m'^{r'-i}$ sinon.
		
		Il est facile d'obtenir les puissances successives de $m'$, $m'+1$, de les sommer et de calculer les deux premières sommes ; cela a même été fait dans la preuve du lemme \ref{lem:decomp_rang_par_LSRS}. Le calcul des $c_i$ n'est pas plus compliqué ; s'inspirer de ce qui a été fait précédemment pour les calculs de puissances.
	\end{proof}
	
	
	\begin{lemma}
		\label{lem:operation_simulable}
		Supposons que $g_1, g_2$ sont des symboles de fonctions utilisés ou définies dans un $a$-LSRS, et sont simulées par $\hat{g_1},\hat{g_2}$ dans un LSRS.
		
		Alors l'opération $f\left(\overline{x}\right) = g_1\left(\bar{x_1}\right) * g_2\left(\bar{x_2}\right)$ d'un $a$-LSRS, pour $* \in \{+, -, \times \}$, est simulable par LSRS.
	\end{lemma}
	
	\begin{proof}
		Tout d'abord, présentons quelques notations. Notons $\text{sub}_j(x)$ l'application qui à un code $x$ associe $x_j$, code du $\leqa$-uplet $\bar{x_j} \ll \overline{x}$. 
		
		Dans la \hyperref[def:LSRS_2]{deuxième définition du LSRS}, on a rajouté l'opération de composition bornée ; cet ajout facilite le calcul suivant. 
		
		Le LSRS suivant\footnotemark  simule $f\left(\overline{x}\right) = g_1\left(\bar{x_1}\right) * g_2\left(\bar{x_2}\right)$ :
			
			\footnotetext{La numérotation est disjointe de celles précédemment données.}
		
		\begin{eqnarray}
			f_1(x) & = & \hat{g_1}\left[ \text{sub}_1(x) \right]_x \\
			f_2(x) & = & \hat{g_2}\left[ \text{sub}_2(x) \right]_x \\
			f^1(x) & = & f_1(x) * f_2(x) 
		\end{eqnarray}
	\end{proof}
	
	\begin{remark}
		\label{rk:fonctions_en_attente}
		Avant de traiter l'opération suivante, il faut apporter une précision. Soit le $2$-LSRS suivant :
		
		\begin{eqnarray}
			f_1(x) & = & n(x) + 1(x) \\
			f_2(x,y) & = & f_1(x) + 1(y) \\
			f_3(x,y) & = & f_1(x) + id(y)
		\end{eqnarray}
	
		Supposons-le simulé par un LSRS $S$ sur $\hat{f_1}, \hat{f_2}, \hat{f_3}$ (avec éventuellement d'autres fonctions, mais elles ne sont pas importantes pour la remarque).
		
		L'ordre est : $(0) < (0,0) < (1) < (0,1) < (1,0) < (1,1) < (2) \dots$. Soit $\overline{x}$ un $\left(\leqslant 2\right)$-uplet, notons $r(\overline{x})$ son rang. La définition donnée ici du déroulement d'un $a$-LSRS suppose, en quelque sorte, qu'une fois que $f_1(0)$ est calculé, on avance d'un pas dans l'ordre et $f_1(0)$ reste \emph{figé} en attendant qu'on ait calculé $f_2(0,0)$ et $f_3(0,0)$. Dans la simulation par un LSRS, chaque fonction renvoie à chaque tour un résultat ; ici, que devrait renvoyer $\hat{f_1}(r(0,0))$ ? Une idée pourrait être simplement de renvoyer son dernier résultat. L'idée est que $\hat{f_1}$ effectue son calcul sur $r\left(\overline{x}\right)$ tant que $\abs{\overline{x}} = \text{arité}\left(f_1\right)$, puis renvoie son dernier résultat tant que l'arité n'est pas la bonne. Il faut donc que chaque fonction ait accès à l'arité de la fonction qu'elle simule, ce qui peut se faire en rajoutant des fonctions auxiliaires, incluses dans le LSRS. 
		
		En conclusion, si $f\left(\overline{x}\right) = \sigma\left(\overline{x}\right)$ est une équation de $a$-LSRS, avec $f$ d'arité $r$, alors sa simulation est de la forme :
	
		\begin{eqnarray}
			\text{arité}_1(x) & = & 1(x) \\
			 & \dots & \nonumber \\
			\text{arité}_a(x) & = & \text{arité}_{a-1}(x) + 1(x) \\
			 & \dots & \nonumber \\
			\text{a\_c}(x) & = & \text{arité courante} \\
			 & \dots & \nonumber \\
			\hat{f}(x) & = & \casedist{
								\hat{\sigma}\left(\overline{x}\right) & \text{si $\text{a\_c}(x) = \text{arité}_r(x)$}\\
								\hat{f}(x-1) & \text{sinon}
								}
		\end{eqnarray}
	
		Dans la suite, pour des raisons de lisibilité, on évitera ce genre d'écriture, et on notera simplement la simulation sous sa forme $\hat{f}(x) = \hat{\sigma}\left(\overline{x}\right)$.
	\end{remark}
	
	\begin{lemma}
		Supposons que $g_1, g_2$ sont des symboles de fonctions utilisés ou définies dans un $a$-LSRS, et sont simulées par $\hat{g_1},\hat{g_2}$ dans un LSRS.
		
		Alors l'opération $f\left(\overline{x}\right) = \eqpredf{g_1}{g_2}{\overline{x}'}{\overline{x}'}$ d'un $a$-LSRS est simulable par un LSRS.
	\end{lemma}
	
	\begin{proof}		
		Pour simplifier, commençons par le cas où $\overline{x}' = \overline{x}$. 
		
		La \hyperref[rk:fonctions_en_attente]{remarque précédente} impose de revoir le calcul de $\eqpredf{g_1}{g_2}{\overline{x}}{\overline{x}}$. Rappelons la définition donnée plus haut : 
		
		\[
			\eqpred{g'}{g}{\overline{x}} = \casedist{
				g'\left( \bar{y} \right) & \text{où $\bar{y} = \maxset{\bar{z} < \overline{x} | g\left( \overline{x} \right) = g\left( \bar{z} \right)}$ si un tel $\bar{z}$ existe} \\
				x'_1 & \text{sinon, où $x'_1$ est la première coordonnée de $\overline{x}'$}
			}
		\]
		
		Observons cet exemple.
		
		\espace 
		\begin{center}
			\begin{tabular}{|c|cccccccc|}
				\hline
									& (0)	& (0,0)	& (1)	& (0,1)	& (1,0)	& (1,1)	& (2)	& \dots \\
				\hline
				$f_1(x)$ 			& $a_0$	& ND	& $a_1$	& ND	& ND	& ND	& $a_2$	& \dots \\
				$f_2(x)$ 			& $b_0$	& ND	& $b_1$	& ND	& ND	& ND	& $b_2$	& \dots \\
				$f_3(x)$	 		& $b_0$	& ND	& $b_0$	& ND	& ND	& ND	& $b_0$	& \dots \\
				$\eqpred{f_1}{f_2}{x}$
									& 0		& ND	& 1		& ND	& ND	& ND	& 2		& \dots \\
				$\eqpred{f_1}{f_3}{x}$
									& 0		& ND	& $a_0$	& ND	& ND	& ND	& $a_1$	& \dots \\
				\hline 
				
			\end{tabular}
		\end{center}

			
		\espace 
		(ND signifie "Non-Défini".)
			
		Supposons que ce tableau représente les valeurs de quelques fonctions unaires d'un $2$-LSRS. Une simulation naïve serait :
		
		\espace 
		
		\begin{center}
			\begin{tabular}{|c|cccccccc|}
				\hline
				$\overline{x}$			& (0)	& (0,0)	& (1)	& (0,1)	& (1,0)	& (1,1)	& (2)	& \dots \\
				\hline
				$x$					& 0 	& 1	 	& 2		& 3 	& 4 	& 5 	& 6 	& \dots \\
				\hline 
				$\hat{f_1}(x)$
									& $a_0$	& $a_0$	& $a_1$	& $a_1$	& $a_1$	& $a_1$	& $a_2$	& \dots \\
				$\hat{f_2}(x)$ 		
									& $b_0$	& $b_0$	& $b_1$	& $b_1$	& $b_1$	& $b_1$	& $b_2$	& \dots \\
				$\hat{f_3}(x)$ 		
									& $b_0$	& $b_0$	& $b_0$	& $b_0$	& $b_0$	& $b_0$	& $b_0$	& \dots \\
				$\eqpred{\hat{f_1}}{\hat{f_2}}{x}$
									& 0		& 0		& 2		& 2		& 2		& 2		& 6		& \dots \\
				$\eqpred{\hat{f_1}}{\hat{f_3}}{x}$
									& 0		& 0		& $a_0$ & $a_0$	& $a_0$	& $a_0$	& $a_1$	& \dots \\
				\hline
			\end{tabular}
		\end{center}
				
		La simulation en LSRS ne renvoie pas le même résultat par défaut, mais cela peut être réglé rapidement.
				
		Premièrement, $\eqpred{f_1}{f_2}{x}$ renvoie une coordonnée en cas d'échec de la recherche, alors que $\eqpred{\hat{f_1}}{\hat{f_2}}{x}$ renvoie le rang de l'entrée. C'est pour ça que les valeurs de sortie n'ont rien à voir. On peut corriger ce bug en rajoutant une nouvelle équation qui va tester si la recherche a échoué :
		
		\begin{eqnarray}
		g_1(x) 	& = 	& \eqpred{\text{id}}{\hat{f_2}}{x} = \casedist{
			\maxset{y<x | \hat{f_2}(x) = \hat{f_2}(y)} & \text{si de tels $y$ existent}\\
			x		& \text{sinon}
		} \\
		g(x) & = & \casedist{
			\eqpred{\hat{f_1}}{\hat{f_2}}{x} & \text{si $g_1(x) < x$}\\
			x_i		& \text{sinon}
		}
		\end{eqnarray}
		
		Deuxièmement, dans la simulation naïve les fonctions renvoient leur dernière valeur quand leur équivalente n'est pas définie. Cette condition permet d'assurer que l'equal-predecessor fonctionne correctement. Ici, l'equal-predecessor parcourt une ligne jusqu'à trouver une case dont la valeur est la même que celle de départ. On parcourt ensuite cette colonne pour arriver à la deuxième fonction de l'equal-predecessor et on regarde quelle valeur on trouve dans cette case ; c'est ce que renvoie l'equal-predecessor. Le fait de garder une fonction constante là où elle n'est pas définie remplit simplement des colonnes, et décale le numéro de colonne visitée, mais le résultat renvoyé est le bon. 
		
		Il s'agit plutôt d'une convention à appliquer. Dans le $a$-LSRS de base, on ne peut pas faire appel à des fonctions à d'autres arités que la leur, donc ça n'est pas gênant dans le cadre d'une simulation.
		

		
		Enfin, pour le cas général où on ne considère pas $\overline{x} = \overline{x}'$, on se ramène au cas précédent en utilisant $f'\left( \bar{y} \right) = \eqpredf{g_1}{g_2}{\bar{y}}{\bar{y}}$ et $f\left( \overline{x} \right) = f'\left( \overline{x}' \right)$, où $\abs{\bar{y}} = \abs{\overline{x}'}$.
		
	\end{proof}
	
	
	\begin{proof} (Du théorème \ref{thm:rep_a_LSRS_rep_na_LSRS})
		
		(Preuve du sens $\Rightarrow$ du théorème \ref{thm:rep_a_LSRS_rep_na_LSRS})
		
		On a donc un LSRS pour extraire les coordonnées de $\overline{x}$ à partir de leur rang $x$ (lemme \ref{lem:decomp_rang_par_LSRS}), et on a vu que chaque opération d'un $a$-LSRS pouvait se simuler par un LSRS.
		
		La taille de sortie du LSRS est égale à $\rang{\bar{n}} = \grozo{n ^a}$, où $\bar{n} = \left( n, \dots, n\right)$.
		
		\espace
		
		(Preuve du sens $\Leftarrow$ du théorème \ref{thm:rep_a_LSRS_rep_na_LSRS})
		
		Comme nous l'avons fait remarquer dans une \hyperref[rmq:choix_arites]{remarque précédente}, il est possible d'obtenir un $a$-LSRS dont toutes les fonctions sont d'arité $a$ à partir d'un LSRS d'arité 1. La remarque passait sous silence un détail subtil.
		
		Naïvement, il suffit de considérer qu'un entier $x < n^a$ peut être représenté par un $a$-uplet $\overline{x}$ où $\rang{\overline{x}} = x$, où le rang considéré est celui de l'ordre $<$ sur les $a$-uplets (et non pas les $\leqa$-uplets).
		
		Dès lors, les opérations $f(x) = g_1(x) * g_2(x)$ et $f(x) = \eqpred{g_1}{g_2}{x}$ se simulent naturellement par : $f\left(\overline{x}\right) = g_1\left(\overline{x}\right) * g_2\left(\overline{x}\right)$ et $f\left(\overline{x}\right) = \eqpred{g_1}{g_2}{\overline{x}}$.
		
		Il y a cependant un problème pour l'opération de composition $f(x) = g_1\left[g_2(x)\right]_x$, qui ne peut pas se simuler telle quelle ; cependant, rappelons qu'elle a été ajoutée à la définition \ref{def:LSRS_2} pour faciliter l'écriture, mais elle est simulable par un LSRS à partir des deux autres opérations de base \cite{GrandjeanSchwentick2002} ; il n'y a donc pas besoin de la prendre en compte.
	\end{proof}
	
	
	
	\pagebreak
	
	
	\section*{Conclusion}
	
	On a donc deux caractérisations fonctionnelles de $\textbf{P}$ et de ses sous-classes sur machine $\sigma$-RAM. La première caractérisation par LSRS \emph{simple}, et encore plus celle par LSRS à arité multiple, sont des outils théoriques, mais leur utilisation pratique n'est pas aisée.
	
	La caractérisation par LSRS \emph{simple} a pour principal défaut que sa puissance est limitée au moment de l'écriture du LSRS, puisque son temps de calcul est défini à ce moment-là. 
	
	La caractérisation par LSRS à arité multiple reproduit ce même défaut, puisque cette fois, la puissance de calcul est limitée au moment du choix des arités utilisées. De plus, l'ordre utilisé, s'il est intéressant théoriquement, n'est en pratique pas simple d'usage, car il n'est pas intuitif de réfléchir à un algorithme avec ce type de déroulement.
	
	Il reste donc à modifier ces caractérisations pour les rendre plus utilisables en pratique. 
	
	
	
	% Euh......
	
	
%	Construire une clique de taille $k$ =====> Pour l'exposé.
%	
%	Peut-être essayer de faire une preuve $\grozo{n^a} \Rightarrow$ $a$-LSRS =====> NO TIME LEFT
%	
%	Expliciter le sens manquant : $a$-LSRS $\Rightarrow$ $n^a$-représentable par LSRS normal. =====> DONE
%	
%	Mettre des trucs en annexes parce que c'est trop long. =====> DONE
	
	
	
	
	
	
	
	
	
	
%	\bibliographystyle{plain}
%	\bibliography{biblio}
%	
%\end{document}

















