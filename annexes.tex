	
	
	% ----------------------------------------------------------------------------
	% ----------------------------------------------------------------------------
	% 
	% ANNEXES 
	% 
	% ----------------------------------------------------------------------------
	% ----------------------------------------------------------------------------
	

	\begin{appendices}
		\label{appendices}
		
		
		
		\chapter{Simulation de la $\sigma$-RAM par la $\bbA$-RAM}
			\label{sec:annexes_programmes}
			
			\section{Conventions d'écriture}
			\label{subsec:conventions_ecriture}
			Pour des raisons de lisibilité, on écrira des fonctions pour $\bbA$-RAM. Pour faire appel à la fonction, on suivra la convention d'écriture suivante : 
			
			\[
				\text{(fn)} \ s_a \text{NOM\_FONCTION} (\text{arguments})s_b
			\]
			ce qui se lit : à l'état $s_a$, appliquer la fonction puis passer à l'état $s_b$. Si la fonction contient un embranchement, comme ce sera le cas pour l'égalité, alors $s_b$ est inutile mais devrait être précisé comme \emph{bonne pratique} (à l'instar du slash de \text{<br />} en HMTL). De plus, cela permet d'indiquer, à l'écriture d'une fonction, quel est l'état final de la machine qui calcule la fonction.
			
			Deuxièmement, on s'autorisera une boucle \emph{For}, non pas pour rendre le calcul plus puissant, mais bien en tant qu'astuce de lisibilité. Il s'agit en fait d'une boucle \emph{méta} sur l'écriture des commandes.
			
			Par exemple :
			
			\espace
			
			\begin{algorithm}[H]
				\For{\text{$p$ from $1$ to $r$}}{
					($p$-dest) $s_{a_p} \alpha \pi_p s_{a_{p+1}}$\;
					}
			\end{algorithm}
			
			\espace
			
			doit se comprendre :
			
			\espace
			
			\begin{algorithm}[H]
				($1$-dest) 	$s_{a_1} \alpha \pi_1 s_{a_{2}}$\;
				$\vdots$		\hspace{1cm} $\vdots$ \\
				($r$-dest) 	$s_{a_r} \alpha \pi_r s_{a_{r+1}}$\;
			\end{algorithm}
			
			\espace
			
			c'est-à-dire comme un enchaînement de commandes. 
			
			
			Troisièmement, pour alléger l'écriture, on autorisera de corriger les labels d'états en notant $s_a = s_b$ (utile par exemple, lors d'une boucle \emph{méta} sur des indices d'états).
			
			Par exemple, en reprenant notre exemple précédent, les commandes suivantes :
			
			\espace
			
			\begin{algorithm}[H]
				\For{\text{$p$ from $1$ to $r$}}{
					($p$-dest) $s_{a_p} \alpha \pi_p s_{a_{p+1}}$\;
				}
				(correction)		$s_{a_{r+1}} = s_b$
			\end{algorithm}
			
			\espace
			
			doivent se comprendre :
			
			\espace
			
			\begin{algorithm}[H]
				($1$-dest) 	$s_{a_1} \alpha \pi_1 s_{a_{2}}$\;
				$\vdots$		\hspace{1cm} $\vdots$ \\
				($r$-dest) 	$s_{a_r} \alpha \pi_r s_{b}$\;
			\end{algorithm}
			
			\espace
			
			(Remarquons que le dernier état est devenu $s_b$ au lieu de $s_{a_{r+1}}$.)
			
			
			\espace
			
			
			Enfin, on suppose que les états n'engendrent pas de conflits : si le label de deux états est différent, alors les deux états sont différents. Si un état a le même label dans deux fonctions différentes, alors ces deux états sont différents (chaque fonction est vue comme une boîte noire qui n'agit \emph{que} sur ses paramètres, d'où parfois la nécessité de mettre en paramètre un ou des états).
		
		
		\section{Ecriture des programmes}
		\label{subsec:ecriture_programmes}
		
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_COPY}

			\textbf{Fonction} $s_{a_1}\text{COPY}(\alpha, \beta, \bar{\pi}) s_b$
			
			\tcc{Récupère chacun des premiers sous-termes de $\alpha$ puis réassemble ces sous-termes dans $\beta$.}
			
			\espace 
			
			\KwIn{$\alpha, \beta, \pi_1, \dots, \pi_r$}
			\tcp{Détruit le terme contenu dans $\alpha$, stocke ses composantes dans chaque $\pi_p$, puis reconstruit le terme dans $\beta$.}
			
			\For{\text{$p$ from $1$ to $r$}}{
				($p$-dest) 	$s_{a_p} \alpha \pi_1 s_{a_{p+1}}$\;
				}
			
			(switch) 	$s_{a_{r+1}} \alpha s_{b_1} \dots s_{b_k}$\;
			
			
			\For{\text{$i$ from $1$ to $k$}}{
				(const) 	$s_{b_i} \pi_1 \dots \pi_{r_i} C_i \beta s_b$\;
				}
			
			\caption{Programme de la fonction $s_{a_1}\text{COPY}(\alpha, \beta, \bar{\pi}) s_b$. Algorithme \hyperref[algo:A_RAM_fn_COPY]{ci-dessus}.}
		\end{algorithm}
		
		\espace
		
		Pour coder le test d'égalité, on a besoin de quelques sous-fonctions. On fera ici un premier usage du constructeur dédié $\text{MEM}\left( -, -, -\right)$.
		
		\espace
		
		\begin{algorithm}[H]
			\label{algo:A_RAM_fn_extract_first}
			
			\textbf{Fonction} $s_a \text{EXTRACT\_FIRST}(\mu, \pi_1) s_f$

			\espace 
			
			\KwIn{$\mu, \pi_1$}
			\tcp{Si $\mu$ est un terme de mémoire non vide, on extrait la première valeur de cette mémoire en laissant le reste inchangé.}
			
			\If{le terme contenu dans $\mu$ est de la forme $\text{MEM}\left( i, A, m\right)$}{
				Stocker $A$ dans $\pi_1$ \;
				Remplacer le contenu de $\mu$ par $m$ \;
			}
			
			\caption{Fonction $s_a \text{EXTRACT\_FIRST}(\mu, \pi_1) s_f$. }
		\end{algorithm}
		
		\espace
		
		Cette fonction calcule en $3$ étapes. Le code se trouve ici :
		
		\espace
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_extract_first}
			
			\textbf{Fonction} $s_a \text{EXTRACT\_FIRST}(\mu, \pi_1) s_f$
			
			\espace
			
			\KwIn{$\mu, \pi_1$}
			\tcp{Si $\mu$ est un terme de mémoire non vide, on extrait la première valeur de cette mémoire en laissant le reste inchangé.}
			
			(switch)	$s_a \mu s_f s_{\text{MEM}}$\;
			
			(2-dest) 	$s_{\text{MEM}} \mu \pi_1 s_b$\;
			(3-dest) 	$s_b \mu \mu s_f$\;
			
			
			\caption{Fonction $s_a \text{EXTRACT\_FIRST}(\mu, \pi_1) s_f$. Algorithme \hyperref[algo:A_RAM_fn_extract_first]{plus haut}. }
		\end{algorithm}
		
		\espace 
		
		Cette fonction va de pair avec la fonction $\text{DECOMPOSE}$.

		\espace 
		
		\begin{algorithm}[H]
			\label{algo:A_RAM_fn_DECOMPOSE}
			
			\textbf{Fonction} $s_a \text{DECOMPOSE}(\alpha, \pi, \pi', \bar{\pi}) s_c$
			
			\espace 
			
			\KwIn{$\alpha, \pi, \pi', \bar{\pi}$}
			
			\tcp{Détruit le terme contenu dans $\alpha$, stocke ses sous-termes dans une seule mémoire $\pi$ et copie $\alpha$ dans $\pi'$. Les registres $\bar{\pi}$ sont des registres de travail utiles à la fonction $\text{COPY}$.}
			
			Selon le constructeur extérieur de $\alpha$ \;
			
			\ForEach{sous-terme de $\alpha$}{
				Stocker le sous-terme dans $\pi'$\;
				Construire dans $\pi$ le terme $\text{MEM}\left( \pi', \pi', \pi \right)$ \;
			}
			
			Copier $\alpha$ dans $\pi'$ \;
			
			\caption{Fonction $s_a \text{DECOMPOSE}(\alpha, \pi, \pi', \bar{\pi}) s_c$.}
		\end{algorithm}
		
		\espace
		
		Cette fonction calcule en $\leq (k+1)r+3$ étapes. On peut voir ici un premier usage de la mémoire, comme liste non ordonnée indexée par des termes. Le code de cette fonction se trouve là :
		
		\espace
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_DECOMPOSE}
			
			\textbf{Fonction} $s_a \text{DECOMPOSE}(\alpha, \pi, \pi', \bar{\pi}) s_c$
			
			\espace
			
			\KwIn{$\alpha, \pi, \pi', \bar{\pi}$}
			
			\tcp{Détruit le terme contenu dans $\alpha$, stocke ses sous-termes dans une seule mémoire $\pi$ et copie $\alpha$ dans $\pi'$. Les registres $\bar{\pi}$ sont des registres de travail utiles à la fonction $\text{COPY}$.}
			
			(switch) 	$s_a \alpha s_{a_1} \dots s_{a_k,j}$\;
			
			\For{$i$ from $1$ to $k$} {
				\For{$j$ from $r_i$ to $1$}{
					($j$-dest) 		$s_{a_{i-1},j} \alpha \pi' s_{b_{i-1},j}$\;
					(const) 		$s_{b_{i-1},j} \pi' \pi' \pi \text{MEM} \pi s_{a_{i-1},j+1}$\;
					(correction) 	$s_{a_{i-1},r_i+1} = s_{a_i,1}$\;
				}
				(correction)	$s_{a_k,1} = s_b$\;
			}
			
			(fn) 	$s_b \text{COPY}(\alpha, \pi', \bar{\pi}) s_c$\;
			
			\caption{Fonction $s_a \text{DECOMPOSE}(\alpha, \pi, \pi', \bar{\pi}) s_c$.}
		\end{algorithm}
				
		\espace 		
		
		On peut enfin coder le test d'égalité. On rappelle que l'algorithme se trouve \hyperref[algo:A_RAM_fn_IF]{ici}.
		
		\espace 
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_IF}
			
			\textbf{Fonction} $\text{IF}(\alpha, \beta, s_1, s_0, \pi_1, \pi_2, \pi'_1, \pi'_2)$
			
			\tcc{Vérifie si $\alpha = \beta$ en faisant une analyse inductive sur la construction des termes qu'ils contiennent, c'est-à-dire en vérifiant si le constructeur extérieur est le même, puis en déconstruisant le terme et effectuant ces mêmes comparaisons sur les sous-termes.}
			
			\espace 
			
			\KwIn{$\alpha, \beta, s_1, s_0, \pi_1, \pi_2, \pi'_1, \pi'_2$}
			\tcc{$\alpha, \beta$ : les registres contenant les termes à tester.\\
				$s_1, s_0$ : place la machine dans l'état $s_1$ si $\alpha = \beta$, $s_0$ sinon. \\
				$\pi_1, \pi_2$ : registres de travail ; contiendront respectivement la liste des sous-termes de $\alpha$ et $\beta$ qui n'ont pas encore été comparés. \\
				$\pi'_1, \pi'_2$ : registres de travail ; contiendront les sous-termes courants. \\ \\
				Vérifie si $\alpha = \beta$ en faisant une analyse inductive sur la construction des termes qu'ils contiennent.}
			
			\espace
			\espace
			
			(switch)	$s_a \alpha s_{b_1} \dots s_{b_k}$\;
			
			\For{$i$ from $1$ to $k$}{
				(switch)	$s_{b_i} \beta s_0 \dots s_c \dots s_0$\;
				
				\tcp{Pour la commande $s_{b_i}$, l'état $s_c$ est en $i$-ième position. Ceci permet de vérifier si les constructeurs extérieurs de $\alpha$ et $\beta$ sont les mêmes.\\
					Si $C_i$ est un constructeur d'arité $0$, alors on écrit $s_1$ au lieu de $s_c$ (il n'y a pas de sous-terme à vérifier).}
			}
			
			\espace
			
			(const)		$s_c \varepsilon \pi_1 s_{c_1}$\;
			(const)		$s_{c_1} \varepsilon \pi_2 s_{c_2}$\;
			
			\tcp{On initialise les registres de travail qui serviront à stocker les sous-termes de $\alpha$ et $\beta$.}
			
			\espace
			
			(fn)		$s_{c_2} \text{DECOMPOSE}(\alpha, \pi'_1, \pi_1) s_{c_3}$\;
			(fn)		$s_{c_3} \text{DECOMPOSE}(\beta, \pi'_2, \pi_2) s_{d_1}$\;
			
			\tcp{On récupère les sous-termes de $\alpha$ et $\beta$, on les stocke dans $\pi_1$ et $\pi_2$, et on récupère le premier sous-terme dans $\pi'_1$ et $\pi'_1$.}
			
			\espace
			
			(fn)		$s_{d_1} \text{EXTRACT\_FIRST}(\pi_1, \pi'_1) s_{d_2}$\;
			(fn)		$s_{d_2} \text{EXTRACT\_FIRST}(\pi_2, \pi'_2) s_{e_1}$\;
			
			\tcp{On récupère le premier sous-terme mémorisé dans $\pi_1, \pi_2$ pour le mettre dans $\pi'_1, \pi'_2$.}
			
			\espace
			
			(switch)	$s_{e_1} \pi'_1 s_{f_1} \dots s_{f_k}$\;
			
			\For{$i$ from $1$ to $k$}{
				(switch)	$s_{f_i} \beta s_0 \dots s_g \dots s_0$\;
				
				\tcp{De même que pour $s_{b_i}$, à la commande $s_{f_i}$, l'état $s_c$ est en $i$-ième position. \\
					De plus, si le constructeur $C_i$ est d'arité $0$, alors au lieu de $s_g$, on écrit $s_{g_2}$. }
			}
			
			\espace
			
			(fn)		$s_{g} \text{DECOMPOSE}(\pi'_1, \pi'_1, \pi_1) s_{g_1}$\;
			(fn)		$s_{g_1} \text{DECOMPOSE}(\pi'_2, \pi'_2, \pi_2) s_{g_2}$\;
			
			\tcp{On récupère les sous-termes de $\pi'_1$ et $\pi'_2$, on les stocke dans $\pi_1$ et $\pi_2$, et on remplace $\pi'_1$ et $\pi'_2$ par leur premier sous-terme.}
			
			\espace
			
			(switch)	$s_{g_2} \pi_1 s_{g_3} s_{g_4}$\;
			(switch)	$s_{g_3} \pi_2 s_{1} s_{0}$\;
			(switch)	$s_{g_4} \pi_2 s_{0} s_{d_1}$\;
			
			\tcp{On vérifie les contenus des mémoires temporaires. Si les deux mémoires sont vides ($s_{g_2} \rightarrow s_{g_3} \rightarrow s_{1}$) alors il y a égalité entre les termes de départ (on a fini de tout comparer). Si l'une des mémoires est vide et l'autre non ($s_{g_2} \rightarrow s_{g_3} \rightarrow s_0$ ou $s_{g_2} \rightarrow s_{g_4} \rightarrow s_0$) alors les deux termes n'étaient pas égaux. Enfin, si les deux mémoires ont encore des termes à vérifier, alors on revient à l'état $s_{d_1}$.}
			
			\caption{Fonction $\text{IF}(\alpha, \beta, s_1, s_0, \pi_1, \pi_2, \pi'_1, \pi'_2)$. Algorithme \hyperref[algo:A_RAM_fn_IF]{ci-dessus}.}
		\end{algorithm}
		
		\espace
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_INSERT}
			
			\textbf{Fonction} $\text{INSERT}\left( \mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2\right)$
			
			\tcc{Insère la valeur de $\beta$ dans la case mémoire d'indice $\alpha$.}
			
			\espace 
			
			\KwIn{$\mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2$}
			
			\tcc{
				$\mu, (\mu_i)_{i\in 4}$ sont tels que décrits plus haut.\\
				$\alpha$ est l'indice auquel on veut insérer. \\
				$\beta$ est la valeur qu'on veut insérer à l'indice $\alpha$. \\
				$\pi_1, \pi_2, \pi'_1, \pi'_2$ sont les registres de travail du test d'égalité.
			}
			
			\espace
			
			\tcp{Initialisation de la mémoire}
			
			$\aramconsz{a}{\varepsilon}{\mu_4}{a_1}$ \;
			$\aramdest{$1$}{a_1}{\mu}{\mu_1}{a_2}$ \;
			$\aramdest{$2$}{a_2}{\mu}{\mu_2}{a_3}$ \;
			$\aramdest{$3$}{a_3}{\mu}{\mu_3}{a_4}$ \;
			$\aramconst{a_4}{\mu_1 \mu_2 \mu_3}{\text{MEM}}{\mu_3}{b}$ \;
			
			\espace
			
			\tcp{Première boucle : on avance dans la mémoire en vérifiant à chaque fois si on est au bon indice.}
			
			$\aramsw{b}{\mu_3}{s_{\varepsilon} s_{\text{MEM}}}$ \;
			
			\espace
			
			$\aramconst{\varepsilon}{\alpha \beta \mu_3}{\text{MEM}}{\mu_3}{b}$ \;
			
			\espace
			
			$\aramfn{\text{MEM}}{\text{IF}\left( \mu_1, \alpha, s_{\text{true}}, s_{\text{false}}, \pi_1, \pi_2, \pi'_1, \pi'_2 \right)}{s_f} $\;
			
			$\aramconst{\text{true}}{\alpha \beta \mu_3}{\text{MEM}}{\mu_3}{c}$ \;
			
			\espace 
			
			$\aramconst{\text{false}}{\mu_1 \mu_2 \mu_4}{\text{MEM}}{\mu_4}{f_1}$ \;
			$\aramdest{$1$}{f_1}{\mu}{\mu_1}{f_2}$ \;
			$\aramdest{$2$}{f_2}{\mu}{\mu_2}{f_3}$ \;
			$\aramdest{$3$}{f_3}{\mu}{\mu_3}{b}$ \;
			
			\espace 
			
			\tcp{Deuxième boucle : étape inverse : on récupère tout ce qu'on a visité précédemment.}
			
			$\aramsw{c}{\mu_4}{s_{c,\varepsilon} s_{c, m}}$ \;
			
			\espace
			
			$\aramconst{c, \varepsilon}{\mu_1 \mu_2 \mu_3}{\text{MEM}}{\mu_3}{\text{copie}}$ \;
			
			
			$\aramdest{$1$}{c,m}{\mu_4}{\mu_1}{m_1}$ \;
			$\aramdest{$2$}{m_1}{\mu_4}{\mu_2}{m_2}$ \;
			$\aramdest{$3$}{m_2}{\mu_4}{\mu_4}{m_3}$ \;
			$\aramconst{m_3}{\mu_1 \mu_2 \mu_3}{\text{MEM}}{\mu_3}{\text{copie}}$ \;
			
			$\aramfn{\text{copie}}{\text{COPY}\left( \mu_3, \mu, \mu_1, \mu_2, \mu_4 \right)}{f}$ \;
			
			\caption{Fonction $\text{INSERT}\left( \mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2\right)$. Algorithme \hyperref[algo:A_RAM_fn_INSERT]{ici}. }
		\end{algorithm}
		
		\espace
		
		\begin{algorithm}[H]
			\label{prog:A_RAM_fn_ACCESS}
			
			\textbf{Fonction} $\text{ACCESS}\left( \mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2\right)$
			
			\tcc{Copie la valeur de la case mémoire d'indice $\alpha$ dans $\beta$.}
			
			\espace 
			
			\KwIn{$\mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2$}
			
			\tcc{
				$\mu, (\mu_i)_{i\in 4}$ sont tels que décrits plus haut.\\
				$\alpha$ est l'indice auquel on veut accéder. \\
				$\beta$ est le registre dans lequel on veut stocker la valeur d'indice $\alpha$. \\
				$\pi_1, \pi_2, \pi'_1, \pi'_2$ sont les registres de travail du test d'égalité.
			}
			
			\espace
			
			\tcp{Initialisation de la mémoire}
			
			$\aramconsz{a}{\varepsilon}{\mu_4}{a_1}$ \;
			$\aramdest{$1$}{a_1}{\mu}{\mu_1}{a_2}$ \;
			$\aramdest{$2$}{a_2}{\mu}{\mu_2}{a_3}$ \;
			$\aramdest{$3$}{a_3}{\mu}{\mu_3}{a_4}$ \;
			$\aramconst{a_4}{\mu_1 \mu_2 \mu_3}{\text{MEM}}{\mu_3}{b}$ \;
			
			\espace
			
			\tcp{Boucle : on avance dans la mémoire en vérifiant à chaque fois si on est au bon indice.}
			
			$\aramsw{b}{\mu_3}{s_{c} s_{\text{MEM}}}$ \;
			
			\espace
			
			$\aramfn{\text{MEM}}{\text{IF}\left( \mu_1, \alpha, s_{c}, s_{\text{false}}, \pi_1, \pi_2, \pi'_1, \pi'_2 \right)}{s_f} $\;
			
			\espace 
												
			$\aramdest{$1$}{\text{false}}{\mu_3}{\mu_1}{f_1}$ \;
			$\aramdest{$2$}{f_1}{\mu_3}{\mu_2}{f_2}$ \;
			$\aramdest{$3$}{f_2}{\mu_3}{\mu_3}{b}$ \;
			
			\espace 

			
			$\aramfn{c}{\text{COPY}\left( \mu_2, \beta, \mu_1, \mu_2, \mu_4 \right)}{f}$ \;
			
			\caption{Fonction $\text{ACCESS}\left( \mu, \mu_1, \mu_2, \mu_3, \mu_4, \alpha, \beta, \pi_1, \pi_2, \pi'_1, \pi'_2\right)$. Algorithme \hyperref[algo:A_RAM_fn_ACCESS]{ici}. }
		\end{algorithm}
		
		\espace
		
		
		
		
		
		
		
		
		
	\chapter{Vers une caractérisation algébrique du temps polynomial sur $\sigma$-RAM}
		\label{chap:LSRS}
		
		\section{Première adaptation du résultat de Grandjean et Schwentick}
		\label{sec:LRS_et_temps_poly}
		
		Ici se trouve la démonstration de l'adaptation du résultat de Grandjean et Schwentick, basé uniquement sur l'élargissement des domaines de départ et d'arrivée des fonctions.
		
		Cette partie est une annexe à \ref{sec:theorem_principal_grandjean_schwentick}.
		
		\begin{conj}
			\label{conj:big_theorem}
			Soit $\Gamma$ une fonction de RAM. Les propositions suivantes sont équivalentes :
			
			\begin{enumerate}[itemsep=-1mm]
				\item 	\label{num:gamma_in_nk}
				$\Gamma \in \dtimeramarg{K}$.
				
				\item 	\label{num:gamma_nk_rep_LSRS}
				$\Gamma$ est $n^K$-représentée par un LSRS.
				
				\item	\label{num:gamma_nk_rep_LRS}
				$\Gamma$ est $n^K$-représentée par un LRS.
			\end{enumerate}
		\end{conj}
		
		On a montré $\ref{num:gamma_nk_rep_LSRS} \Rightarrow \ref{num:gamma_nk_rep_LRS}$ \hyperref[conj:rep_LSRS_rep_LRS]{juste dessous.}. 
		La preuve de $\ref{num:gamma_nk_rep_LRS} \Rightarrow \ref{num:gamma_in_nk}$ se trouve \hyperref[conj:rep_LRS_calc_n_K]{après}.
		La preuve de $\ref{num:gamma_in_nk} \Rightarrow \ref{num:gamma_nk_rep_LSRS}$ se trouve \hyperref[conj:poly_implique_LSRS]{plus loin}.
		
		
		\begin{conj}[Passage de LSRS à LRS]
			\label{conj:rep_LSRS_rep_LRS}
			Toute fonction de RAM $n^K$-représentée par LSRS est aussi $n^K$-représentable par LRS.
		\end{conj}
		
		\begin{proof}
			%(La preuve qui suit est quasiment identique à celle de \cite{GrandjeanSchwentick2002}, adaptée au temps polynomial.)
			
			Soit $\Gamma$ une fonction de RAM $n^K$-représentée par un LSRS $S$ sur $F_{t_1}$ pour $f_0, \dots, f_{k-1}$, comme dans la définition, et soit $P$ la projection affine associée. Soit $s$ une RAM-structure $c$-bornée de type $t_1$. On note $s' = S(s)$ la structure définie par $S$ avec entrée $s$, et on note $s'' = \Gamma(s) = P\left( (s.n)^K, s'\right)$. Pour simplifier les notations, on va simplement écrire $n$ au lieu de $s.n$.
			
			L'idée est de coder les fonctions $f_0, \dots, f_{k-1} : cn^K \to cn^K$ par une unique fonction $g$. Pour s'assurer que la fonction \emph{Equal-Predecessor} fonctionne correctement, le codage des $f_i$ doit avoir des domaines disjoints et des images disjointes. Cela peut se faire, pour chaque $i<k$, en n'utilisant que des valeurs congrues à $i$ modulo $k$ pour la fonction $f_i$. Puisqu'on va en avoir besoin pour coder/décoder les opérations, on va aussi coder la fonction $\div k$ dans $g$.\\
			
			On va étendre le domaine de $g$ pour encoder $\Gamma(s).f$ pour chaque symbole de fonction $f$ du type de la structure de sortie.
			
			Précisément, $g$ va être définie sur $2kcn^K$ de sorte que :
			
			\begin{itemize}[itemsep=-1mm]
				\item	pour tout $b \in kcn^K$, on a $g(b) = b \div k$ ;
				\item 	pour tous $a \in cn^K$ et $i \in k$, on a $g\left( kcn^K + ka + i \right) = kcn^K + k f_i(a) + i$ $\textcolor{red}{(\star)}$
			\end{itemize}
			% Commentaire bidon
			
			\espace
			
			%			\fbox{
			%				\begin{minipage}{0.9\textwidth}
			%					
			%					{\footnotesize \begin{minipage}{0.95\textwidth}
			%						On a donc besoin d'un moyen de calculer $n^K$ dans la fonction de sortie.
			%						
			%						Pour l'instant, pour des raisons de simplicité, puisqu'on sait que la fonction est $n^K$-représentable, on va supposer qu'on a accès à une fonction $i \mapsto n^K$. Pour ce faire, on peut soit la définir par multiplication en rajoutant $K$ équations \emph{(ce qui ne me semble pas naturel, parce qu'on modifie le LSRS à chaque fois qu'on veut changer la taille d'arrivée ?)} soit on peut rajouter une fonction constante $i \mapsto n^K$ \emph{(on ajoute ou remplace une fonction dans les fonctions de base du LSRS ? Ça veut dire qu'on rajoute un symbole de fonction au type de départ, qui sait déjà ce qu'on va en faire ?)}.
			%						
			%						Problème : apparemment le LSRS sait qu'il représente des fonctions de taille $n^K$, mais est-ce suffisant pour justifier l'insertion de cette fonction en tant que fonction du domaine ? Apparemment, un même LSRS peut être utilisé pour toutes les tailles de domaines, donc peut-être qu'on peut, au cas par cas, rajouter une fonction en plus selon le domaine d'arrivée ?
			%						
			%						(La question ici est de savoir si on peut rajouter naturellement cette fonction constante d'accès direct au max, sans avoir l'air de tricher pour se faciliter la vie.)
			%						
			%						Ou bien on peut voir l'ajout de cette fonction comme un ajout de contrainte. C'est l'ajout de cette fonction qui impose que le domaine d'arrivée sera de taille $\grozo{n^K}$.
			%					\end{minipage} }
			%				
			%					
			%					\redtext{Solution}
			%					%En fait, dans la démonstration originale, $kcn$ n'est pas calculée à la volée dans le système. Le LSRS est défini à l'aide d'une fonction constante $n(-)$ qui, malgré la notation, n'a rien à voir avec le $n$ de $kcn = kcs.n$. En fait, $n(-)$ permet d'accéder à la borne du domaine de définition des fonctions du LSRS. Ici, $n(y) = kcs.n$, ce qui permet au LSRS d'avoir accès à cette valeur. Dans le cas polynomial, on a $n(y) = kc(s.n)^K$, ce qui est bien ce qu'on voulait. La démonstration se tient. 
			%
			%					
			%				\end{minipage}
			%			}
			
			
			\begin{remark}
				Dans la démonstration originale, $kcn$ est calculé à la volée dans le LRS. Il s'agit d'une abréviation : $cn = \underset{c \text{ fois}}{\underbrace{n(y) + \dots + n(y)}}$, et $kcn = \underset{k \text{ fois}}{\underbrace{cn + \dots + cn}}$. 
				
				La constante $c$ est toujours une valeur explicite, donnée par définition de la fonction. On peut donc construire ce terme. Idem pour $k$, qui est le nombre d'équations. 
				
				Dans notre cas, il faut autoriser la multiplication pour permettre l'écriture d'un terme analogue : $n^K = \underset{K \text{ fois}}{\underbrace{n(y) \times \dots \times n(y)}}$, puis $cn^K = \underset{c \text{ fois}}{\underbrace{n^K + \dots + n^K}}$, et $kcn^K = \underset{k \text{ fois}}{\underbrace{cn^K + \dots + cn^K}}$. 
				
				L'ajout de la multiplication ne rend pas le LSRS, le LRS et la RAM plus puissants, d'après la partie 3.1 et la proposition 2.1 de \cite{GrandjeanSchwentick2002}. 
			\end{remark}
			
			\espace
			
			Revenons à la démonstration.
			
			En d'autres termes, pour $b \geqslant kcn^K$ et $b \mod{k} = j$, on a $g(b) = kcn^K + k f_j\left( (b-kcn^K) \div k \right) + j$.
			On va définir la valeur de $g(b)$ pour $b \in 2kcn^K$ par distinction de cas sur $b \mod{k}$, comme décrit dans (1) à (3) ci-dessous.
			
			\begin{enumerate}[itemsep=-1mm,leftmargin=2cm]
				%\setlength{\itemsep}{-1mm}
				\item  
				Renumérotons les symboles de $F_{t_1} = \{f | f \in t_1\} \cup \{f_C | C \in t_1\} \cup \{1(-), n(-), id(-)\}$ de la manière suivante : $\{ f^0, \dots, f^{l-1} \}$. On va limiter les occurrences de ces symboles. Premièrement, les symboles $f_0, \dots, f_{k-1}$ sont remplacés par $f^{l}, \dots, f^{l+k-1}$ respectivement. Ensuite, on va introduire $l$ nouvelles fonctions $f_0, \dots, f_{l-1}$ et $l$ équations pour les définir :
				
				\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
					\item	Si $f^i$ vient de $t_1$, alors l'équation associée est $f_i(x) = f^i(x)$ ;
					\item 	Si $f^i = id$, alors l'équation associée est $f_i(x) = x$ ;
					\item 	Si $f^i = f_C$, ou $1$ ou $n$, alors l'équation associée est (respectivement) $f_i(x) = C, 1, n$. 
				\end{itemize}
				
				% Subitem ne marche pas ?
				
				On appellera ces équations des \emph{équations d'entrée} ; elles servent justement à remplacer les entrées du LSRS.
				
				Enfin, on remplace dans le LSRS $S$ tous les anciens symboles de fonctions (ceux de $F_{t_1}$) par les nouveaux (les $(f_i)_{i \in l+k}$). Après ces remplacements, $S$ ne contient plus aucune référence à $F_{t_1}$, sauf pour les équations d'entrée. 
				
				\item 	
				Les équations de $S$ sont combinées en une seule équation $g(y) = \sigma(y)$ comme suit. Le terme $\sigma(y)$ est principalement une distinction de cas dépendant de $y$ et $y \mod{k}$. 
				
				\[
				g(y) = 
				\left\lbrace \begin{array}{ll}
				0 				& \text{si $y \leqslant k-1$} \\
				g(y-k) + 1 		& \text{si $k \leqslant y < kcn^K$} \\
				\sigma_i(y)		& \text{si $kcn^K \leqslant y$ et $y \mod{k} = i$}
				\end{array}\right.
				\]
				
				où $\sigma_i(y)$ est un terme récursif qu'on explicitera tout de suite après. 
				
				Notons que les deux premiers cas donnent $g(y) = y \div k$ pour chaque $y < kcn^K$, comme voulu. Cela nous permet d'exprimer $y \mod{k}$ pour $kcn^K \leqslant y < 2kcn^K$. %, puisque $(y-kcn^K) - kg(y-kcn^K) = (y-kcn^K) - \sum_{j=1}^{k} g(y-kcn^K)$. \redtext{(Et alors ???)}
				
				Ensuite, on a vu que la distinction de cas ne rendait pas le LSRS plus puissant. 
				
				Enfin, on décrit la construction des termes de récurrence $\sigma_i(y)$ (on distingue la variable $y$ du terme de récurrence, de la variable $x$ des équations) :
				
				
				% RELU JUSQU'ICI ------------------------------------------------------------------------------------------------------------------------------
				
				
				
				\begin{itemize}[itemsep=-1mm, leftmargin=1cm]
					\item
					Si $E_i$ est une équation d'entrée, alors $\sigma_i(y)$ est construit comme suit :
					
					\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
						\item %\subsubitem -		
						Si le terme de droite de l'équation est une constante $C$ (éventuellement $1$ ou $n$), alors $\sigma_i(y) = kcn^K + kC + i$ ;
						
						\item %\subsubitem	-	
						Si le terme de droite de l'équation est $x$, alors $\sigma_i(y) = kcn^K + kg\left( y - kcn^K \right) + i$ ;
						
						\item %\subsubitem	-	
						Si le terme de droite de l'équation est $f^i(x)$, alors $\sigma_i(y) = kcn^K + k f^i\left( g\left( y - kcn^K \right) \right) + i$.
						
						Justifions pourquoi cette définition de $\sigma_i(y)$ est correcte. On le fait pour le troisième cas ; les deux autres sont plus simples. Soit $b = kcn^K + ka + i$ avec $a < cn^K$. Alors $g(b-kcn) = g(ka + i) = (ka + i) \div k = a$, et $\sigma_i(b) = kcn^K + kf^i(a) + i$, comme voulu. 
					\end{itemize}
					
					
					\item
					Si $E_i$ est de la forme $f_i(x) = f_j(x) - f_{j'}(x)$, alors $\sigma_i(y)$ est défini par :
					
					\[
					\sigma_i(y) = \left( g(y - \delta) - k cn^K - j \right) - \left( g(y - \delta') - k cn^K - j' \right) + kcn^K + i
					\]
					
					où $\delta = i -j$ et $\delta' = i - j'$ et, par définition d'un LSRS, $i > j, j'$. Pour vérifier que cette expression est correcte, soient $a \in cn^K$, $b = kcn + ka + i$, où $i<k$. Si $g(b - \delta) = g(kcn^K + ka + j) = kcn^K + kf_j(a) + j$ et $g(b - \delta') = g(kcn^K + ka + j') = kcn^K + kf_{j'}(a) + j'$ alors :
					
					\[
					\left( g\left( b - \delta \right) - kcn^K - j \right) - \left( g\left( b - \delta' \right) - kcn^K - j' \right)
					=  k \left( f_{j}(a) - f_{j'}(a) \right) + kcn^K + i
					\]
					
					
					Ce qui est ce qu'on voulait.
					
					\item
					Si $E_i$ est de la forme $f_i(x) = f_j(x) + f_{j'}(x)$, alors son traitement est similaire au cas précédent, à ceci près qu'il faut que l'addition $x + y$ renvoie $0$ si $x + y > cn^K$. On redéfinit alors $\sigma_i(y)$ : 
					
					\[
					\sigma_i(y) = \casedist{
						\tau(y) + kcn^K + i & \text{si $\tau(y) < kcn^K$} \\
						kcn^K + i & \text{sinon}
					}
					\]
					
					où $\tau(y) = \left( g\left( b - \delta \right) - kcn^K - j \right) - \left( g\left( b - \delta' \right) - kcn^K - j' \right)$ avec $\delta = i - j$ et $\delta' = i - j'$. 
					
					La vérification se passe de la même manière que précédemment.
					
					\item
					Si $E_i$ est de la forme $f_i(x) = \eqpred{f_{j'}}{f_j}{x}$, où $j < i$ et on suppose sans perte de généralité que $i \leqslant j'$ 
					\footnote{Si $i > j'$ alors on doit rajouter une nouvelle fonction $f_l$ à $S$, telle que $l > i$, et définie par la nouvelle équation $f_l(x) = f_{j'}$ et remplacer $E_i$ par $f_i(x) = \eqpred{f_{l}}{f_j}{x}$}
					, alors $\sigma_i(y)$ est définie par :
					
					\[
					\sigma_i(y) = \eqpredfi{g}{g}{y-\delta}{ + \delta'}{y} - j' + i
					\]
					
					où $\delta = i - j$ et $\delta' = j' - j$.
					
					Pour justifier ce remplacement, on doit s'assurer que le codage de plusieurs fonctions en une seule ne cause par d'effets de bord quand on utilise \emph{Equal-Predecesor}. Ce qui est crucial ici, c'est que, pour chaque $i<k$, les valeurs de $g$ qui codent $f_i$ soient congruentes à $i$ modulo $k$. Pour être plus précis, soit $b = kcn^K + ka + i$, avec $a< cn^K$. Alors $b-\delta = kcn^K + ka + i - (i - j) \underset{i>j}{=} kcn^K + ka + j$. On a deux sous-cas à étudier :
					
					\begin{itemize}[itemsep=-1mm,leftmargin=1cm]
						\item
						$f_j^{\leftarrow}(a) = a$.   Dans ce cas, pour aucun $a' < a$, on n'a $f_j(a') = f_j(a)$, donc il n'y a pas de $a' < a$ pour lequel $g\left( kcn^K + ka' + j \right) = g\left( kcn^K + ka + j \right)$. La définition de $g$ assure que, pour chaque $e \geqslant kcn^K$, on a $\left(g(e) \mod{k} = j \Leftrightarrow e \mod{k} = j\right)$ et $\forall e, e' \in 2kcn^K$, si $g(e) = g(e')$, alors soit $e, e' < kcn^K$, soit $e, e' \geqslant kcn^K$. Donc $g^{\leftarrow} \left( kcn^K + ka + j \right) = kcn^K + ka + j$ et $g^{\leftarrow}\left( b - \delta \right) + \delta' = kcn^K + ka + j' \geqslant kcn^K + ka + i = b$. Ainsi :
						
				 
						\begin{eqnarray}
						\sigma_i(b) 	& = &	\eqpredfi{g}{g}{b-\delta}{+\delta'}{b} - j' + i \\
						& = & 	\left( kcn^K + ka + j' \right) - j' + i \\
						& = & 	kcn^K + ka + i \\
						& = & 	kcn^K + k \eqpred{f_{j'}}{f_j}{a} + i \\
						& = & 	kcn^K + k f_i(a) + i \\
						& = & 	g(b),
						\end{eqnarray}
						
						comme voulu.
						
						\item
						$f_j^{\leftarrow}(a) = a'$ pour un certain $a' < a$. Dans ce cas, $g\left( kcn^K + ka + j \right) = kcn^K + ka' + j$. En conséquence : 
						
						\begin{equation}
							g^{\leftarrow}\left( b - \delta \right) + \delta' = kcn^K + ka' + j' < kcn^K + ka + i = b
						\end{equation}
						
						Donc :
						
				 
						\begin{eqnarray}
						\sigma_i(b) 	& = &	\eqpredfi{g}{g}{b-\delta}{+\delta'}{b} - j' + i \\
						& = & 	g\left( kcn^K + ka' + j' \right) - j' + i \\
						& = & 	\left( kcn^K + k f_{j'}(a') + j' \right) - j' + i \\
						& = & 	kcn^K + k f_{j'}(a') + j' + i \\
						& = & 	kcn^K + k \eqpred{f_{j'}}{f_j}{a} + i \\
						& = & 	kcn^K + k f_i(a) + i \\
						& = & 	g(b),
						\end{eqnarray}
						
						comme souhaité.
					\end{itemize}
				\end{itemize}
				
				
				\item	
				Maintenant, on complète le LRS pour $g$. Pour une question de simplicité, on va supposer que $t_2$ ne contient que le symbole de constante $n$ et un seul symbole de fonction $h$. Soient $j <k$ et $\alpha$ une fonction affine tels que, pour toute structure $s$, on ait $\Gamma(s).n = P\left((s.n)^K, s'\right).n = s'.f_j\left(\alpha\left( (s.n)^K \right)\right)$, et soient $i <k$ et $A$ une fonction affine tels que, pour toute structure $s$ et tout $a < \Gamma(s).n$, on ait $\Gamma(s).h(a) = P\left((s.n)^K, s'\right).h(a) = s'.f_i\left(A\left( (s.n)^K, a \right)\right)$. Ici, on ne fait que donner des noms aux éléments qui permettent de définir la structure $\Gamma(s)$ ; ces fonctions affines $\alpha, A$ et ces entiers $i,j$ existent par \hyperref[def:representee_par_LSRS]{définition d'une fonction $n^K$-représentable par un LSRS}.
				
				On a construit $g$ de telle manière que toutes les valeurs de fonctions $f_i(a)$ sont, en quelque sorte, disponibles dans $g$, mais on a encore deux problèmes à résoudre. Premièrement, les $f_i(a)$ apparaissent uniquement sous une forme codée ; deuxièmement, elles ne forment pas un intervalle contigu mais sont éparpillées modulo $k$. Il faut donc, avant d'extraire les valeurs à l'aide d'une projection affine bien choisie, décoder les valeurs des fonctions et les ramener dans un même intervalle. Pour ça, on élargit le domaine de $g$ à $(2k+2)cn^K$ et on complète la définition de $g$ :
				
				\[
				g(y) = \casedist{
					\text{comme avant} & \text{si $y < 2kcn^K$} \\
					g\left[ g\left[ k\left( y - 2kcn^K \right) + kcn^K  + i\right]_y - kcn^K  \right]_y & \text{si $2kcn^K \leqslant y < (2k+1)cn^K$} \\
					g\left[ g\left[ k\left( y - (2k + 1)cn^K \right) + kcn^K  + j \right]_y - kcn^K  \right]_y & \text{si $(2k+1)cn^K \leqslant y < (2k+2)cn^K$} \\
				}
				\]
				
				Il découle de l'équation \redtext{$(\star)$} (la définition de $g$ sur $kcn^K$) et de cette définition que, pour tout $a < cn^K$, on a :
				
		 
				\begin{eqnarray}
					g\left( 2kcn^K + a\right)	& = & 		g\left[ g\left[ k\left( \left(2kcn^K + a\right) - 2kcn^K \right) + kcn^K  + i\right]_{2kcn^K + a} - kcn^K  \right]_{2kcn^K + a} \label{eqn:res_01} \\
												& = & 		g\left[ g\left[ k a + kcn^K  + i\right]_{2kcn^K + a} - kcn^K  \right]_{2kcn^K + a} \label{eqn:res_02}\\
												& = & 		g\left[ g\left( k a + kcn^K  + i\right) - kcn^K  \right]_{2kcn^K + a} \label{eqn:res_03}\\
												& = & 		g\left[ \left(kcn^K  +  k f_i(a) + i\right) - kcn^K  \right]_{2kcn^K + a} \label{eqn:res_04}\\
												& = & 		g\left[ k f_i(a) + i \right]_{2kcn^K + a} \label{eqn:res_05}\\
												& = & 		f_i(a) \label{eqn:res_06}
				\end{eqnarray}
					
				Analysons :
					\begin{itemize}
						\setlength{\itemsep}{-1mm}
						\item	$\ref{eqn:res_01} \rightarrow \ref{eqn:res_02}$ : par soustraction propre.
						\item 	$\ref{eqn:res_02} \rightarrow \ref{eqn:res_03}$ : par définition de l'opérateur \emph{application bornée}.
						\item 	$\ref{eqn:res_03} \rightarrow \ref{eqn:res_04}$ : parce que $k a + kcn^K  + i < 2kcn^K$ donc on reprend la définition de la fonction $g$ sur l'intervalle $2kcn^K$.
						\item 	$\ref{eqn:res_04} \rightarrow \ref{eqn:res_05}$ : par soustraction propre.
						\item 	$\ref{eqn:res_05} \rightarrow \ref{eqn:res_06}$ : parce que $f_i(a) < cn^K$ et on a le bon décalage avec $i$.
					\end{itemize}	
					
					
				De la même manière, on obtient, pour $a < cn^K$, $g\left( (2k+1)cn^K + a\right) = f_j(a)$. 
				
				Maintenant, il est aisé de définir une projection affine $P'$ qui extrait $\Gamma(s)$ à partir de $s'''$ de type $\{g\}$, définie par le LRS :
				
				\begin{equation}
					\Gamma(s).n = s'''.g\left( \alpha\left( cn^K\right) + (2k+1)cn^K\right) \label{eq:gamma_n_01}
				\end{equation}
				
				et : 
				
				\begin{eqnarray}
					\Gamma(s).h(a) 	& = &	P\left( n^K, s' \right).h(a) \label{eqn:gamma_h_01}\\
									& = &	s'.f_i\left( A\left( n^K, a \right) \right) \label{eqn:gamma_h_02}\\
									& = & 	s'''.g\left( 2kcn^K + A\left( n^K, a \right) \right) \label{eqn:gamma_h_03}
				\end{eqnarray}
				
				Expliquons. Le résultat de \ref{eq:gamma_n_01} est bien ce qu'il nous faut, parce que $\alpha': a \mapsto \alpha\left(ca\right) + (2k+1)ca$ est bien une fonction affine telle que $P'\left(n^K, s'\right).n = s'''.g\left(\alpha'\left(n^K\right)\right)$.
				
				%\footnote{On rappelle que $s'$ est la structure résultante du LSRS originel, de type $\{f_0, \dots, f_{k-1}\}$, qu'on a compactée dans une seule structure de type $\{g\}$.}
				Les calculs pour $\Gamma(s).h(a)$ se justifient comme suit :
					\begin{itemize}
						\setlength{\itemsep}{-1mm}
						\item 	$\ref{eqn:gamma_h_01} \rightarrow \ref{eqn:gamma_h_02}$ : par définition de $A$ et $i$.
						\item 	$\ref{eqn:gamma_h_02} \rightarrow \ref{eqn:gamma_h_03}$ : parce qu'on a $f_i(a) = g\left( 2kcn^K + a \right)$ par construction de $g$.
					\end{itemize}

				
				Le résultat est de plus exprimé sous la forme demandée : $A' : a,b \mapsto 2kcb + A\left( b, a \right)$ est bien une fonction affine telle que $P'\left(n^K, s'\right).h(a) = s'''.g(A'(n^K, a))$.
			\end{enumerate}
			
			
		\end{proof}
		
		

		
		
		\subsection{$n^K$-représentable par LRS $\Rightarrow$ calculable en temps $\grozo{n^K}$}
		\label{subsec:LRS_implique_poly}
		
		\begin{conj}
			\label{conj:rep_LRS_calc_n_K}
			Si une fonction de RAM $\Gamma$ est $n^K$-représentable par LRS alors $\Gamma \in \dtimeramarg{K}$.
		\end{conj}
		
		\begin{proof}
			{
				% Je sais, c'est très sale de les mettre ici. Je trouve juste l'écriture plus agréable avec ces macros, et c'est un usage purement local.
				\newcommand{\Fin}{F_{\text{in}}}
				\newcommand{\Ginv}{G_{\text{inverse}}}
				
				Soit $\Gamma$ une fonction de RAM $n^K$-représentée par une équation $E$, et soit $P$ la projection affine correspondante. Pour des raisons de simplicité, on va considérer que le type d'entrée de $\Gamma$ ne contient qu'un seul symbole de fonction $f_{\text{in}}$. Rappelons-nous que $E$ est une équation de la forme $g(x) = \sigma(x)$, où $\sigma(x)$ est une terme de récursion, et qu'il existe des constantes $c$ et $K$ telles que la sortie $s' = \Gamma(s)$ peut être extraite de $(s.n)^K$ et $g : cn^K \to cn^K$.
				
				Au lieu de définir formellement la RAM pour $\Gamma$, on va donner un algorithme qui pourra facilement être converti en une RAM à plusieurs mémoires, puis en une RAM à une mémoire.
				
				Sa structure de données associée sera constituée de variables $p$, $x$ et, en plus de la structure d'entrée $s$ et de la structure de sortie $s'$, de quatre tableaux à une dimension $\Fin, G, \Ginv$ et $EP$. 
				
				Dans $p$, on va stocker $c(s.n)^K$ qui borne le domaine de $g$, définie par $E$ et $s$.

				Ensuite, on a décrire le sens de $F_{\text{in}}, G, G_{\text{inverse}}$ et $EP$. Pour des indices plus grands que $s.n$, $F_{\text{in}}$ vaut $0$; pour les indices plus petits que $s.n$, $\Fin$ contient la valeur de $s.f$. Le calcul principal se fait en $p$ étapes, numérotés de $0$ à $p-1 = cn^K-1$.
				
				Après le tour n°$i$, on devrait avoir les invariants suivants : 
				
				\begin{enumerate}[itemsep=-1mm,leftmargin=1cm,label=(\alph*)]
					\item	pour tout $j < p$, $G[j]= \casedist{
						g(j) 	& 	\text{si $j \leqslant i$} \\ 
						j		&	\text{si $i < j < p$}
					}$ ;
					\item 	$EP[j] = g^{\leftarrow}(j)$ pour chaque $j \leqslant i$ ;
					\item 	pour tout $j < p$, $\Ginv[j] = \casedist{
						\maxset{l \leqslant i | g(l) = j} 	& 	\text{si un tel $l$ existe} \\
						p									& 	\text{sinon}
					}$ .
				\end{enumerate}
				
				Le calcul des valeurs de $G[i]$ est assez immédiat. On associe à chaque terme de récurrence $\sigma(x)$ un terme de programmation $\sigma[x]$ de la façon suivante : 
				
				\begin{itemize}[itemsep=-1mm]
					\item 	Si $\sigma(x)$ est $1, n, x$ alors $\sigma[x]$ est $1, n, x$, respectivement ;
					
					\item 	Si $\sigma(x) = g^{\leftarrow}(x - \delta)$ pour une certain $\delta$, alors $\sigma[x] = EP[x - \delta]$ ;
					
					\item 	Si $\sigma(x) = g[\tau(x)]_x$ pour un certain terme de récurrence $\tau(x)$, alors $\sigma[x] = G[\tau[x]]$ ;
					
					\item 	Si $\sigma(x) = f_{\text{in}}(\tau(x))$ pour un certain terme de récurrence $\tau(x)$, alors $\sigma[x] = \Fin[\tau[x]]$ ;
					
					\item 	Si $\sigma(x) = \tau_1(x) * \tau_2(x) $ pour des termes de récurrence $\tau_1(x)$ et $\tau_2(x)$, alors $\sigma[x] = \tau_1[x] * \tau_2[x]$.
				\end{itemize}
				
				Au tour $i$, on calcule $G[i]$ en évaluant $\sigma[i]$. La seule difficulté est de bien évaluer les termes $g^{\leftarrow}(x-\delta)$, qui devrait prendre plus qu'un nombre constant d'étapes pour être évalué, suivant la méthode directe. Au lieu de faire ça, on va utiliser le tableau $EP$ qui contient, après le tour $i$, la valeur de $g^{\leftarrow}(j)$ pour chaque $j \leqslant i$. $\Ginv$ contient toujours une inverse partielle de $g$ sur $\intint{0}{j}$ et est utilisé pour calculer $EP$. 
				
				Voici l'algorithme pour $\Gamma$ :
				
				\begin{algorithm}[H]
					\KwIn{s}
					
					\tcp{Initializations}
					
					$p := c(s.n)^K$ \;
					$EP[0] := 0$ \;
					\For{$j=0 \text{ to } s.n-1$}{
						$\Fin[j] := s.f(j)$\;
					}
					\For{$j=s.n \text{ to } p-1$}{
						$\Fin[j] := 0$\;
					}
					\For{$j = 0 \text{ to } p-1$}{
						$G[j] := j$\;
						$\Ginv[j] := p$ \;
					}
					
					\tcp{Main loop}
					
					\For{$i=0 \text{ to } p-1$}{
						$G[i] = \sigma[i]$ \;
						$EP[i] := \minset{i, \Ginv[G[i]]}$ \;
						$\Ginv[G[i]] := i$ \;
					}
					
					\KwOut{Compute $s''$ by applying the affine projection $P_{(s.n)^K}$ to the 	structure $s' = (p, G)$}
				\end{algorithm}
				
				On montre par induction que les invariants (a), (b), (c) sont maintenus pendant chaque étape de calcul. Bien évidemment, les invariants sont respectés après l'initialisation, c'est-à-dire, avant le tour $0$. 
				Pour l'induction, supposons que les invariants (a), (b), (c) sont respectés \emph{avant} le tour $i \in p$. On montre qu'ils sont valides \emph{après} le tour $i$, et donc avant le tour $i+1$. 
				
				\begin{itemize}
					\item 	On a $G[i] = g(i)$ parce que, par induction, $G[j] = g[j]_i$ pour tout $j \in p$, et $EP[j] = g^{\leftarrow}(j)$ pour tout $j \in i$ ; ainsi, l'opérateur d'application bornée et la fonction \emph{Equal-Predecesor} sont évaluées correctement.
					
					\item 	L'assignation $EP[i] := \minset{i, \Ginv[G[i]]}$ implique que $EP[i] = g^{\leftarrow}(i)$ par induction.
					
					\item 	Il est immédiat de voir que (c) est maintenu par l'assignation $\Ginv[G[i]] := i$. 
				\end{itemize}
				
				Donc (a), (b) et (c), et en particulier (a), sont maintenus, donc le programme est correct.
				
				Le nombre d'étapes pour évaluer un terme de récurrence est linéaire en la longueur du terme. Comme le terme de récurrence de $E$ est fixé, il s'agit d'un nombre constant d'étapes. Ainsi, $G[i]$ n'a besoin que d'un nombre constant d'étapes, de sorte que le programme tourne en temps $\grozo{p} = \grozo{n^K}$.
				
			}
			
		\end{proof}
		
		
		\subsection{Calculable en temps polynomial $\Rightarrow$ LSRS}
		\label{subsec:poly_implique_LSRS}
		
		\begin{conj}
			\label{conj:poly_implique_LSRS}
			Si une fonction de RAM $\Gamma$ est calculable en temps $\grozo{n^K }$ sur une $\{+, -, \times \}$-RAM $M$, alors $\Gamma$ est $n^K$-représentable par un LSRS.
		\end{conj}
		
		\begin{proof}
			Soit $\Gamma$ une fonction de RAM. Pour des raisons de simplicité, on va supposer que les types d'entrée et de sortie de la fonction n'ont qu'un seul symbole de fonction $f$. Soit $M$ une RAM calculant $\Gamma$ comme stipulé dans les hypothèses. Soit $c$ tel que le temps de calcul est borné par $c n^K$ sur des entrées $c(\Gamma)$-bornées de taille $n$.
			
			On va construire un LSRS qui utilise des fonctions $I, A, B, R_A, N$, qui décrivent l'état courant de la RAM \emph{avant} chaque étape $x$, de la façon suivante :
			
			\begin{itemize}[itemsep=-1mm]
				\item	$I(x)$ contient le numéro courant d'instruction du programme de la RAM ;
				\item 	$A(x), B(x), N(x)$ contiennent les valeurs des registres $A, B, N$ de $M$ ;
				\item 	$R_A(x)$ contient la valeur du registre dont l'adresse est actuellement dans le registre $A$.
			\end{itemize}
			
			Par commodité, on va aussi utiliser des fonctions $I', A', B', R_A', N'$, qui décrivent l'état de la RAM \emph{après} l'étape $x$. 
			
			En définissant $I(x)$ par distinction de cas, une bonne partie de la simulation de la RAM est immédiate. Par exemple, si $M$ exécute $A:= A + B$ alors les équations doivent forcer $A'(x) = A(x) + B(x)$. Le principal problème réside dans l'instruction $A:= R_A$, qui récupère le contenu d'un registre. Notons que les fonctions définies dans $S$ n'encodent pas explicitement les valeurs de tous les registres de $M$ à chaque étape $t$ mais seulement le contenu du registre dont l'indice est contenu dans $A$. Pour obtenir la valeur de $R_A(t)$, on doit trouver le dernier instant avant $t$ auquel le registre $A$ avait la valeur $A(t)$. Si un tel instant n'existe pas, on doit se référer à l'entrée. Rappelons que $s.f(i)$ est stocké dans $R_i$ au début du calcul. C'est pour cela que l'opération de récursion entre en jeu\footnotemark.
			
			\footnotetext{C'est l'opération $g, g' \mapsto \eqpred{g'}{g}{x}$}
			
			Pour faciliter les instructions $A:=R_A$, on va diviser le domaine en trois sous-domaines : $\intint{0}{cn^K-1}$, $\intint{cn^K}{2cn^K-1}$ et $\intint{2cn^K}{3cn^K-1}$.
			On va utiliser le premier intervalle pour stocker $s.f$, le deuxième pour simuler le calcul de $M$, et le troisième pour extraire la fonction de sortie ; pour ce faire, on va utiliser $R_A$ de manière à encoder la sortie (c'est un usage différent de celui introduit).
							
			On va s'autoriser la définition par cas (on a vu dans un lemme précédent que ça ne rendait pas le LSRS plus puissant) et quelques opérations simples qui ne sont pas directement disponibles dans la définition d'un LSRS. Le système qu'on va proposer n'est pas un LSRS à proprement parler, mais sa traduction en véritable LSRS est immédiate, quoiqu'elle nécessite quelques fonctions en plus.
			
			Pour simplifier la présentation, on va présenter les définitions des fonctions sur les trois intervalles ; elles peuvent être combinées via un LSRS. 
			
			Dans un LSRS, l'ordre est important. Ici, les fonctions doivent être présentées dans cet ordre : $I$, $A$, $B$, $N$, $R_A$, $I'$, $A'$, $B'$, $N'$, $R_A'$.
			
			Dans l'article d'origine \cite{GrandjeanSchwentick2002} de ce brouillon, les auteurs prennent la convention que l'addition est bornée ; si $a+b > cn$ où $cn$ est la taille du domaine, alors $a+b$ est en fait évalué à $0$ dans le LSRS. On peut utiliser cette convention pour simuler une définition par cas sur la taille entière du domaine. Si le LSRS est défini sur $3cn^K$ alors le test $x < cn^K$ peut se simuler avec le test : $x=0$ ou $x + x + x >0$.
			
			Ainsi, avant de définir les autres fonctions, intéressons-nous à un moyen d'obtenir $cn^K$ dans le LSRS, ce qui nous sera fort utile pour la suite.
			
			\begin{equation}
			f_0(x) = \casedist{
				1 & \text{ si $x = 0$} \\
				x + 1 & \text{ si $3x >0$} \\
				f_0(x-1) & \text{ sinon}
			}
			\end{equation}
			
			
			La définition de $f_0$ est telle que $f_0(cn^K-1) = cn^K$ et pour $x>cn^K$, $f_0(x) = cn^K$. 
			
			On peut utiliser cette astuce pour définir, en même temps que la phase d'initialisation, une fonction qui permet d'obtenir $cn^K$. Si $x<cn^K$, c'est-à-dire, $x=0$ ou $x + x + x >0$, on définit $A$ et $R_A'$ par :
			
			
			%Pour $x<cn^K$, $A$ et $R_A$ sont définies par\footnotemark :
			
			%	\footnotetext{On pourrait profiter de cette initialisation pour définir une fonction qui va calculer }
			
	 
			\begin{eqnarray}
			A(x) & = & x \\
			R_A'(x) & = & f(x)
			\end{eqnarray}
			
			Les autres fonctions valent $0$ sur cet intervalle, sauf $f_0(x) = x+1$. Pour l'instant, on n'utilise pas $f_0(x)$, qui de toute façon n'est pas prête à l'emploi.
			
			Cette initialisation va permettre de récupérer les valeurs de $R_A$ dans la deuxième partie du domaine. 
			
			\espace
			
			Sur le domaine $\intint{cn^K}{2cn^K-1}$ : 
			
			\espace 
			
			\fbox{
				\begin{minipage}{0.9\textwidth}
					
			 
					\begin{eqnarray}
					I(x) & = & \casedist{
						1			& 	\text{si $x = c n^K$} \\
						I'(x-1)		& 	\text{sinon}
					} \\
					I'(x) & = & \casedist{
						i			& 	\text{si $I(x)$ est de la forme $\sRAMif{i}{j}$ et $A(x) = B(x)$} \\
						j			& 	\text{si $I(x)$ est de la forme $\sRAMif{i}{j}$ et $A(x) \neq B(x)$} \\
						I(x)		&  	\text{si $I(x)$ est de la forme $\text{HALT}$} \\
						I(x) + 1 	& 	\text{sinon} 
					} \\
					A(x) & = & \casedist{
						0			& 	\text{si $x = cn^K$} \\
						A'(x-1)		& 	\text{sinon}
					} \\
					A'(x) & = & \casedist{
						c			& 	\text{si $I(x)$ est de la forme $A := c$} \\
						A(x) * B(x)	& 	\text{si $I(x)$ est de la forme $A := A + B$} \\
						R_A(x) 		& 	\text{si $I(x)$ est de la forme $A := R_A$} \\
						N(x)		& 	\text{si $I(x)$ est de la forme $A := N$} \\
						A(x)		& 	\text{sinon}
					} \\
					B(x) & = & \casedist{
						0			& 	\text{si $x = cn^K$} \\
						B'(x-1)		& 	\text{sinon}
					} \\
					B'(x) & = & \casedist{
						A(x)		& 	\text{si $I(x)$ est de la forme $B := A$} \\
						B(x)		& 	\text{sinon}
					} \\
					N(x) & = & \casedist{
						n			& 	\text{si $x = cn^K$} \\
						N'(x-1)		& 	\text{sinon}
					} \\
					N'(x) & = & \casedist{
						A(x)		& 	\text{si $I(x)$ est de la forme $N := A$} \\
						N(x)		& 	\text{sinon}
					} \\
					R_A(x) & = & \eqpred{R_A'}{A}{x} \\
					R_A'(x) & = & \casedist{
						B(x)		& 	\text{si $I(x)$ est de la forme $R_A := B$} \\
						R_A(x)		& 	\text{sinon}
					}
					\end{eqnarray}
					
				\end{minipage}
			}
			
			\espace 
			
			Notons que $cn^K$ code le premier instant du calcul. 
			
			Les fonctions $I(x)$ et $I'(x)$ ne prennent qu'un nombre fini de valeurs. Ainsi, en écrivant \emph{si $I(x)$ est de la forme $R_A := B$}, on écrit en fait \emph{si $I(x) = i_1$ ou $I(x) = i_2$ \dots ou $I(x) = i_m$} où $i_1, \dots, i_m$ sont les numéros des instructions $R_A := B$ dans le programme de la RAM.
			
			De plus, les opérations $I'(x-1)$ sont en réalité écrites $\eqpred{I}{1}{x}$ dans un vrai LSRS. Comme on l'a précédemment dit, on utilise $R_A$ pour extraire les valeurs de sortie du calcul de $M$, donné par la valeur $n'$ de $N$ et le contenu $R(0), \dots, R(n'-1)$, à la fin du calcul. Pour $2cn^K \leqslant x < 3cn^K$, le LSRS consiste en les équations suivantes :
			
	 
			\begin{eqnarray}
			A(x) & = & x - 2cn^K \\
			R_A(x) & = & \eqpred{R_A'}{A}{x}
			\end{eqnarray}
			
			Les autres fonctions du LSRS sont définies par $h(x) = h(x-1)$ (les fonctions stationnent).
			
			Il ne reste plus qu'à voir que les équations de $S$ définissent les fonctions attendues pour décrire le calcul de $M$ et que $S$ produit la bonne sortie.
			
			Soit $s$ la RAM-structure, $1(-), n(-), id(-)$ \footnotemark comme précédemment, et soient $I, A, B, N, R_A, I', A', B', N', R_A'$ des fonctions qui vérifient les équations de $S$.
			
				\footnotetext{On obtient la fonction $i \mapsto cn^K$ avec une astuce. En fait, le $n^K$ doit être explicité, et on peut le faire, même si cette preuve le passe sous silence pour des raisons de clarté. On peut calculer $cn^K$ en rajoutant quelques équations : $c$ et $K$ étant fixés, si on a la multiplication, il suffit de rajouter $K$ équations $f_1(x) = n(x)$ et $f_{i+1}(x) = f_i(x) \times n(x)$, puis $c-1$ équations $f_{K+j+1}(x) = f_{K+j}(x) + f_{K}(x)(x)$ ; on obtient alors $cn^K$ dès le premier tour de calcul. En revanche, si on n'a que l'addition, on ne peut pas reproduire ce schéma. On peut envisager d'étudier $g$ non pas sur $\intint{0}{3cn^K-1}$ mais sur $\intint{0}{4cn^K-1}$, et de couper ce domaine en quatre ; le premier domaine servirait alors à calculer $cn^K$ (mais comment délimiter ce premier domaine explicitement, avec un LSRS ?) et le reste serait le même que présenté ici.}
			
			Premièrement, il est clair que pour $a < c(s.n)^K$, on a :
			
	 
			\begin{eqnarray}
			A(a) & = & a \\
			R_A'(a) & = & s.f(a) \\
			I(a) & = & B(a) = N(a) = R_A(a) = I'(a) = A'(a) = B'(a) = N'(a) = 0
			\end{eqnarray}
			
			Les valeurs des fonctions au point $c(s.n)^K$ décrivent la configuration de la RAM au début du calcul. De plus, la définition de $A$ et $R_A'$ sur la première partie du domaine permet de rendre compte du faire que $M$ contient $s.f(i)$ dans le registre $R_i$.
			
			On montre facilement par induction sur $t$ que la valeur des fonctions au temps $c(s.n)^K+t$ sont correctes. La partie la plus difficile réside dans le calcul de $R_A$. Remarquons premièrement que, puisque $A(a) < c(s.n)^K$ pour chaque $a$, l'initialisation sur la première partie du domaine assure que $A^{\leftarrow}(a) < a$ pour tout $a \in \intint{cn^K}{2cn^K-1}$. Deux cas possibles :
			
			\begin{itemize}[itemsep=-1mm]
				\item 	
				$c(s.n)^K \leqslant A^{\leftarrow}(a) < a$. Dans ce cas, il existe $b \in \intint{c(s.n)^K}{a-1}$ tel que $A(a) = A(b)$ ; donc le registre courant $R$ a déjà été visité pendant le calcul, et $A^{\leftarrow}(a)$ est la dernière étape où cela s'est produit ; ainsi $R_A'\left(A^{\leftarrow}(a)\right)$ donne la bonne valeur de $R_A(a)$.
				
				\item 
				$A^{\leftarrow}(a) < c(s.n)^K$. Dans ce cas, il n'y a pas de $b \in \intint{c(s.n)^K}{a-1}$ tel que $A(a) = A(b)$ ; donc le registre courant $R$ n'a pas été visité pour le moment, et devrait toujours contenir la valeur de $f(A^{\leftarrow}(a))$\footnotemark. Par initialisation, il découle que $A^{\leftarrow}(a) = A(a)$ et $R_A'(A(a)) = f(A(a))$, comme voulu.
				
				\footnotetext{Petite erreur de frappe dans l'article original ? Il y est écrit : $f(A(a))$.}
			\end{itemize}
			
			Enfin, on doit encore montrer que $S$ définit correctement $\Gamma(s)$. Par définition de $R_A$ et $N$ sur la troisième partie du domaine du LSRS, pour chaque $a$, $R_A\left( 2 c(s.n)^K + a \right)$ contient les valeurs de du registre $R_a$ à la fin du calcul. De plus, $N(3c(s.n)^K-1)$ contient la valeur de $N$ à la fin du calcul. Ainsi, avec une projection bien choisie, $\Gamma(s)$ peut être extraite des fonctions définies par $S$. 
			
			Ainsi, on a montré que $S$ calcule correctement $\Gamma(s)$. Donc $\Gamma$ est $n^K$-représentée par $S$.
			
		\end{proof}
		
		
		
	\end{appendices}
	
	
	
	